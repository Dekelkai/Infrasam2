INFO 2025-08-12 00:52:45,802 train_utils.py: 108: MACHINE SEED: 4920
INFO 2025-08-12 00:52:45,807 train_utils.py: 154: Logging ENV_VARIABLES
INFO 2025-08-12 00:52:45,807 train_utils.py: 155: BROWSER=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/helpers/browser.sh
BUNDLED_DEBUGPY_PATH=/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/bundled/libs/debugpy
COLORTERM=truecolor
CONDA_DEFAULT_ENV=sam2
CONDA_EXE=/home/guest/miniconda3/bin/conda
CONDA_PREFIX=/home/guest/miniconda3/envs/sam2
CONDA_PREFIX_1=/home/guest/miniconda3
CONDA_PROMPT_MODIFIER=(sam2) 
CONDA_PYTHON_EXE=/home/guest/miniconda3/bin/python
CONDA_SHLVL=2
CUDA_HOME=/usr/local/cuda
CUDA_MODULE_LOADING=LAZY
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus
GIT_ASKPASS=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/extensions/git/dist/askpass.sh
HOME=/home/guest
HYDRA_FULL_ERROR=1
JAVA_HOME=/home/guest/RaoXuefeng/app/jdk-11.0.24
LANG=en_US.UTF-8
LC_ADDRESS=en_US.UTF-8
LC_IDENTIFICATION=en_US.UTF-8
LC_MEASUREMENT=en_US.UTF-8
LC_MONETARY=en_US.UTF-8
LC_NAME=en_US.UTF-8
LC_NUMERIC=en_US.UTF-8
LC_PAPER=en_US.UTF-8
LC_TELEPHONE=en_US.UTF-8
LC_TIME=en_US.UTF-8
LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:
LESS=-R
LOCAL_RANK=0
LOGNAME=guest
LSCOLORS=Gxfxcxdxbxegedabagacad
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
MASTER_ADDR=localhost
MASTER_PORT=35492
MOTD_SHOWN=pam
OLDPWD=/data1/tangxiangkai/sam2
PAGER=less
PAPERSIZE=letter
PATH=/home/guest/RaoXuefeng/app/nvim-linux64/bin:/home/guest/RaoXuefeng/app/jdk-11.0.24/bin:/usr/local/cuda/bin:/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/remote-cli:/home/guest/miniconda3/envs/sam2/bin:/home/guest/miniconda3/condabin:/home/guest/RaoXuefeng/app/nvim-linux64/bin:/home/guest/RaoXuefeng/app/jdk-11.0.24/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/bundled/scripts/noConfigScripts
PWD=/data1/tangxiangkai/sam2
PYDEVD_DISABLE_FILE_VALIDATION=1
PYTHONSTARTUP=/home/guest/.vscode-server/data/User/workspaceStorage/8858c19136ed4d2d77488896ccb3c4fb-1/ms-python.python/pythonrc.py
PYTHON_BASIC_REPL=1
RANK=0
SHELL=/bin/zsh
SHLVL=1
SSH_CLIENT=222.177.140.114 65432 5122
SSH_CONNECTION=222.177.140.114 65432 10.16.10.178 5122
SSL_CERT_DIR=/usr/lib/ssl/certs
SSL_CERT_FILE=/usr/lib/ssl/certs/ca-certificates.crt
TERM=xterm-256color
TERM_PROGRAM=vscode
TERM_PROGRAM_VERSION=1.98.2
TORCH_NCCL_ASYNC_ERROR_HANDLING=1
USER=guest
VSCODE_DEBUGPY_ADAPTER_ENDPOINTS=/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/.noConfigDebugAdapterEndpoints/endpoint-d9dbd2f801c99338.txt
VSCODE_GIT_ASKPASS_EXTRA_ARGS=
VSCODE_GIT_ASKPASS_MAIN=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/extensions/git/dist/askpass-main.js
VSCODE_GIT_ASKPASS_NODE=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/node
VSCODE_GIT_IPC_HANDLE=/run/user/1000/vscode-git-757e6297a1.sock
VSCODE_INJECTION=1
VSCODE_IPC_HOOK_CLI=/run/user/1000/vscode-ipc-c696b4d0-5513-4700-b7ce-c5967ea8ebcc.sock
VSCODE_NONCE=8b526d26-2395-402f-a1e3-c1e5532baa66
VSCODE_STABLE=1
WORLD_SIZE=3
XDG_RUNTIME_DIR=/run/user/1000
XDG_SESSION_CLASS=user
XDG_SESSION_ID=320
XDG_SESSION_TYPE=tty
ZSH=/home/guest/.oh-my-zsh
_=/home/guest/miniconda3/envs/sam2/bin/python
_CE_CONDA=
_CE_M=
nvim_home=/home/guest/RaoXuefeng/app/nvim-linux64

INFO 2025-08-12 00:52:45,807 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.
INFO 2025-08-12 00:52:45,810 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /data1/tangxiangkai/sam2/sam2_logs/configs/sam2.1_training/sam2.1_hiera_b+_MOSE_finetune_AntiUAV.yaml/tensorboard
INFO 2025-08-12 00:52:46,773 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5
INFO 2025-08-12 00:52:46,777 trainer.py:1059: ====================
INFO 2025-08-12 00:52:46,777 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>
INFO 2025-08-12 00:52:46,779 trainer.py:1061: Model is SAM2Train(
  (image_encoder): ImageEncoder(
    (trunk): Hiera(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=112, out_features=336, bias=True)
            (proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=112, out_features=448, bias=True)
              (1): Linear(in_features=448, out_features=112, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=112, out_features=336, bias=True)
            (proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=112, out_features=448, bias=True)
              (1): Linear(in_features=448, out_features=112, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=112, out_features=672, bias=True)
            (proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=224, out_features=896, bias=True)
              (1): Linear(in_features=896, out_features=224, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=112, out_features=224, bias=True)
        )
        (3-4): 2 x MultiScaleBlock(
          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=224, out_features=672, bias=True)
            (proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=224, out_features=896, bias=True)
              (1): Linear(in_features=896, out_features=224, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=224, out_features=1344, bias=True)
            (proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=448, out_features=1792, bias=True)
              (1): Linear(in_features=1792, out_features=448, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=224, out_features=448, bias=True)
        )
        (6-20): 15 x MultiScaleBlock(
          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=448, out_features=1344, bias=True)
            (proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=448, out_features=1792, bias=True)
              (1): Linear(in_features=1792, out_features=448, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (21): MultiScaleBlock(
          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=448, out_features=2688, bias=True)
            (proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=896, out_features=3584, bias=True)
              (1): Linear(in_features=3584, out_features=896, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=448, out_features=896, bias=True)
        )
        (22-23): 2 x MultiScaleBlock(
          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=896, out_features=2688, bias=True)
            (proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=896, out_features=3584, bias=True)
              (1): Linear(in_features=3584, out_features=896, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
      )
    )
    (neck): FpnNeck(
      (position_encoding): PositionEmbeddingSine()
      (convs): ModuleList(
        (0): Sequential(
          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Sequential(
          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Sequential(
          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))
  (memory_attention): MemoryAttention(
    (layers): ModuleList(
      (0-3): 4 x MemoryAttentionLayer(
        (self_attn): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn_image): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=64, out_features=256, bias=True)
          (v_proj): Linear(in_features=64, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (memory_encoder): MemoryEncoder(
    (mask_downsampler): MaskDownSampler(
      (encoder): Sequential(
        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): LayerNorm2d()
        (2): GELU(approximate='none')
        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): LayerNorm2d()
        (5): GELU(approximate='none')
        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (7): LayerNorm2d()
        (8): GELU(approximate='none')
        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (10): LayerNorm2d()
        (11): GELU(approximate='none')
        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fuser): Fuser(
      (proj): Identity()
      (layers): ModuleList(
        (0-1): 2 x CXBlock(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm2d()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
      )
    )
    (position_encoding): PositionEmbeddingSine()
    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (sam_prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (sam_mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): Linear(in_features=2048, out_features=256, bias=True)
            )
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (obj_score_token): Embedding(1, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
        (act): ReLU()
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU()
    )
    (pred_obj_score_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=1, bias=True)
      )
      (act): ReLU()
    )
  )
  (obj_ptr_proj): MLP(
    (layers): ModuleList(
      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
    )
    (act): ReLU()
  )
  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)
)
INFO 2025-08-12 00:52:46,779 trainer.py:1062: 	Total parameters 80.9 M
INFO 2025-08-12 00:52:46,780 trainer.py:1063: 	Trainable parameters 80.9 M
INFO 2025-08-12 00:52:46,780 trainer.py:1066: 	Non-Trainable parameters 0  
INFO 2025-08-12 00:52:46,780 trainer.py:1069: ====================
INFO 2025-08-12 00:52:46,783 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.
INFO 2025-08-12 00:52:46,783 trainer.py: 314: Moving components to device cuda:0 and local rank 0.
INFO 2025-08-12 00:52:46,889 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.
INFO 2025-08-12 00:52:46,902 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.21.proj.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.proj.weight', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.17.norm2.weight'}
INFO 2025-08-12 00:52:46,904 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.neck.convs.0.conv.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.neck.convs.3.conv.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'memory_encoder.out_proj.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'memory_attention.norm.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'memory_attention.layers.3.norm3.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'obj_ptr_tpos_proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'obj_ptr_proj.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.neck.convs.2.conv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'memory_attention.layers.0.norm2.bias', 'memory_attention.layers.2.linear2.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'obj_ptr_proj.layers.1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'memory_attention.layers.1.norm2.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_attention.layers.0.linear1.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'sam_mask_decoder.conv_s1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'memory_attention.layers.1.linear2.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'memory_attention.layers.0.norm3.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'memory_encoder.pix_feat_proj.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'memory_attention.layers.0.linear2.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'memory_attention.layers.1.linear1.bias', 'memory_attention.layers.3.linear1.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'obj_ptr_proj.layers.2.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'sam_mask_decoder.conv_s0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'mask_downsample.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias'}
INFO 2025-08-12 00:52:46,904 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'memory_attention.norm.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.layers.0.norm3.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'memory_attention.layers.1.norm2.weight', 'image_encoder.trunk.blocks.9.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.1.norm3.bias', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'memory_attention.layers.2.norm3.weight', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'memory_attention.layers.2.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'memory_attention.layers.0.norm3.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.bias', 'memory_attention.layers.1.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'memory_attention.layers.3.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_attention.layers.1.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.17.norm2.weight'} 
INFO 2025-08-12 00:52:47,345 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]
INFO 2025-08-12 00:52:48,106 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': '/home/guest/tangxiangkai/sam2/sam2_logs/configs/sam2.1_training/sam2.1_hiera_b+_tzb_finetune.yaml/checkpoints/checkpoint.pt', 'ckpt_state_dict_keys': ['model']}}
INFO 2025-08-12 00:53:00,928 train_utils.py: 271: Train Epoch: [0][  0/136] | Batch Time: 12.31 (12.31) | Data Time: 7.91 (7.91) | Mem (GB): 16.00 (16.00/16.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 3.93e-01 (3.93e-01)
INFO 2025-08-12 00:53:45,943 train_utils.py: 108: MACHINE SEED: 4920
INFO 2025-08-12 00:53:45,947 train_utils.py: 154: Logging ENV_VARIABLES
INFO 2025-08-12 00:53:45,947 train_utils.py: 155: BROWSER=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/helpers/browser.sh
BUNDLED_DEBUGPY_PATH=/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/bundled/libs/debugpy
COLORTERM=truecolor
CONDA_DEFAULT_ENV=sam2
CONDA_EXE=/home/guest/miniconda3/bin/conda
CONDA_PREFIX=/home/guest/miniconda3/envs/sam2
CONDA_PREFIX_1=/home/guest/miniconda3
CONDA_PROMPT_MODIFIER=(sam2) 
CONDA_PYTHON_EXE=/home/guest/miniconda3/bin/python
CONDA_SHLVL=2
CUDA_HOME=/usr/local/cuda
CUDA_MODULE_LOADING=LAZY
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus
GIT_ASKPASS=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/extensions/git/dist/askpass.sh
HOME=/home/guest
HYDRA_FULL_ERROR=1
JAVA_HOME=/home/guest/RaoXuefeng/app/jdk-11.0.24
LANG=en_US.UTF-8
LC_ADDRESS=en_US.UTF-8
LC_IDENTIFICATION=en_US.UTF-8
LC_MEASUREMENT=en_US.UTF-8
LC_MONETARY=en_US.UTF-8
LC_NAME=en_US.UTF-8
LC_NUMERIC=en_US.UTF-8
LC_PAPER=en_US.UTF-8
LC_TELEPHONE=en_US.UTF-8
LC_TIME=en_US.UTF-8
LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:
LESS=-R
LOCAL_RANK=0
LOGNAME=guest
LSCOLORS=Gxfxcxdxbxegedabagacad
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
MASTER_ADDR=localhost
MASTER_PORT=41880
MOTD_SHOWN=pam
OLDPWD=/data1/tangxiangkai/sam2
PAGER=less
PAPERSIZE=letter
PATH=/home/guest/RaoXuefeng/app/nvim-linux64/bin:/home/guest/RaoXuefeng/app/jdk-11.0.24/bin:/usr/local/cuda/bin:/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/remote-cli:/home/guest/miniconda3/envs/sam2/bin:/home/guest/miniconda3/condabin:/home/guest/RaoXuefeng/app/nvim-linux64/bin:/home/guest/RaoXuefeng/app/jdk-11.0.24/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/bundled/scripts/noConfigScripts
PWD=/data1/tangxiangkai/sam2
PYDEVD_DISABLE_FILE_VALIDATION=1
PYTHONSTARTUP=/home/guest/.vscode-server/data/User/workspaceStorage/8858c19136ed4d2d77488896ccb3c4fb-1/ms-python.python/pythonrc.py
PYTHON_BASIC_REPL=1
RANK=0
SHELL=/bin/zsh
SHLVL=1
SSH_CLIENT=222.177.140.114 65432 5122
SSH_CONNECTION=222.177.140.114 65432 10.16.10.178 5122
SSL_CERT_DIR=/usr/lib/ssl/certs
SSL_CERT_FILE=/usr/lib/ssl/certs/ca-certificates.crt
TERM=xterm-256color
TERM_PROGRAM=vscode
TERM_PROGRAM_VERSION=1.98.2
TORCH_NCCL_ASYNC_ERROR_HANDLING=1
USER=guest
VSCODE_DEBUGPY_ADAPTER_ENDPOINTS=/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/.noConfigDebugAdapterEndpoints/endpoint-d9dbd2f801c99338.txt
VSCODE_GIT_ASKPASS_EXTRA_ARGS=
VSCODE_GIT_ASKPASS_MAIN=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/extensions/git/dist/askpass-main.js
VSCODE_GIT_ASKPASS_NODE=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/node
VSCODE_GIT_IPC_HANDLE=/run/user/1000/vscode-git-757e6297a1.sock
VSCODE_INJECTION=1
VSCODE_IPC_HOOK_CLI=/run/user/1000/vscode-ipc-c696b4d0-5513-4700-b7ce-c5967ea8ebcc.sock
VSCODE_NONCE=8b526d26-2395-402f-a1e3-c1e5532baa66
VSCODE_STABLE=1
WORLD_SIZE=3
XDG_RUNTIME_DIR=/run/user/1000
XDG_SESSION_CLASS=user
XDG_SESSION_ID=320
XDG_SESSION_TYPE=tty
ZSH=/home/guest/.oh-my-zsh
_=/home/guest/miniconda3/envs/sam2/bin/python
_CE_CONDA=
_CE_M=
nvim_home=/home/guest/RaoXuefeng/app/nvim-linux64

INFO 2025-08-12 00:53:45,947 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.
INFO 2025-08-12 00:53:45,950 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /data1/tangxiangkai/sam2/sam2_logs/configs/sam2.1_training/sam2.1_hiera_b+_MOSE_finetune_AntiUAV.yaml/tensorboard
INFO 2025-08-12 00:53:47,025 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5
INFO 2025-08-12 00:53:47,028 trainer.py:1059: ====================
INFO 2025-08-12 00:53:47,029 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>
INFO 2025-08-12 00:53:47,031 trainer.py:1061: Model is SAM2Train(
  (image_encoder): ImageEncoder(
    (trunk): Hiera(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=112, out_features=336, bias=True)
            (proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=112, out_features=448, bias=True)
              (1): Linear(in_features=448, out_features=112, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=112, out_features=336, bias=True)
            (proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=112, out_features=448, bias=True)
              (1): Linear(in_features=448, out_features=112, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=112, out_features=672, bias=True)
            (proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=224, out_features=896, bias=True)
              (1): Linear(in_features=896, out_features=224, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=112, out_features=224, bias=True)
        )
        (3-4): 2 x MultiScaleBlock(
          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=224, out_features=672, bias=True)
            (proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=224, out_features=896, bias=True)
              (1): Linear(in_features=896, out_features=224, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=224, out_features=1344, bias=True)
            (proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=448, out_features=1792, bias=True)
              (1): Linear(in_features=1792, out_features=448, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=224, out_features=448, bias=True)
        )
        (6-20): 15 x MultiScaleBlock(
          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=448, out_features=1344, bias=True)
            (proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=448, out_features=1792, bias=True)
              (1): Linear(in_features=1792, out_features=448, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (21): MultiScaleBlock(
          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=448, out_features=2688, bias=True)
            (proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=896, out_features=3584, bias=True)
              (1): Linear(in_features=3584, out_features=896, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=448, out_features=896, bias=True)
        )
        (22-23): 2 x MultiScaleBlock(
          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=896, out_features=2688, bias=True)
            (proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=896, out_features=3584, bias=True)
              (1): Linear(in_features=3584, out_features=896, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
      )
    )
    (neck): FpnNeck(
      (position_encoding): PositionEmbeddingSine()
      (convs): ModuleList(
        (0): Sequential(
          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Sequential(
          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Sequential(
          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))
  (memory_attention): MemoryAttention(
    (layers): ModuleList(
      (0-3): 4 x MemoryAttentionLayer(
        (self_attn): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn_image): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=64, out_features=256, bias=True)
          (v_proj): Linear(in_features=64, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (memory_encoder): MemoryEncoder(
    (mask_downsampler): MaskDownSampler(
      (encoder): Sequential(
        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): LayerNorm2d()
        (2): GELU(approximate='none')
        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): LayerNorm2d()
        (5): GELU(approximate='none')
        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (7): LayerNorm2d()
        (8): GELU(approximate='none')
        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (10): LayerNorm2d()
        (11): GELU(approximate='none')
        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fuser): Fuser(
      (proj): Identity()
      (layers): ModuleList(
        (0-1): 2 x CXBlock(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm2d()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
      )
    )
    (position_encoding): PositionEmbeddingSine()
    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (sam_prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (sam_mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): Linear(in_features=2048, out_features=256, bias=True)
            )
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (obj_score_token): Embedding(1, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
        (act): ReLU()
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU()
    )
    (pred_obj_score_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=1, bias=True)
      )
      (act): ReLU()
    )
  )
  (obj_ptr_proj): MLP(
    (layers): ModuleList(
      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
    )
    (act): ReLU()
  )
  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)
)
INFO 2025-08-12 00:53:47,032 trainer.py:1062: 	Total parameters 80.9 M
INFO 2025-08-12 00:53:47,032 trainer.py:1063: 	Trainable parameters 80.9 M
INFO 2025-08-12 00:53:47,032 trainer.py:1066: 	Non-Trainable parameters 0  
INFO 2025-08-12 00:53:47,032 trainer.py:1069: ====================
INFO 2025-08-12 00:53:47,035 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.
INFO 2025-08-12 00:53:47,035 trainer.py: 314: Moving components to device cuda:0 and local rank 0.
INFO 2025-08-12 00:53:47,157 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.
INFO 2025-08-12 00:53:47,174 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.21.proj.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.proj.weight', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias'}
INFO 2025-08-12 00:53:47,175 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.neck.convs.1.conv.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'obj_ptr_tpos_proj.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'memory_attention.layers.2.linear1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'memory_attention.layers.3.norm3.bias', 'memory_encoder.pix_feat_proj.bias', 'obj_ptr_proj.layers.0.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias', 'image_encoder.neck.convs.2.conv.bias', 'memory_attention.layers.3.linear1.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'obj_ptr_proj.layers.2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'memory_attention.layers.3.norm1.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'obj_ptr_proj.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'memory_encoder.out_proj.bias', 'memory_attention.layers.2.norm2.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'memory_attention.layers.0.linear1.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'memory_attention.layers.0.linear2.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'memory_attention.layers.0.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'memory_attention.layers.3.norm2.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'memory_attention.layers.1.linear2.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.conv_s0.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'memory_attention.layers.2.linear2.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'memory_attention.layers.1.linear1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'sam_mask_decoder.conv_s1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'memory_attention.layers.3.linear2.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'mask_downsample.bias', 'memory_attention.layers.1.norm2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias'}
INFO 2025-08-12 00:53:47,176 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.10.norm2.bias', 'memory_attention.layers.3.norm1.weight', 'image_encoder.trunk.blocks.1.norm1.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'memory_attention.layers.2.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'memory_attention.layers.2.norm3.weight', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.7.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.9.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'memory_attention.layers.3.norm3.bias', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'memory_attention.layers.3.norm2.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.4.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.weight', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'memory_attention.layers.0.norm3.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'memory_attention.layers.0.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'memory_attention.layers.1.norm2.weight', 'memory_attention.layers.1.norm1.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'memory_attention.layers.3.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.17.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.14.norm1.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'memory_attention.layers.0.norm3.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'memory_attention.layers.2.norm1.weight', 'memory_attention.norm.weight', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'memory_attention.layers.1.norm3.weight', 'memory_attention.layers.0.norm2.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.17.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.weight', 'memory_attention.layers.1.norm3.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.18.norm1.weight', 'memory_attention.layers.2.norm2.weight', 'memory_attention.layers.1.norm2.bias'} 
INFO 2025-08-12 00:53:47,621 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]
INFO 2025-08-12 00:53:52,271 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': '/home/guest/tangxiangkai/sam2/sam2_logs/configs/sam2.1_training/sam2.1_hiera_b+_tzb_finetune.yaml/checkpoints/checkpoint.pt', 'ckpt_state_dict_keys': ['model']}}
INFO 2025-08-12 00:54:04,161 train_utils.py: 271: Train Epoch: [0][  0/136] | Batch Time: 11.30 (11.30) | Data Time: 8.17 (8.17) | Mem (GB): 16.00 (16.00/16.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 3.93e-01 (3.93e-01)
INFO 2025-08-12 00:55:41,264 train_utils.py: 108: MACHINE SEED: 4920
INFO 2025-08-12 00:55:41,268 train_utils.py: 154: Logging ENV_VARIABLES
INFO 2025-08-12 00:55:41,269 train_utils.py: 155: BROWSER=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/helpers/browser.sh
BUNDLED_DEBUGPY_PATH=/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/bundled/libs/debugpy
COLORTERM=truecolor
CONDA_DEFAULT_ENV=sam2
CONDA_EXE=/home/guest/miniconda3/bin/conda
CONDA_PREFIX=/home/guest/miniconda3/envs/sam2
CONDA_PREFIX_1=/home/guest/miniconda3
CONDA_PROMPT_MODIFIER=(sam2) 
CONDA_PYTHON_EXE=/home/guest/miniconda3/bin/python
CONDA_SHLVL=2
CUDA_HOME=/usr/local/cuda
CUDA_MODULE_LOADING=LAZY
DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus
GIT_ASKPASS=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/extensions/git/dist/askpass.sh
HOME=/home/guest
HYDRA_FULL_ERROR=1
JAVA_HOME=/home/guest/RaoXuefeng/app/jdk-11.0.24
LANG=en_US.UTF-8
LC_ADDRESS=en_US.UTF-8
LC_IDENTIFICATION=en_US.UTF-8
LC_MEASUREMENT=en_US.UTF-8
LC_MONETARY=en_US.UTF-8
LC_NAME=en_US.UTF-8
LC_NUMERIC=en_US.UTF-8
LC_PAPER=en_US.UTF-8
LC_TELEPHONE=en_US.UTF-8
LC_TIME=en_US.UTF-8
LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:
LESS=-R
LOCAL_RANK=0
LOGNAME=guest
LSCOLORS=Gxfxcxdxbxegedabagacad
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
MASTER_ADDR=localhost
MASTER_PORT=32276
MOTD_SHOWN=pam
OLDPWD=/data1/tangxiangkai/sam2
PAGER=less
PAPERSIZE=letter
PATH=/home/guest/RaoXuefeng/app/nvim-linux64/bin:/home/guest/RaoXuefeng/app/jdk-11.0.24/bin:/usr/local/cuda/bin:/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/bin/remote-cli:/home/guest/miniconda3/envs/sam2/bin:/home/guest/miniconda3/condabin:/home/guest/RaoXuefeng/app/nvim-linux64/bin:/home/guest/RaoXuefeng/app/jdk-11.0.24/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/bundled/scripts/noConfigScripts
PWD=/data1/tangxiangkai/sam2
PYDEVD_DISABLE_FILE_VALIDATION=1
PYTHONSTARTUP=/home/guest/.vscode-server/data/User/workspaceStorage/8858c19136ed4d2d77488896ccb3c4fb-1/ms-python.python/pythonrc.py
PYTHON_BASIC_REPL=1
RANK=0
SHELL=/bin/zsh
SHLVL=1
SSH_CLIENT=222.177.140.114 65432 5122
SSH_CONNECTION=222.177.140.114 65432 10.16.10.178 5122
SSL_CERT_DIR=/usr/lib/ssl/certs
SSL_CERT_FILE=/usr/lib/ssl/certs/ca-certificates.crt
TERM=xterm-256color
TERM_PROGRAM=vscode
TERM_PROGRAM_VERSION=1.98.2
TORCH_NCCL_ASYNC_ERROR_HANDLING=1
USER=guest
VSCODE_DEBUGPY_ADAPTER_ENDPOINTS=/home/guest/.vscode-server/extensions/ms-python.debugpy-2025.11.2025072901-linux-x64/.noConfigDebugAdapterEndpoints/endpoint-d9dbd2f801c99338.txt
VSCODE_GIT_ASKPASS_EXTRA_ARGS=
VSCODE_GIT_ASKPASS_MAIN=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/extensions/git/dist/askpass-main.js
VSCODE_GIT_ASKPASS_NODE=/home/guest/.vscode-server/cli/servers/Stable-ddc367ed5c8936efe395cffeec279b04ffd7db78/server/node
VSCODE_GIT_IPC_HANDLE=/run/user/1000/vscode-git-757e6297a1.sock
VSCODE_INJECTION=1
VSCODE_IPC_HOOK_CLI=/run/user/1000/vscode-ipc-c696b4d0-5513-4700-b7ce-c5967ea8ebcc.sock
VSCODE_NONCE=8b526d26-2395-402f-a1e3-c1e5532baa66
VSCODE_STABLE=1
WORLD_SIZE=3
XDG_RUNTIME_DIR=/run/user/1000
XDG_SESSION_CLASS=user
XDG_SESSION_ID=320
XDG_SESSION_TYPE=tty
ZSH=/home/guest/.oh-my-zsh
_=/home/guest/miniconda3/envs/sam2/bin/python
_CE_CONDA=
_CE_M=
nvim_home=/home/guest/RaoXuefeng/app/nvim-linux64

INFO 2025-08-12 00:55:41,269 trainer.py: 989: Setting up components: Model, loss, optim, meters etc.
INFO 2025-08-12 00:55:41,272 logger.py:  66: TensorBoard SummaryWriter instantiated. Files will be stored in: /data1/tangxiangkai/sam2/sam2_logs/configs/sam2.1_training/sam2.1_hiera_b+_MOSE_finetune_AntiUAV.yaml/tensorboard
INFO 2025-08-12 00:55:42,295 sam2.py:  81: Training with points (sampled from masks) as inputs with p=0.5
INFO 2025-08-12 00:55:42,299 trainer.py:1059: ====================
INFO 2025-08-12 00:55:42,299 trainer.py:1060: Summary for model <class 'training.model.sam2.SAM2Train'>
INFO 2025-08-12 00:55:42,301 trainer.py:1061: Model is SAM2Train(
  (image_encoder): ImageEncoder(
    (trunk): Hiera(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 112, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      )
      (blocks): ModuleList(
        (0): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=112, out_features=336, bias=True)
            (proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=112, out_features=448, bias=True)
              (1): Linear(in_features=448, out_features=112, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (1): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=112, out_features=336, bias=True)
            (proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=112, out_features=448, bias=True)
              (1): Linear(in_features=448, out_features=112, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (2): MultiScaleBlock(
          (norm1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=112, out_features=672, bias=True)
            (proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=224, out_features=896, bias=True)
              (1): Linear(in_features=896, out_features=224, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=112, out_features=224, bias=True)
        )
        (3-4): 2 x MultiScaleBlock(
          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=224, out_features=672, bias=True)
            (proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=224, out_features=896, bias=True)
              (1): Linear(in_features=896, out_features=224, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (5): MultiScaleBlock(
          (norm1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=224, out_features=1344, bias=True)
            (proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=448, out_features=1792, bias=True)
              (1): Linear(in_features=1792, out_features=448, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=224, out_features=448, bias=True)
        )
        (6-20): 15 x MultiScaleBlock(
          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=448, out_features=1344, bias=True)
            (proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=448, out_features=1792, bias=True)
              (1): Linear(in_features=1792, out_features=448, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
        (21): MultiScaleBlock(
          (norm1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
          (attn): MultiScaleAttention(
            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
            (qkv): Linear(in_features=448, out_features=2688, bias=True)
            (proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=896, out_features=3584, bias=True)
              (1): Linear(in_features=3584, out_features=896, bias=True)
            )
            (act): GELU(approximate='none')
          )
          (proj): Linear(in_features=448, out_features=896, bias=True)
        )
        (22-23): 2 x MultiScaleBlock(
          (norm1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (attn): MultiScaleAttention(
            (qkv): Linear(in_features=896, out_features=2688, bias=True)
            (proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=896, out_features=3584, bias=True)
              (1): Linear(in_features=3584, out_features=896, bias=True)
            )
            (act): GELU(approximate='none')
          )
        )
      )
    )
    (neck): FpnNeck(
      (position_encoding): PositionEmbeddingSine()
      (convs): ModuleList(
        (0): Sequential(
          (conv): Conv2d(896, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Sequential(
          (conv): Conv2d(448, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Sequential(
          (conv): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Sequential(
          (conv): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
    )
  )
  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))
  (memory_attention): MemoryAttention(
    (layers): ModuleList(
      (0-3): 4 x MemoryAttentionLayer(
        (self_attn): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=256, out_features=256, bias=True)
          (v_proj): Linear(in_features=256, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (cross_attn_image): RoPEAttention(
          (q_proj): Linear(in_features=256, out_features=256, bias=True)
          (k_proj): Linear(in_features=64, out_features=256, bias=True)
          (v_proj): Linear(in_features=64, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=256, bias=True)
        )
        (linear1): Linear(in_features=256, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=256, bias=True)
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (dropout3): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (memory_encoder): MemoryEncoder(
    (mask_downsampler): MaskDownSampler(
      (encoder): Sequential(
        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): LayerNorm2d()
        (2): GELU(approximate='none')
        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): LayerNorm2d()
        (5): GELU(approximate='none')
        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (7): LayerNorm2d()
        (8): GELU(approximate='none')
        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (10): LayerNorm2d()
        (11): GELU(approximate='none')
        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (fuser): Fuser(
      (proj): Identity()
      (layers): ModuleList(
        (0-1): 2 x CXBlock(
          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)
          (norm): LayerNorm2d()
          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU(approximate='none')
          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)
          (drop_path): Identity()
        )
      )
    )
    (position_encoding): PositionEmbeddingSine()
    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (sam_prompt_encoder): PromptEncoder(
    (pe_layer): PositionEmbeddingRandom()
    (point_embeddings): ModuleList(
      (0-3): 4 x Embedding(1, 256)
    )
    (not_a_point_embed): Embedding(1, 256)
    (mask_downscaling): Sequential(
      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))
      (4): LayerNorm2d()
      (5): GELU(approximate='none')
      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
    )
    (no_mask_embed): Embedding(1, 256)
  )
  (sam_mask_decoder): MaskDecoder(
    (transformer): TwoWayTransformer(
      (layers): ModuleList(
        (0-1): 2 x TwoWayAttentionBlock(
          (self_attn): Attention(
            (q_proj): Linear(in_features=256, out_features=256, bias=True)
            (k_proj): Linear(in_features=256, out_features=256, bias=True)
            (v_proj): Linear(in_features=256, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_token_to_image): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=2048, bias=True)
              (1): Linear(in_features=2048, out_features=256, bias=True)
            )
            (act): ReLU()
          )
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (cross_attn_image_to_token): Attention(
            (q_proj): Linear(in_features=256, out_features=128, bias=True)
            (k_proj): Linear(in_features=256, out_features=128, bias=True)
            (v_proj): Linear(in_features=256, out_features=128, bias=True)
            (out_proj): Linear(in_features=128, out_features=256, bias=True)
          )
        )
      )
      (final_attn_token_to_image): Attention(
        (q_proj): Linear(in_features=256, out_features=128, bias=True)
        (k_proj): Linear(in_features=256, out_features=128, bias=True)
        (v_proj): Linear(in_features=256, out_features=128, bias=True)
        (out_proj): Linear(in_features=128, out_features=256, bias=True)
      )
      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (iou_token): Embedding(1, 256)
    (mask_tokens): Embedding(4, 256)
    (obj_score_token): Embedding(1, 256)
    (output_upscaling): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
      (1): LayerNorm2d()
      (2): GELU(approximate='none')
      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))
      (4): GELU(approximate='none')
    )
    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))
    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
    (output_hypernetworks_mlps): ModuleList(
      (0-3): 4 x MLP(
        (layers): ModuleList(
          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=32, bias=True)
        )
        (act): ReLU()
      )
    )
    (iou_prediction_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
      (act): ReLU()
    )
    (pred_obj_score_head): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=1, bias=True)
      )
      (act): ReLU()
    )
  )
  (obj_ptr_proj): MLP(
    (layers): ModuleList(
      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)
    )
    (act): ReLU()
  )
  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)
)
INFO 2025-08-12 00:55:42,301 trainer.py:1062: 	Total parameters 80.9 M
INFO 2025-08-12 00:55:42,302 trainer.py:1063: 	Trainable parameters 80.9 M
INFO 2025-08-12 00:55:42,302 trainer.py:1066: 	Non-Trainable parameters 0  
INFO 2025-08-12 00:55:42,302 trainer.py:1069: ====================
INFO 2025-08-12 00:55:42,305 trainer.py:1023: Finished setting up components: Model, loss, optim, meters etc.
INFO 2025-08-12 00:55:42,305 trainer.py: 314: Moving components to device cuda:0 and local rank 0.
INFO 2025-08-12 00:55:42,421 trainer.py: 320: Done moving components to device cuda:0 and local rank 0.
INFO 2025-08-12 00:55:42,435 optimizer.py: 248: Matches for param_name [image_encoder.*]: {'image_encoder.trunk.blocks.2.attn.proj.weight', 'image_encoder.trunk.blocks.4.attn.qkv.bias', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.pos_embed_window', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.21.proj.weight', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.attn.proj.weight', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.proj.weight', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'image_encoder.neck.convs.2.conv.weight', 'image_encoder.neck.convs.0.conv.bias', 'image_encoder.trunk.blocks.5.attn.qkv.weight', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.0.attn.proj.weight', 'image_encoder.trunk.blocks.20.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.weight', 'image_encoder.trunk.blocks.23.attn.qkv.weight', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.weight', 'image_encoder.trunk.blocks.18.norm1.weight', 'image_encoder.trunk.blocks.11.mlp.layers.1.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.weight', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.norm1.weight', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'image_encoder.trunk.blocks.6.attn.proj.weight', 'image_encoder.trunk.blocks.0.attn.qkv.weight', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.13.attn.qkv.weight', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.qkv.weight', 'image_encoder.trunk.blocks.12.attn.qkv.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.attn.qkv.weight', 'image_encoder.trunk.blocks.17.mlp.layers.1.weight', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.norm2.weight', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.weight', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'image_encoder.trunk.blocks.1.norm2.weight', 'image_encoder.neck.convs.0.conv.weight', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.14.attn.proj.weight', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.proj.weight', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.15.attn.qkv.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.weight', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.weight', 'image_encoder.trunk.blocks.8.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.weight', 'image_encoder.trunk.blocks.22.mlp.layers.1.weight', 'image_encoder.neck.convs.1.conv.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.18.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.attn.qkv.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'image_encoder.trunk.blocks.8.attn.qkv.weight', 'image_encoder.trunk.blocks.4.mlp.layers.1.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.attn.proj.weight', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.weight', 'image_encoder.trunk.blocks.11.norm1.weight', 'image_encoder.trunk.blocks.2.proj.weight', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.weight', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'image_encoder.trunk.blocks.20.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.weight', 'image_encoder.trunk.blocks.3.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.attn.proj.weight', 'image_encoder.trunk.blocks.13.mlp.layers.0.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.10.attn.proj.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.1.mlp.layers.1.weight', 'image_encoder.trunk.blocks.1.attn.qkv.weight', 'image_encoder.trunk.blocks.0.mlp.layers.0.weight', 'image_encoder.trunk.blocks.4.attn.proj.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.weight', 'image_encoder.trunk.blocks.3.attn.qkv.weight', 'image_encoder.trunk.blocks.4.attn.qkv.weight', 'image_encoder.trunk.blocks.18.mlp.layers.0.weight', 'image_encoder.trunk.blocks.2.mlp.layers.0.weight', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.weight', 'image_encoder.trunk.blocks.12.attn.proj.weight', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.weight', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'image_encoder.trunk.blocks.8.attn.proj.weight', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.19.attn.qkv.weight', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'image_encoder.trunk.blocks.21.attn.proj.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.22.norm2.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.6.attn.qkv.weight', 'image_encoder.trunk.blocks.13.mlp.layers.1.weight', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'image_encoder.trunk.blocks.21.norm2.weight', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'image_encoder.trunk.blocks.3.attn.proj.weight', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'image_encoder.trunk.blocks.7.attn.proj.weight', 'image_encoder.trunk.blocks.13.attn.proj.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.weight', 'image_encoder.trunk.patch_embed.proj.weight', 'image_encoder.trunk.blocks.11.mlp.layers.0.weight', 'image_encoder.trunk.blocks.7.attn.qkv.weight', 'image_encoder.trunk.blocks.15.attn.proj.weight', 'image_encoder.trunk.blocks.22.attn.proj.weight', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.weight', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.20.attn.proj.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.weight', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.17.attn.qkv.weight', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'image_encoder.neck.convs.3.conv.weight', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.weight', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'image_encoder.trunk.blocks.22.attn.qkv.weight', 'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.11.attn.proj.weight', 'image_encoder.trunk.blocks.16.mlp.layers.1.weight', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'image_encoder.trunk.blocks.18.attn.proj.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.14.attn.qkv.weight', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.weight', 'image_encoder.trunk.blocks.16.attn.qkv.weight', 'image_encoder.trunk.blocks.11.attn.qkv.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.weight', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'image_encoder.trunk.blocks.20.norm2.weight', 'image_encoder.trunk.blocks.9.mlp.layers.0.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.10.attn.qkv.weight', 'image_encoder.trunk.blocks.1.attn.proj.weight', 'image_encoder.trunk.blocks.0.norm2.weight', 'image_encoder.trunk.pos_embed', 'image_encoder.trunk.blocks.17.attn.proj.weight', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.weight', 'image_encoder.trunk.blocks.5.proj.weight'}
INFO 2025-08-12 00:55:42,437 optimizer.py: 248: Matches for param_name [*bias*]: {'image_encoder.trunk.blocks.4.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.18.mlp.layers.0.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'sam_prompt_encoder.mask_downscaling.3.bias', 'image_encoder.trunk.blocks.10.attn.proj.bias', 'image_encoder.trunk.blocks.21.norm2.bias', 'memory_attention.layers.1.norm1.bias', 'sam_prompt_encoder.mask_downscaling.4.bias', 'image_encoder.trunk.blocks.17.mlp.layers.0.bias', 'memory_attention.layers.0.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.17.attn.qkv.bias', 'image_encoder.trunk.blocks.16.norm2.bias', 'image_encoder.trunk.blocks.11.mlp.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.1.bias', 'image_encoder.trunk.blocks.10.attn.qkv.bias', 'image_encoder.trunk.blocks.3.mlp.layers.1.bias', 'memory_attention.layers.3.linear1.bias', 'image_encoder.trunk.blocks.1.attn.qkv.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias', 'image_encoder.trunk.blocks.5.attn.qkv.bias', 'image_encoder.trunk.blocks.5.mlp.layers.1.bias', 'memory_attention.layers.1.cross_attn_image.q_proj.bias', 'memory_attention.layers.3.self_attn.v_proj.bias', 'memory_attention.layers.2.norm2.bias', 'image_encoder.trunk.blocks.23.attn.proj.bias', 'sam_mask_decoder.output_upscaling.3.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias', 'sam_mask_decoder.pred_obj_score_head.layers.1.bias', 'image_encoder.trunk.blocks.3.attn.proj.bias', 'image_encoder.trunk.blocks.12.attn.qkv.bias', 'image_encoder.trunk.blocks.7.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.neck.convs.0.conv.bias', 'memory_attention.layers.1.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.16.attn.proj.bias', 'image_encoder.trunk.blocks.4.norm2.bias', 'memory_attention.layers.2.self_attn.k_proj.bias', 'memory_attention.layers.3.cross_attn_image.v_proj.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'image_encoder.trunk.blocks.2.mlp.layers.0.bias', 'image_encoder.trunk.blocks.20.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias', 'image_encoder.trunk.blocks.5.attn.proj.bias', 'memory_attention.norm.bias', 'memory_attention.layers.0.self_attn.k_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias', 'memory_encoder.pix_feat_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias', 'image_encoder.trunk.blocks.14.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.12.bias', 'image_encoder.trunk.blocks.1.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias', 'memory_encoder.mask_downsampler.encoder.7.bias', 'memory_attention.layers.3.norm3.bias', 'memory_attention.layers.0.cross_attn_image.q_proj.bias', 'image_encoder.trunk.blocks.15.attn.qkv.bias', 'memory_encoder.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.5.proj.bias', 'image_encoder.trunk.blocks.10.mlp.layers.1.bias', 'memory_attention.layers.1.self_attn.q_proj.bias', 'memory_attention.layers.3.cross_attn_image.out_proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.16.mlp.layers.0.bias', 'memory_attention.layers.2.cross_attn_image.v_proj.bias', 'memory_attention.layers.2.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'sam_mask_decoder.iou_prediction_head.layers.2.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'image_encoder.trunk.blocks.9.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.bias', 'sam_mask_decoder.output_upscaling.1.bias', 'image_encoder.trunk.blocks.10.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.proj.bias', 'image_encoder.trunk.blocks.11.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.0.bias', 'image_encoder.trunk.blocks.15.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'image_encoder.trunk.blocks.8.mlp.layers.1.bias', 'image_encoder.trunk.blocks.2.mlp.layers.1.bias', 'memory_attention.layers.2.norm1.bias', 'memory_encoder.fuser.layers.1.pwconv1.bias', 'image_encoder.trunk.blocks.20.attn.proj.bias', 'image_encoder.trunk.blocks.15.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias', 'image_encoder.trunk.blocks.19.attn.proj.bias', 'obj_ptr_proj.layers.2.bias', 'memory_attention.layers.3.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.6.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.norm1.bias', 'memory_attention.layers.3.cross_attn_image.q_proj.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias', 'memory_encoder.fuser.layers.0.pwconv1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias', 'memory_encoder.fuser.layers.1.norm.bias', 'obj_ptr_proj.layers.1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.neck.convs.1.conv.bias', 'image_encoder.trunk.blocks.23.mlp.layers.1.bias', 'sam_mask_decoder.iou_prediction_head.layers.1.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'image_encoder.trunk.blocks.19.attn.qkv.bias', 'image_encoder.trunk.blocks.8.norm2.bias', 'memory_attention.layers.3.self_attn.k_proj.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'memory_attention.layers.1.norm3.bias', 'memory_attention.layers.2.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.21.mlp.layers.0.bias', 'image_encoder.trunk.blocks.15.attn.proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'image_encoder.trunk.blocks.5.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.17.attn.proj.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.8.mlp.layers.0.bias', 'memory_encoder.mask_downsampler.encoder.6.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias', 'memory_attention.layers.2.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.13.norm1.bias', 'image_encoder.neck.convs.2.conv.bias', 'image_encoder.trunk.blocks.22.mlp.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'obj_ptr_proj.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.1.bias', 'image_encoder.trunk.blocks.1.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias', 'image_encoder.trunk.blocks.21.attn.proj.bias', 'image_encoder.trunk.patch_embed.proj.bias', 'image_encoder.trunk.blocks.18.mlp.layers.1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias', 'sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.16.attn.qkv.bias', 'memory_encoder.mask_downsampler.encoder.3.bias', 'image_encoder.trunk.blocks.13.attn.proj.bias', 'image_encoder.trunk.blocks.22.attn.proj.bias', 'image_encoder.trunk.blocks.23.norm2.bias', 'memory_attention.layers.3.linear2.bias', 'sam_mask_decoder.conv_s0.bias', 'image_encoder.trunk.blocks.7.attn.proj.bias', 'sam_mask_decoder.iou_prediction_head.layers.0.bias', 'image_encoder.trunk.blocks.9.mlp.layers.1.bias', 'image_encoder.trunk.blocks.19.mlp.layers.0.bias', 'memory_attention.layers.0.cross_attn_image.out_proj.bias', 'memory_encoder.mask_downsampler.encoder.9.bias', 'image_encoder.trunk.blocks.20.mlp.layers.0.bias', 'image_encoder.trunk.blocks.13.mlp.layers.0.bias', 'memory_attention.layers.1.self_attn.k_proj.bias', 'memory_attention.layers.0.self_attn.v_proj.bias', 'memory_attention.layers.0.cross_attn_image.v_proj.bias', 'memory_encoder.mask_downsampler.encoder.0.bias', 'memory_attention.layers.1.cross_attn_image.k_proj.bias', 'image_encoder.trunk.blocks.3.norm1.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.6.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias', 'image_encoder.trunk.blocks.12.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'image_encoder.trunk.blocks.21.mlp.layers.1.bias', 'memory_encoder.mask_downsampler.encoder.10.bias', 'memory_encoder.mask_downsampler.encoder.4.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.14.mlp.layers.1.bias', 'sam_mask_decoder.conv_s1.bias', 'mask_downsample.bias', 'image_encoder.trunk.blocks.2.proj.bias', 'image_encoder.trunk.blocks.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.22.mlp.layers.0.bias', 'image_encoder.trunk.blocks.18.attn.proj.bias', 'image_encoder.trunk.blocks.8.attn.proj.bias', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.0.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm1.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias', 'memory_attention.layers.0.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias', 'obj_ptr_tpos_proj.bias', 'image_encoder.trunk.blocks.9.attn.qkv.bias', 'image_encoder.trunk.blocks.14.attn.proj.bias', 'memory_attention.layers.0.linear1.bias', 'image_encoder.trunk.blocks.10.norm1.bias', 'memory_encoder.fuser.layers.1.pwconv2.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.0.attn.qkv.bias', 'sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias', 'image_encoder.trunk.blocks.4.attn.proj.bias', 'image_encoder.trunk.blocks.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.13.attn.qkv.bias', 'memory_encoder.fuser.layers.0.dwconv.bias', 'image_encoder.trunk.blocks.0.attn.proj.bias', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.21.attn.qkv.bias', 'memory_attention.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'memory_attention.layers.3.norm2.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'image_encoder.trunk.blocks.12.mlp.layers.1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.0.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'memory_attention.layers.1.self_attn.out_proj.bias', 'image_encoder.trunk.blocks.12.norm1.bias', 'memory_attention.layers.1.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.21.proj.bias', 'memory_attention.layers.2.cross_attn_image.out_proj.bias', 'image_encoder.trunk.blocks.12.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.18.attn.qkv.bias', 'sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.3.mlp.layers.0.bias', 'image_encoder.trunk.blocks.2.attn.qkv.bias', 'memory_encoder.fuser.layers.0.norm.bias', 'image_encoder.trunk.blocks.16.norm1.bias', 'memory_attention.layers.0.norm2.bias', 'memory_attention.layers.2.self_attn.v_proj.bias', 'memory_attention.layers.1.linear2.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias', 'memory_encoder.fuser.layers.1.dwconv.bias', 'image_encoder.trunk.blocks.11.attn.qkv.bias', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.6.mlp.layers.0.bias', 'sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias', 'image_encoder.trunk.blocks.0.norm1.bias', 'sam_mask_decoder.pred_obj_score_head.layers.2.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'image_encoder.trunk.blocks.6.attn.proj.bias', 'image_encoder.trunk.blocks.8.attn.qkv.bias', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.0.bias', 'memory_attention.layers.0.self_attn.q_proj.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias', 'image_encoder.trunk.blocks.14.mlp.layers.0.bias', 'image_encoder.trunk.blocks.7.mlp.layers.0.bias', 'image_encoder.trunk.blocks.16.mlp.layers.1.bias', 'sam_prompt_encoder.mask_downscaling.1.bias', 'image_encoder.trunk.blocks.4.mlp.layers.1.bias', 'memory_attention.layers.2.linear1.bias', 'memory_attention.layers.3.self_attn.q_proj.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.20.mlp.layers.1.bias', 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias', 'sam_mask_decoder.output_upscaling.0.bias', 'memory_attention.layers.2.norm3.bias', 'image_encoder.trunk.blocks.23.mlp.layers.0.bias', 'image_encoder.trunk.blocks.17.mlp.layers.1.bias', 'memory_attention.layers.0.linear2.bias', 'image_encoder.trunk.blocks.22.norm1.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias', 'memory_attention.layers.1.linear1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'sam_prompt_encoder.mask_downscaling.6.bias', 'memory_attention.layers.2.linear2.bias', 'image_encoder.trunk.blocks.3.attn.qkv.bias', 'image_encoder.trunk.blocks.2.norm1.bias', 'image_encoder.trunk.blocks.9.attn.proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias', 'image_encoder.trunk.blocks.19.mlp.layers.1.bias', 'image_encoder.trunk.blocks.11.attn.proj.bias', 'memory_attention.layers.2.cross_attn_image.q_proj.bias', 'memory_encoder.fuser.layers.0.pwconv2.bias', 'sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias', 'sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias', 'image_encoder.trunk.blocks.18.norm2.bias', 'sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias', 'memory_attention.layers.1.self_attn.v_proj.bias', 'image_encoder.neck.convs.3.conv.bias', 'image_encoder.trunk.blocks.23.attn.qkv.bias', 'memory_attention.layers.3.cross_attn_image.k_proj.bias'}
INFO 2025-08-12 00:55:42,437 optimizer.py: 220: Matches for module_cls_name [torch.nn.LayerNorm]: {'image_encoder.trunk.blocks.4.norm1.weight', 'image_encoder.trunk.blocks.5.norm1.weight', 'image_encoder.trunk.blocks.14.norm2.bias', 'image_encoder.trunk.blocks.9.norm2.bias', 'image_encoder.trunk.blocks.11.norm2.weight', 'image_encoder.trunk.blocks.21.norm2.bias', 'memory_attention.layers.1.norm1.bias', 'image_encoder.trunk.blocks.10.norm2.weight', 'image_encoder.trunk.blocks.16.norm2.bias', 'memory_attention.layers.0.norm2.weight', 'memory_attention.layers.2.norm2.bias', 'memory_attention.layers.0.norm1.bias', 'image_encoder.trunk.blocks.3.norm2.weight', 'image_encoder.trunk.blocks.4.norm2.bias', 'image_encoder.trunk.blocks.15.norm2.bias', 'memory_attention.layers.0.norm3.weight', 'memory_attention.norm.bias', 'image_encoder.trunk.blocks.13.norm1.weight', 'image_encoder.trunk.blocks.17.norm2.weight', 'image_encoder.trunk.blocks.14.norm1.weight', 'image_encoder.trunk.blocks.18.norm1.weight', 'memory_attention.layers.3.norm3.bias', 'image_encoder.trunk.blocks.8.norm1.bias', 'image_encoder.trunk.blocks.2.norm2.weight', 'image_encoder.trunk.blocks.15.norm1.weight', 'image_encoder.trunk.blocks.22.norm1.weight', 'memory_attention.layers.1.norm2.bias', 'image_encoder.trunk.blocks.5.norm1.bias', 'image_encoder.trunk.blocks.20.norm2.bias', 'image_encoder.trunk.blocks.6.norm2.weight', 'image_encoder.trunk.blocks.23.norm1.bias', 'image_encoder.trunk.blocks.14.norm1.bias', 'sam_mask_decoder.transformer.layers.1.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm1.weight', 'image_encoder.trunk.blocks.10.norm2.bias', 'sam_mask_decoder.transformer.layers.1.norm4.weight', 'image_encoder.trunk.blocks.22.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm3.bias', 'memory_attention.layers.2.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm3.weight', 'image_encoder.trunk.blocks.1.norm2.weight', 'memory_attention.layers.0.norm1.weight', 'image_encoder.trunk.blocks.11.norm1.bias', 'image_encoder.trunk.blocks.19.norm2.bias', 'image_encoder.trunk.blocks.17.norm2.bias', 'sam_mask_decoder.transformer.layers.0.norm2.weight', 'image_encoder.trunk.blocks.16.norm2.weight', 'image_encoder.trunk.blocks.8.norm2.bias', 'image_encoder.trunk.blocks.17.norm1.bias', 'memory_attention.layers.1.norm3.bias', 'sam_mask_decoder.transformer.layers.0.norm4.weight', 'sam_mask_decoder.transformer.layers.0.norm4.bias', 'memory_attention.norm.weight', 'sam_mask_decoder.transformer.layers.1.norm4.bias', 'image_encoder.trunk.blocks.21.norm1.weight', 'image_encoder.trunk.blocks.3.norm2.bias', 'image_encoder.trunk.blocks.8.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.weight', 'image_encoder.trunk.blocks.13.norm1.bias', 'sam_mask_decoder.transformer.layers.0.norm2.bias', 'image_encoder.trunk.blocks.23.norm1.weight', 'image_encoder.trunk.blocks.17.norm1.weight', 'image_encoder.trunk.blocks.18.norm2.weight', 'image_encoder.trunk.blocks.18.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.weight', 'image_encoder.trunk.blocks.19.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm2.weight', 'image_encoder.trunk.blocks.4.norm2.weight', 'image_encoder.trunk.blocks.23.norm2.bias', 'memory_attention.layers.1.norm3.weight', 'image_encoder.trunk.blocks.2.norm1.weight', 'image_encoder.trunk.blocks.3.norm1.bias', 'image_encoder.trunk.blocks.0.norm2.bias', 'image_encoder.trunk.blocks.13.norm2.weight', 'sam_mask_decoder.transformer.norm_final_attn.bias', 'image_encoder.trunk.blocks.11.norm1.weight', 'memory_attention.layers.2.norm1.weight', 'sam_mask_decoder.transformer.layers.1.norm3.bias', 'memory_attention.layers.3.norm1.bias', 'image_encoder.trunk.blocks.14.norm2.weight', 'image_encoder.trunk.blocks.7.norm1.bias', 'image_encoder.trunk.blocks.3.norm1.weight', 'memory_attention.layers.3.norm1.weight', 'image_encoder.trunk.blocks.5.norm2.weight', 'image_encoder.trunk.blocks.12.norm2.bias', 'image_encoder.trunk.blocks.0.norm1.weight', 'image_encoder.trunk.blocks.7.norm2.weight', 'image_encoder.trunk.blocks.6.norm1.bias', 'image_encoder.trunk.blocks.15.norm2.weight', 'image_encoder.trunk.blocks.12.norm2.weight', 'image_encoder.trunk.blocks.10.norm1.bias', 'image_encoder.trunk.blocks.19.norm1.bias', 'image_encoder.trunk.blocks.9.norm2.weight', 'image_encoder.trunk.blocks.11.norm2.bias', 'image_encoder.trunk.blocks.12.norm1.weight', 'memory_attention.layers.0.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm2.bias', 'image_encoder.trunk.blocks.13.norm2.bias', 'image_encoder.trunk.blocks.2.norm2.bias', 'memory_attention.layers.3.norm2.bias', 'memory_attention.layers.1.norm1.weight', 'sam_mask_decoder.transformer.norm_final_attn.weight', 'image_encoder.trunk.blocks.7.norm1.weight', 'image_encoder.trunk.blocks.12.norm1.bias', 'image_encoder.trunk.blocks.15.norm1.bias', 'image_encoder.trunk.blocks.22.norm2.bias', 'memory_attention.layers.3.norm3.weight', 'image_encoder.trunk.blocks.21.norm2.weight', 'sam_mask_decoder.transformer.layers.0.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.weight', 'image_encoder.trunk.blocks.16.norm1.bias', 'memory_attention.layers.0.norm2.bias', 'memory_attention.layers.2.norm3.weight', 'image_encoder.trunk.blocks.6.norm2.bias', 'image_encoder.trunk.blocks.6.norm1.weight', 'image_encoder.trunk.blocks.0.norm1.bias', 'image_encoder.trunk.blocks.5.norm2.bias', 'memory_attention.layers.2.norm2.weight', 'image_encoder.trunk.blocks.8.norm1.weight', 'image_encoder.trunk.blocks.21.norm1.bias', 'image_encoder.trunk.blocks.9.norm1.bias', 'image_encoder.trunk.blocks.1.norm1.bias', 'image_encoder.trunk.blocks.20.norm1.weight', 'memory_attention.layers.2.norm3.bias', 'sam_mask_decoder.transformer.layers.1.norm1.weight', 'image_encoder.trunk.blocks.19.norm2.weight', 'image_encoder.trunk.blocks.22.norm1.bias', 'image_encoder.trunk.blocks.1.norm2.bias', 'image_encoder.trunk.blocks.7.norm2.bias', 'image_encoder.trunk.blocks.4.norm1.bias', 'image_encoder.trunk.blocks.20.norm1.bias', 'image_encoder.trunk.blocks.10.norm1.weight', 'image_encoder.trunk.blocks.2.norm1.bias', 'memory_attention.layers.1.norm2.weight', 'image_encoder.trunk.blocks.20.norm2.weight', 'sam_mask_decoder.transformer.layers.1.norm3.weight', 'image_encoder.trunk.blocks.16.norm1.weight', 'image_encoder.trunk.blocks.18.norm2.bias', 'image_encoder.trunk.blocks.0.norm2.weight', 'memory_attention.layers.3.norm2.weight'} 
INFO 2025-08-12 00:55:42,900 sam2_datasets.py: 125: Dataset mixing probabilities: [1.0]
INFO 2025-08-12 00:55:43,728 trainer.py: 417: Loading pretrained checkpoint from {'_partial_': True, '_target_': 'training.utils.checkpoint_utils.load_state_dict_into_model', 'strict': True, 'ignore_unexpected_keys': None, 'ignore_missing_keys': None, 'state_dict': {'_target_': 'training.utils.checkpoint_utils.load_checkpoint_and_apply_kernels', 'checkpoint_path': '/home/guest/tangxiangkai/sam2/sam2_logs/configs/sam2.1_training/sam2.1_hiera_b+_tzb_finetune.yaml/checkpoints/checkpoint.pt', 'ckpt_state_dict_keys': ['model']}}
INFO 2025-08-12 00:55:56,794 train_utils.py: 271: Train Epoch: [0][  0/136] | Batch Time: 12.62 (12.62) | Data Time: 8.46 (8.46) | Mem (GB): 16.00 (16.00/16.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 3.93e-01 (3.93e-01)
INFO 2025-08-12 00:56:09,259 train_utils.py: 271: Train Epoch: [0][ 10/136] | Batch Time: 1.40 (2.28) | Data Time: 0.00 (0.77) | Mem (GB): 21.00 (19.18/23.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 2.60e+00 (1.33e+00)
INFO 2025-08-12 00:56:20,340 train_utils.py: 271: Train Epoch: [0][ 20/136] | Batch Time: 1.04 (1.72) | Data Time: 0.00 (0.40) | Mem (GB): 19.00 (19.29/23.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 4.60e-01 (1.31e+00)
INFO 2025-08-12 00:56:31,914 train_utils.py: 271: Train Epoch: [0][ 30/136] | Batch Time: 1.04 (1.54) | Data Time: 0.00 (0.27) | Mem (GB): 19.00 (19.81/24.00) | Time Elapsed: 00d 00h 00m | Losses/train_all_loss: 4.35e-01 (1.51e+00)
INFO 2025-08-12 00:56:43,025 train_utils.py: 271: Train Epoch: [0][ 40/136] | Batch Time: 0.89 (1.44) | Data Time: 0.00 (0.21) | Mem (GB): 17.00 (19.73/24.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 3.23e-01 (1.54e+00)
INFO 2025-08-12 00:56:54,152 train_utils.py: 271: Train Epoch: [0][ 50/136] | Batch Time: 1.58 (1.37) | Data Time: 0.00 (0.17) | Mem (GB): 22.00 (19.47/24.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 3.26e+00 (1.47e+00)
INFO 2025-08-12 00:57:06,498 train_utils.py: 271: Train Epoch: [0][ 60/136] | Batch Time: 0.94 (1.35) | Data Time: 0.00 (0.14) | Mem (GB): 17.00 (19.54/24.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 4.20e-01 (1.55e+00)
INFO 2025-08-12 00:57:17,374 train_utils.py: 271: Train Epoch: [0][ 70/136] | Batch Time: 1.09 (1.31) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (19.42/24.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 7.19e-01 (1.44e+00)
INFO 2025-08-12 00:57:29,999 train_utils.py: 271: Train Epoch: [0][ 80/136] | Batch Time: 1.16 (1.31) | Data Time: 0.00 (0.11) | Mem (GB): 19.00 (19.46/24.00) | Time Elapsed: 00d 00h 01m | Losses/train_all_loss: 4.98e-01 (1.44e+00)
INFO 2025-08-12 00:57:41,661 train_utils.py: 271: Train Epoch: [0][ 90/136] | Batch Time: 0.93 (1.29) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (19.46/24.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 3.54e-01 (1.44e+00)
INFO 2025-08-12 00:57:52,930 train_utils.py: 271: Train Epoch: [0][100/136] | Batch Time: 1.43 (1.27) | Data Time: 0.00 (0.08) | Mem (GB): 23.00 (19.45/24.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 3.47e+00 (1.42e+00)
INFO 2025-08-12 00:58:05,170 train_utils.py: 271: Train Epoch: [0][110/136] | Batch Time: 1.36 (1.27) | Data Time: 0.00 (0.08) | Mem (GB): 24.00 (19.57/24.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 2.58e+00 (1.46e+00)
INFO 2025-08-12 00:58:17,108 train_utils.py: 271: Train Epoch: [0][120/136] | Batch Time: 1.33 (1.26) | Data Time: 0.00 (0.07) | Mem (GB): 21.00 (19.55/24.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 1.97e+00 (1.44e+00)
INFO 2025-08-12 00:58:29,597 train_utils.py: 271: Train Epoch: [0][130/136] | Batch Time: 1.09 (1.26) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (19.60/24.00) | Time Elapsed: 00d 00h 02m | Losses/train_all_loss: 4.59e-01 (1.45e+00)
INFO 2025-08-12 00:58:37,088 trainer.py: 950: Estimated time remaining: 00d 01h 51m
INFO 2025-08-12 00:58:37,090 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 00:58:37,090 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4531588173087906, 'Losses/train_all_loss_mask': 0.006137107700958629, 'Losses/train_all_loss_dice': 5.439212203025818, 'Losses/train_all_loss_iou': 0.6764562967616845, 'Losses/train_all_loss_class': 0.015226332822852755, 'Losses/train_all_core_loss': 1.4531588173087906, 'Trainer/where': 0.024816176470588237, 'Trainer/epoch': 0, 'Trainer/steps_train': 136}
INFO 2025-08-12 00:58:48,301 train_utils.py: 271: Train Epoch: [1][  0/136] | Batch Time: 9.80 (9.80) | Data Time: 7.05 (7.05) | Mem (GB): 24.00 (24.00/24.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.76e+00 (2.76e+00)
INFO 2025-08-12 00:58:59,207 train_utils.py: 271: Train Epoch: [1][ 10/136] | Batch Time: 0.91 (1.88) | Data Time: 0.00 (0.64) | Mem (GB): 17.00 (20.00/24.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 2.95e-01 (1.64e+00)
INFO 2025-08-12 00:59:10,919 train_utils.py: 271: Train Epoch: [1][ 20/136] | Batch Time: 1.24 (1.54) | Data Time: 0.00 (0.34) | Mem (GB): 21.00 (19.95/24.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 1.34e+00 (1.61e+00)
INFO 2025-08-12 00:59:22,255 train_utils.py: 271: Train Epoch: [1][ 30/136] | Batch Time: 0.94 (1.41) | Data Time: 0.00 (0.23) | Mem (GB): 17.00 (19.84/24.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 3.03e-01 (1.42e+00)
INFO 2025-08-12 00:59:34,113 train_utils.py: 271: Train Epoch: [1][ 40/136] | Batch Time: 0.96 (1.36) | Data Time: 0.00 (0.17) | Mem (GB): 17.00 (19.93/24.00) | Time Elapsed: 00d 00h 03m | Losses/train_all_loss: 3.16e-01 (1.50e+00)
INFO 2025-08-12 00:59:45,250 train_utils.py: 271: Train Epoch: [1][ 50/136] | Batch Time: 1.27 (1.31) | Data Time: 0.00 (0.14) | Mem (GB): 21.00 (19.78/24.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 1.65e+00 (1.41e+00)
INFO 2025-08-12 00:59:57,226 train_utils.py: 271: Train Epoch: [1][ 60/136] | Batch Time: 1.37 (1.29) | Data Time: 0.00 (0.12) | Mem (GB): 24.00 (19.93/24.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 3.00e+00 (1.48e+00)
INFO 2025-08-12 01:00:09,446 train_utils.py: 271: Train Epoch: [1][ 70/136] | Batch Time: 1.10 (1.28) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (19.99/24.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 5.11e-01 (1.52e+00)
INFO 2025-08-12 01:00:20,573 train_utils.py: 271: Train Epoch: [1][ 80/136] | Batch Time: 0.94 (1.26) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (19.85/24.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 3.09e-01 (1.53e+00)
INFO 2025-08-12 01:00:31,850 train_utils.py: 271: Train Epoch: [1][ 90/136] | Batch Time: 1.27 (1.25) | Data Time: 0.00 (0.08) | Mem (GB): 22.00 (19.82/24.00) | Time Elapsed: 00d 00h 04m | Losses/train_all_loss: 3.07e+00 (1.49e+00)
INFO 2025-08-12 01:00:43,825 train_utils.py: 271: Train Epoch: [1][100/136] | Batch Time: 0.92 (1.24) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.88/24.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 3.98e-01 (1.54e+00)
INFO 2025-08-12 01:00:55,670 train_utils.py: 271: Train Epoch: [1][110/136] | Batch Time: 1.17 (1.24) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.92/24.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 4.36e-01 (1.55e+00)
INFO 2025-08-12 01:01:08,526 train_utils.py: 271: Train Epoch: [1][120/136] | Batch Time: 1.08 (1.24) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (20.06/24.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 4.84e-01 (1.62e+00)
INFO 2025-08-12 01:01:19,157 train_utils.py: 271: Train Epoch: [1][130/136] | Batch Time: 1.20 (1.23) | Data Time: 0.00 (0.05) | Mem (GB): 19.00 (19.93/24.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 4.50e-01 (1.53e+00)
INFO 2025-08-12 01:01:26,420 trainer.py: 950: Estimated time remaining: 00d 01h 45m
INFO 2025-08-12 01:01:26,422 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:01:26,423 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.506555120296338, 'Losses/train_all_loss_mask': 0.004756253498378689, 'Losses/train_all_loss_dice': 5.907863891300033, 'Losses/train_all_loss_iou': 0.6764020335805767, 'Losses/train_all_loss_class': 0.02323304808742384, 'Losses/train_all_core_loss': 1.506555120296338, 'Trainer/where': 0.04981617647058824, 'Trainer/epoch': 1, 'Trainer/steps_train': 272}
INFO 2025-08-12 01:01:36,558 train_utils.py: 271: Train Epoch: [2][  0/136] | Batch Time: 8.66 (8.66) | Data Time: 6.62 (6.62) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 00h 05m | Losses/train_all_loss: 3.16e+00 (3.16e+00)
INFO 2025-08-12 01:01:48,389 train_utils.py: 271: Train Epoch: [2][ 10/136] | Batch Time: 1.15 (1.86) | Data Time: 0.00 (0.60) | Mem (GB): 19.00 (19.91/23.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 8.19e-01 (1.61e+00)
INFO 2025-08-12 01:02:01,056 train_utils.py: 271: Train Epoch: [2][ 20/136] | Batch Time: 1.08 (1.58) | Data Time: 0.00 (0.32) | Mem (GB): 19.00 (20.62/24.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 3.89e-01 (1.77e+00)
INFO 2025-08-12 01:02:12,678 train_utils.py: 271: Train Epoch: [2][ 30/136] | Batch Time: 0.95 (1.44) | Data Time: 0.00 (0.21) | Mem (GB): 17.00 (20.52/24.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 3.03e-01 (1.81e+00)
INFO 2025-08-12 01:02:23,418 train_utils.py: 271: Train Epoch: [2][ 40/136] | Batch Time: 0.95 (1.35) | Data Time: 0.00 (0.16) | Mem (GB): 17.00 (20.10/24.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 4.12e-01 (1.63e+00)
INFO 2025-08-12 01:02:34,799 train_utils.py: 271: Train Epoch: [2][ 50/136] | Batch Time: 0.92 (1.31) | Data Time: 0.00 (0.13) | Mem (GB): 17.00 (19.96/24.00) | Time Elapsed: 00d 00h 06m | Losses/train_all_loss: 3.73e-01 (1.56e+00)
INFO 2025-08-12 01:02:46,187 train_utils.py: 271: Train Epoch: [2][ 60/136] | Batch Time: 1.10 (1.28) | Data Time: 0.00 (0.11) | Mem (GB): 19.00 (19.87/24.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 5.29e-01 (1.42e+00)
INFO 2025-08-12 01:02:57,947 train_utils.py: 271: Train Epoch: [2][ 70/136] | Batch Time: 1.38 (1.27) | Data Time: 0.00 (0.09) | Mem (GB): 24.00 (19.90/24.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 3.31e+00 (1.45e+00)
INFO 2025-08-12 01:03:09,963 train_utils.py: 271: Train Epoch: [2][ 80/136] | Batch Time: 1.72 (1.26) | Data Time: 0.00 (0.08) | Mem (GB): 23.00 (19.95/24.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 3.37e+00 (1.51e+00)
INFO 2025-08-12 01:03:20,481 train_utils.py: 271: Train Epoch: [2][ 90/136] | Batch Time: 1.31 (1.24) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.78/24.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 2.96e+00 (1.46e+00)
INFO 2025-08-12 01:03:31,960 train_utils.py: 271: Train Epoch: [2][100/136] | Batch Time: 1.29 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 21.00 (19.77/24.00) | Time Elapsed: 00d 00h 07m | Losses/train_all_loss: 2.13e+00 (1.44e+00)
INFO 2025-08-12 01:03:43,250 train_utils.py: 271: Train Epoch: [2][110/136] | Batch Time: 1.10 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 16.00 (19.71/24.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 3.05e-01 (1.39e+00)
INFO 2025-08-12 01:03:54,643 train_utils.py: 271: Train Epoch: [2][120/136] | Batch Time: 1.35 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 24.00 (19.74/24.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 3.49e+00 (1.41e+00)
INFO 2025-08-12 01:04:06,423 train_utils.py: 271: Train Epoch: [2][130/136] | Batch Time: 1.37 (1.21) | Data Time: 0.00 (0.05) | Mem (GB): 21.00 (19.80/24.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 1.94e+00 (1.46e+00)
INFO 2025-08-12 01:04:13,613 trainer.py: 950: Estimated time remaining: 00d 01h 41m
INFO 2025-08-12 01:04:13,615 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:04:13,616 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4423253663322504, 'Losses/train_all_loss_mask': 0.004590972593552365, 'Losses/train_all_loss_dice': 5.725774893865866, 'Losses/train_all_loss_iou': 0.6809838118634242, 'Losses/train_all_loss_class': 0.00028955500794622553, 'Losses/train_all_core_loss': 1.4423253663322504, 'Trainer/where': 0.07481617647058823, 'Trainer/epoch': 2, 'Trainer/steps_train': 408}
INFO 2025-08-12 01:04:24,703 train_utils.py: 271: Train Epoch: [3][  0/136] | Batch Time: 9.59 (9.59) | Data Time: 5.05 (5.05) | Mem (GB): 21.00 (21.00/21.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 1.49e+00 (1.49e+00)
INFO 2025-08-12 01:04:36,679 train_utils.py: 271: Train Epoch: [3][ 10/136] | Batch Time: 0.95 (1.96) | Data Time: 0.00 (0.46) | Mem (GB): 17.00 (19.64/23.00) | Time Elapsed: 00d 00h 08m | Losses/train_all_loss: 3.31e-01 (1.24e+00)
INFO 2025-08-12 01:04:47,330 train_utils.py: 271: Train Epoch: [3][ 20/136] | Batch Time: 0.93 (1.53) | Data Time: 0.00 (0.24) | Mem (GB): 17.00 (19.14/23.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 3.36e-01 (1.07e+00)
INFO 2025-08-12 01:04:59,073 train_utils.py: 271: Train Epoch: [3][ 30/136] | Batch Time: 1.27 (1.42) | Data Time: 0.00 (0.16) | Mem (GB): 22.00 (19.65/24.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 3.99e+00 (1.48e+00)
INFO 2025-08-12 01:05:10,759 train_utils.py: 271: Train Epoch: [3][ 40/136] | Batch Time: 1.23 (1.36) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (19.71/24.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 1.80e+00 (1.58e+00)
INFO 2025-08-12 01:05:22,368 train_utils.py: 271: Train Epoch: [3][ 50/136] | Batch Time: 1.32 (1.32) | Data Time: 0.00 (0.10) | Mem (GB): 18.00 (19.67/24.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 9.16e-01 (1.54e+00)
INFO 2025-08-12 01:05:33,591 train_utils.py: 271: Train Epoch: [3][ 60/136] | Batch Time: 1.28 (1.29) | Data Time: 0.00 (0.08) | Mem (GB): 22.00 (19.67/24.00) | Time Elapsed: 00d 00h 09m | Losses/train_all_loss: 2.78e+00 (1.56e+00)
INFO 2025-08-12 01:05:44,714 train_utils.py: 271: Train Epoch: [3][ 70/136] | Batch Time: 1.27 (1.26) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.66/24.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 2.90e+00 (1.53e+00)
INFO 2025-08-12 01:05:56,594 train_utils.py: 271: Train Epoch: [3][ 80/136] | Batch Time: 1.09 (1.25) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.74/24.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 8.72e-01 (1.59e+00)
INFO 2025-08-12 01:06:07,909 train_utils.py: 271: Train Epoch: [3][ 90/136] | Batch Time: 1.10 (1.24) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.73/24.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 4.12e-01 (1.54e+00)
INFO 2025-08-12 01:06:19,951 train_utils.py: 271: Train Epoch: [3][100/136] | Batch Time: 1.11 (1.24) | Data Time: 0.00 (0.05) | Mem (GB): 16.00 (19.77/24.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 3.38e-01 (1.56e+00)
INFO 2025-08-12 01:06:31,366 train_utils.py: 271: Train Epoch: [3][110/136] | Batch Time: 1.10 (1.23) | Data Time: 0.00 (0.05) | Mem (GB): 19.00 (19.77/24.00) | Time Elapsed: 00d 00h 10m | Losses/train_all_loss: 4.05e-01 (1.54e+00)
INFO 2025-08-12 01:06:42,945 train_utils.py: 271: Train Epoch: [3][120/136] | Batch Time: 1.25 (1.22) | Data Time: 0.00 (0.04) | Mem (GB): 22.00 (19.74/24.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 2.76e+00 (1.51e+00)
INFO 2025-08-12 01:06:54,797 train_utils.py: 271: Train Epoch: [3][130/136] | Batch Time: 1.23 (1.22) | Data Time: 0.00 (0.04) | Mem (GB): 22.00 (19.79/24.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 2.98e+00 (1.57e+00)
INFO 2025-08-12 01:07:02,891 trainer.py: 950: Estimated time remaining: 00d 01h 39m
INFO 2025-08-12 01:07:02,893 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:07:02,893 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5874753127641537, 'Losses/train_all_loss_mask': 0.004685484614396153, 'Losses/train_all_loss_dice': 6.183142981108497, 'Losses/train_all_loss_iou': 0.6856935697552913, 'Losses/train_all_loss_class': 0.04971600874378456, 'Losses/train_all_core_loss': 1.5874753127641537, 'Trainer/where': 0.09981617647058824, 'Trainer/epoch': 3, 'Trainer/steps_train': 544}
INFO 2025-08-12 01:07:13,120 train_utils.py: 271: Train Epoch: [4][  0/136] | Batch Time: 8.76 (8.76) | Data Time: 6.76 (6.76) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 4.23e-01 (4.23e-01)
INFO 2025-08-12 01:07:23,151 train_utils.py: 271: Train Epoch: [4][ 10/136] | Batch Time: 0.90 (1.71) | Data Time: 0.00 (0.62) | Mem (GB): 17.00 (18.27/22.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 3.26e-01 (6.90e-01)
INFO 2025-08-12 01:07:35,006 train_utils.py: 271: Train Epoch: [4][ 20/136] | Batch Time: 1.20 (1.46) | Data Time: 0.00 (0.32) | Mem (GB): 21.00 (19.67/24.00) | Time Elapsed: 00d 00h 11m | Losses/train_all_loss: 1.29e+00 (1.41e+00)
INFO 2025-08-12 01:07:46,865 train_utils.py: 271: Train Epoch: [4][ 30/136] | Batch Time: 1.38 (1.37) | Data Time: 0.00 (0.22) | Mem (GB): 23.00 (19.94/24.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 3.09e+00 (1.47e+00)
INFO 2025-08-12 01:07:58,967 train_utils.py: 271: Train Epoch: [4][ 40/136] | Batch Time: 1.07 (1.33) | Data Time: 0.00 (0.17) | Mem (GB): 19.00 (20.20/24.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 4.54e-01 (1.60e+00)
INFO 2025-08-12 01:08:10,853 train_utils.py: 271: Train Epoch: [4][ 50/136] | Batch Time: 0.92 (1.30) | Data Time: 0.00 (0.13) | Mem (GB): 17.00 (20.24/24.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 3.42e-01 (1.63e+00)
INFO 2025-08-12 01:08:22,350 train_utils.py: 271: Train Epoch: [4][ 60/136] | Batch Time: 0.92 (1.28) | Data Time: 0.00 (0.11) | Mem (GB): 17.00 (20.20/24.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 3.08e-01 (1.66e+00)
INFO 2025-08-12 01:08:33,338 train_utils.py: 271: Train Epoch: [4][ 70/136] | Batch Time: 0.92 (1.25) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (20.08/24.00) | Time Elapsed: 00d 00h 12m | Losses/train_all_loss: 2.87e-01 (1.59e+00)
INFO 2025-08-12 01:08:44,942 train_utils.py: 271: Train Epoch: [4][ 80/136] | Batch Time: 1.09 (1.24) | Data Time: 0.00 (0.08) | Mem (GB): 16.00 (19.98/24.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 3.12e-01 (1.69e+00)
INFO 2025-08-12 01:08:56,377 train_utils.py: 271: Train Epoch: [4][ 90/136] | Batch Time: 1.25 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.97/24.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 3.06e+00 (1.70e+00)
INFO 2025-08-12 01:09:08,763 train_utils.py: 271: Train Epoch: [4][100/136] | Batch Time: 1.41 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 23.00 (20.09/24.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 2.20e+00 (1.71e+00)
INFO 2025-08-12 01:09:20,145 train_utils.py: 271: Train Epoch: [4][110/136] | Batch Time: 1.09 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (20.06/24.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 4.02e-01 (1.68e+00)
INFO 2025-08-12 01:09:31,637 train_utils.py: 271: Train Epoch: [4][120/136] | Batch Time: 1.11 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (20.02/24.00) | Time Elapsed: 00d 00h 13m | Losses/train_all_loss: 5.94e-01 (1.67e+00)
INFO 2025-08-12 01:09:43,073 train_utils.py: 271: Train Epoch: [4][130/136] | Batch Time: 1.10 (1.21) | Data Time: 0.00 (0.05) | Mem (GB): 19.00 (20.00/24.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 4.46e-01 (1.64e+00)
INFO 2025-08-12 01:09:49,960 trainer.py: 950: Estimated time remaining: 00d 01h 35m
INFO 2025-08-12 01:09:49,962 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:09:49,962 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.6143893159049398, 'Losses/train_all_loss_mask': 0.005468622758117443, 'Losses/train_all_loss_dice': 6.256621535648318, 'Losses/train_all_loss_iou': 0.6697373715612818, 'Losses/train_all_loss_class': 0.03771572699782202, 'Losses/train_all_core_loss': 1.6143893159049398, 'Trainer/where': 0.12481617647058822, 'Trainer/epoch': 4, 'Trainer/steps_train': 680}
INFO 2025-08-12 01:10:02,075 train_utils.py: 271: Train Epoch: [5][  0/136] | Batch Time: 9.08 (9.08) | Data Time: 7.83 (7.83) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 4.41e-01 (4.41e-01)
INFO 2025-08-12 01:10:13,329 train_utils.py: 271: Train Epoch: [5][ 10/136] | Batch Time: 0.90 (1.85) | Data Time: 0.00 (0.71) | Mem (GB): 17.00 (20.27/23.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 2.81e-01 (1.80e+00)
INFO 2025-08-12 01:10:24,338 train_utils.py: 271: Train Epoch: [5][ 20/136] | Batch Time: 1.23 (1.49) | Data Time: 0.00 (0.37) | Mem (GB): 22.00 (19.76/23.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 2.92e+00 (1.77e+00)
INFO 2025-08-12 01:10:35,542 train_utils.py: 271: Train Epoch: [5][ 30/136] | Batch Time: 0.93 (1.37) | Data Time: 0.00 (0.25) | Mem (GB): 17.00 (19.77/24.00) | Time Elapsed: 00d 00h 14m | Losses/train_all_loss: 3.32e-01 (1.58e+00)
INFO 2025-08-12 01:10:46,762 train_utils.py: 271: Train Epoch: [5][ 40/136] | Batch Time: 1.10 (1.31) | Data Time: 0.00 (0.19) | Mem (GB): 19.00 (19.80/24.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 4.70e-01 (1.56e+00)
INFO 2025-08-12 01:10:58,256 train_utils.py: 271: Train Epoch: [5][ 50/136] | Batch Time: 1.08 (1.28) | Data Time: 0.00 (0.15) | Mem (GB): 19.00 (19.86/24.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 4.64e-01 (1.59e+00)
INFO 2025-08-12 01:11:10,111 train_utils.py: 271: Train Epoch: [5][ 60/136] | Batch Time: 0.92 (1.26) | Data Time: 0.00 (0.13) | Mem (GB): 17.00 (19.97/24.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 3.52e-01 (1.61e+00)
INFO 2025-08-12 01:11:21,278 train_utils.py: 271: Train Epoch: [5][ 70/136] | Batch Time: 0.95 (1.24) | Data Time: 0.00 (0.11) | Mem (GB): 17.00 (19.90/24.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 4.45e-01 (1.60e+00)
INFO 2025-08-12 01:11:32,138 train_utils.py: 271: Train Epoch: [5][ 80/136] | Batch Time: 0.93 (1.22) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (19.80/24.00) | Time Elapsed: 00d 00h 15m | Losses/train_all_loss: 3.52e-01 (1.53e+00)
INFO 2025-08-12 01:11:43,654 train_utils.py: 271: Train Epoch: [5][ 90/136] | Batch Time: 1.41 (1.22) | Data Time: 0.00 (0.09) | Mem (GB): 23.00 (19.78/24.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 5.64e+00 (1.55e+00)
INFO 2025-08-12 01:11:54,790 train_utils.py: 271: Train Epoch: [5][100/136] | Batch Time: 1.31 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 22.00 (19.71/24.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 3.11e+00 (1.51e+00)
INFO 2025-08-12 01:12:06,370 train_utils.py: 271: Train Epoch: [5][110/136] | Batch Time: 1.27 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.76/24.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 2.43e+00 (1.51e+00)
INFO 2025-08-12 01:12:17,851 train_utils.py: 271: Train Epoch: [5][120/136] | Batch Time: 1.25 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 21.00 (19.77/24.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 2.05e+00 (1.50e+00)
INFO 2025-08-12 01:12:29,570 train_utils.py: 271: Train Epoch: [5][130/136] | Batch Time: 1.34 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (19.81/24.00) | Time Elapsed: 00d 00h 16m | Losses/train_all_loss: 3.62e+00 (1.54e+00)
INFO 2025-08-12 01:12:36,466 trainer.py: 950: Estimated time remaining: 00d 01h 31m
INFO 2025-08-12 01:12:36,595 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:12:36,595 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5100043391918434, 'Losses/train_all_loss_mask': 0.004495181301749921, 'Losses/train_all_loss_dice': 5.780459702891462, 'Losses/train_all_loss_iou': 0.6371513160274309, 'Losses/train_all_loss_class': 0.06915077226694297, 'Losses/train_all_core_loss': 1.5100043391918434, 'Trainer/where': 0.14981617647058823, 'Trainer/epoch': 5, 'Trainer/steps_train': 816}
INFO 2025-08-12 01:12:46,987 train_utils.py: 271: Train Epoch: [6][  0/136] | Batch Time: 8.95 (8.95) | Data Time: 7.25 (7.25) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 6.02e-01 (6.02e-01)
INFO 2025-08-12 01:12:58,046 train_utils.py: 271: Train Epoch: [6][ 10/136] | Batch Time: 1.23 (1.82) | Data Time: 0.00 (0.66) | Mem (GB): 22.00 (20.18/24.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 3.21e+00 (1.57e+00)
INFO 2025-08-12 01:13:09,260 train_utils.py: 271: Train Epoch: [6][ 20/136] | Batch Time: 1.22 (1.49) | Data Time: 0.00 (0.35) | Mem (GB): 21.00 (20.00/24.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 1.79e+00 (1.62e+00)
INFO 2025-08-12 01:13:20,842 train_utils.py: 271: Train Epoch: [6][ 30/136] | Batch Time: 0.93 (1.38) | Data Time: 0.00 (0.23) | Mem (GB): 17.00 (20.00/24.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 2.84e-01 (1.59e+00)
INFO 2025-08-12 01:13:32,558 train_utils.py: 271: Train Epoch: [6][ 40/136] | Batch Time: 1.06 (1.33) | Data Time: 0.00 (0.18) | Mem (GB): 19.00 (20.02/24.00) | Time Elapsed: 00d 00h 17m | Losses/train_all_loss: 3.95e-01 (1.56e+00)
INFO 2025-08-12 01:13:44,021 train_utils.py: 271: Train Epoch: [6][ 50/136] | Batch Time: 0.91 (1.29) | Data Time: 0.00 (0.14) | Mem (GB): 17.00 (20.00/24.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 3.01e-01 (1.55e+00)
INFO 2025-08-12 01:13:55,782 train_utils.py: 271: Train Epoch: [6][ 60/136] | Batch Time: 1.42 (1.27) | Data Time: 0.00 (0.12) | Mem (GB): 23.00 (20.07/24.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 3.13e+00 (1.59e+00)
INFO 2025-08-12 01:14:08,190 train_utils.py: 271: Train Epoch: [6][ 70/136] | Batch Time: 1.31 (1.27) | Data Time: 0.00 (0.10) | Mem (GB): 22.00 (20.20/24.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 2.98e+00 (1.67e+00)
INFO 2025-08-12 01:14:18,947 train_utils.py: 271: Train Epoch: [6][ 80/136] | Batch Time: 1.37 (1.25) | Data Time: 0.00 (0.09) | Mem (GB): 23.00 (20.00/24.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 3.29e+00 (1.59e+00)
INFO 2025-08-12 01:14:30,278 train_utils.py: 271: Train Epoch: [6][ 90/136] | Batch Time: 0.93 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (19.92/24.00) | Time Elapsed: 00d 00h 18m | Losses/train_all_loss: 2.95e-01 (1.54e+00)
INFO 2025-08-12 01:14:41,868 train_utils.py: 271: Train Epoch: [6][100/136] | Batch Time: 1.42 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 23.00 (19.93/24.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 3.38e+00 (1.55e+00)
INFO 2025-08-12 01:14:53,918 train_utils.py: 271: Train Epoch: [6][110/136] | Batch Time: 1.25 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.01/24.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 3.47e+00 (1.62e+00)
INFO 2025-08-12 01:15:05,237 train_utils.py: 271: Train Epoch: [6][120/136] | Batch Time: 1.23 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.02/24.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 2.90e+00 (1.72e+00)
INFO 2025-08-12 01:15:16,865 train_utils.py: 271: Train Epoch: [6][130/136] | Batch Time: 1.27 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 21.00 (20.05/24.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 3.00e+00 (1.74e+00)
INFO 2025-08-12 01:15:24,047 trainer.py: 950: Estimated time remaining: 00d 01h 30m
INFO 2025-08-12 01:15:24,293 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:15:24,293 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.7443690828102476, 'Losses/train_all_loss_mask': 0.005401545435982852, 'Losses/train_all_loss_dice': 6.647099519477171, 'Losses/train_all_loss_iou': 0.6497164006235406, 'Losses/train_all_loss_class': 0.09595179795135025, 'Losses/train_all_core_loss': 1.7443690828102476, 'Trainer/where': 0.17481617647058822, 'Trainer/epoch': 6, 'Trainer/steps_train': 952}
INFO 2025-08-12 01:15:34,925 train_utils.py: 271: Train Epoch: [7][  0/136] | Batch Time: 9.16 (9.16) | Data Time: 7.72 (7.72) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 00h 19m | Losses/train_all_loss: 2.79e-01 (2.79e-01)
INFO 2025-08-12 01:15:46,582 train_utils.py: 271: Train Epoch: [7][ 10/136] | Batch Time: 1.05 (1.89) | Data Time: 0.00 (0.70) | Mem (GB): 19.00 (20.64/23.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 5.01e-01 (1.90e+00)
INFO 2025-08-12 01:15:57,721 train_utils.py: 271: Train Epoch: [7][ 20/136] | Batch Time: 1.07 (1.52) | Data Time: 0.00 (0.37) | Mem (GB): 16.00 (20.19/23.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 3.40e-01 (1.73e+00)
INFO 2025-08-12 01:16:08,558 train_utils.py: 271: Train Epoch: [7][ 30/136] | Batch Time: 1.08 (1.38) | Data Time: 0.00 (0.25) | Mem (GB): 19.00 (19.90/24.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 4.69e-01 (1.77e+00)
INFO 2025-08-12 01:16:19,438 train_utils.py: 271: Train Epoch: [7][ 40/136] | Batch Time: 1.35 (1.31) | Data Time: 0.00 (0.19) | Mem (GB): 24.00 (19.76/24.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 3.58e+00 (1.57e+00)
INFO 2025-08-12 01:16:30,557 train_utils.py: 271: Train Epoch: [7][ 50/136] | Batch Time: 1.24 (1.27) | Data Time: 0.00 (0.15) | Mem (GB): 22.00 (19.78/24.00) | Time Elapsed: 00d 00h 20m | Losses/train_all_loss: 3.02e+00 (1.62e+00)
INFO 2025-08-12 01:16:41,937 train_utils.py: 271: Train Epoch: [7][ 60/136] | Batch Time: 0.93 (1.25) | Data Time: 0.00 (0.13) | Mem (GB): 17.00 (19.82/24.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 2.66e-01 (1.60e+00)
INFO 2025-08-12 01:16:54,406 train_utils.py: 271: Train Epoch: [7][ 70/136] | Batch Time: 1.25 (1.25) | Data Time: 0.00 (0.11) | Mem (GB): 22.00 (20.06/24.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 2.78e+00 (1.65e+00)
INFO 2025-08-12 01:17:05,478 train_utils.py: 271: Train Epoch: [7][ 80/136] | Batch Time: 0.95 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (19.95/24.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 3.94e-01 (1.60e+00)
INFO 2025-08-12 01:17:17,431 train_utils.py: 271: Train Epoch: [7][ 90/136] | Batch Time: 1.08 (1.23) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (20.02/24.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 4.69e-01 (1.62e+00)
INFO 2025-08-12 01:17:28,422 train_utils.py: 271: Train Epoch: [7][100/136] | Batch Time: 1.12 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (19.93/24.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 4.05e-01 (1.55e+00)
INFO 2025-08-12 01:17:40,378 train_utils.py: 271: Train Epoch: [7][110/136] | Batch Time: 1.38 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 23.00 (20.00/24.00) | Time Elapsed: 00d 00h 21m | Losses/train_all_loss: 3.34e+00 (1.59e+00)
INFO 2025-08-12 01:17:52,095 train_utils.py: 271: Train Epoch: [7][120/136] | Batch Time: 0.92 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 15.00 (20.02/24.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 3.16e-01 (1.60e+00)
INFO 2025-08-12 01:18:03,453 train_utils.py: 271: Train Epoch: [7][130/136] | Batch Time: 0.93 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (20.02/24.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 2.59e-01 (1.60e+00)
INFO 2025-08-12 01:18:10,331 trainer.py: 950: Estimated time remaining: 00d 01h 26m
INFO 2025-08-12 01:18:10,473 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:18:10,473 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5833549867658054, 'Losses/train_all_loss_mask': 0.0048692173886058085, 'Losses/train_all_loss_dice': 6.267859271344016, 'Losses/train_all_loss_iou': 0.6553563077605384, 'Losses/train_all_loss_class': 0.028289237841151175, 'Losses/train_all_core_loss': 1.5833549867658054, 'Trainer/where': 0.19981617647058822, 'Trainer/epoch': 7, 'Trainer/steps_train': 1088}
INFO 2025-08-12 01:18:21,115 train_utils.py: 271: Train Epoch: [8][  0/136] | Batch Time: 9.19 (9.19) | Data Time: 7.33 (7.33) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 3.17e-01 (3.17e-01)
INFO 2025-08-12 01:18:31,743 train_utils.py: 271: Train Epoch: [8][ 10/136] | Batch Time: 1.23 (1.80) | Data Time: 0.00 (0.67) | Mem (GB): 22.00 (18.91/22.00) | Time Elapsed: 00d 00h 22m | Losses/train_all_loss: 2.78e+00 (1.08e+00)
INFO 2025-08-12 01:18:41,918 train_utils.py: 271: Train Epoch: [8][ 20/136] | Batch Time: 1.05 (1.43) | Data Time: 0.00 (0.35) | Mem (GB): 19.00 (18.86/22.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 4.24e-01 (1.02e+00)
INFO 2025-08-12 01:18:52,886 train_utils.py: 271: Train Epoch: [8][ 30/136] | Batch Time: 1.21 (1.32) | Data Time: 0.00 (0.24) | Mem (GB): 21.00 (19.06/22.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 1.61e+00 (1.14e+00)
INFO 2025-08-12 01:19:03,968 train_utils.py: 271: Train Epoch: [8][ 40/136] | Batch Time: 1.25 (1.27) | Data Time: 0.00 (0.18) | Mem (GB): 22.00 (19.24/22.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 2.49e+00 (1.26e+00)
INFO 2025-08-12 01:19:15,754 train_utils.py: 271: Train Epoch: [8][ 50/136] | Batch Time: 1.09 (1.25) | Data Time: 0.00 (0.14) | Mem (GB): 19.00 (19.55/24.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 4.67e-01 (1.40e+00)
INFO 2025-08-12 01:19:27,317 train_utils.py: 271: Train Epoch: [8][ 60/136] | Batch Time: 1.07 (1.24) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (19.66/24.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 4.24e-01 (1.44e+00)
INFO 2025-08-12 01:19:39,006 train_utils.py: 271: Train Epoch: [8][ 70/136] | Batch Time: 0.94 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (19.73/24.00) | Time Elapsed: 00d 00h 23m | Losses/train_all_loss: 3.11e-01 (1.44e+00)
INFO 2025-08-12 01:19:50,150 train_utils.py: 271: Train Epoch: [8][ 80/136] | Batch Time: 0.92 (1.21) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (19.72/24.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 3.11e-01 (1.45e+00)
INFO 2025-08-12 01:20:01,743 train_utils.py: 271: Train Epoch: [8][ 90/136] | Batch Time: 1.22 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 21.00 (19.79/24.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 1.80e+00 (1.50e+00)
INFO 2025-08-12 01:20:13,280 train_utils.py: 271: Train Epoch: [8][100/136] | Batch Time: 1.27 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 21.00 (19.82/24.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 1.99e+00 (1.52e+00)
INFO 2025-08-12 01:20:25,025 train_utils.py: 271: Train Epoch: [8][110/136] | Batch Time: 1.09 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (19.86/24.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 4.59e-01 (1.51e+00)
INFO 2025-08-12 01:20:36,333 train_utils.py: 271: Train Epoch: [8][120/136] | Batch Time: 1.24 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (19.87/24.00) | Time Elapsed: 00d 00h 24m | Losses/train_all_loss: 3.18e+00 (1.51e+00)
INFO 2025-08-12 01:20:47,693 train_utils.py: 271: Train Epoch: [8][130/136] | Batch Time: 1.10 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.88/24.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 4.74e-01 (1.50e+00)
INFO 2025-08-12 01:20:54,969 trainer.py: 950: Estimated time remaining: 00d 01h 23m
INFO 2025-08-12 01:20:55,166 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:20:55,167 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.491937319364618, 'Losses/train_all_loss_mask': 0.00494836843919235, 'Losses/train_all_loss_dice': 6.008866594994769, 'Losses/train_all_loss_iou': 0.5648775425027398, 'Losses/train_all_loss_class': 0.0039955666175798986, 'Losses/train_all_core_loss': 1.491937319364618, 'Trainer/where': 0.2248161764705882, 'Trainer/epoch': 8, 'Trainer/steps_train': 1224}
INFO 2025-08-12 01:21:05,748 train_utils.py: 271: Train Epoch: [9][  0/136] | Batch Time: 9.10 (9.10) | Data Time: 7.81 (7.81) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 3.04e-01 (3.04e-01)
INFO 2025-08-12 01:21:16,481 train_utils.py: 271: Train Epoch: [9][ 10/136] | Batch Time: 1.36 (1.80) | Data Time: 0.00 (0.71) | Mem (GB): 23.00 (19.09/23.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 3.51e+00 (1.24e+00)
INFO 2025-08-12 01:21:27,914 train_utils.py: 271: Train Epoch: [9][ 20/136] | Batch Time: 0.91 (1.49) | Data Time: 0.00 (0.37) | Mem (GB): 17.00 (19.67/23.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 2.88e-01 (1.45e+00)
INFO 2025-08-12 01:21:39,289 train_utils.py: 271: Train Epoch: [9][ 30/136] | Batch Time: 0.94 (1.38) | Data Time: 0.00 (0.25) | Mem (GB): 17.00 (19.71/24.00) | Time Elapsed: 00d 00h 25m | Losses/train_all_loss: 2.74e+00 (1.51e+00)
INFO 2025-08-12 01:21:51,454 train_utils.py: 271: Train Epoch: [9][ 40/136] | Batch Time: 0.92 (1.34) | Data Time: 0.00 (0.19) | Mem (GB): 17.00 (20.15/24.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.72e-01 (1.73e+00)
INFO 2025-08-12 01:22:02,846 train_utils.py: 271: Train Epoch: [9][ 50/136] | Batch Time: 1.24 (1.30) | Data Time: 0.00 (0.15) | Mem (GB): 22.00 (20.14/24.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 3.14e+00 (1.74e+00)
INFO 2025-08-12 01:22:14,344 train_utils.py: 271: Train Epoch: [9][ 60/136] | Batch Time: 1.23 (1.27) | Data Time: 0.00 (0.13) | Mem (GB): 21.00 (20.13/24.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.02e+00 (1.71e+00)
INFO 2025-08-12 01:22:25,790 train_utils.py: 271: Train Epoch: [9][ 70/136] | Batch Time: 1.38 (1.26) | Data Time: 0.00 (0.11) | Mem (GB): 19.00 (19.96/24.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 2.24e+00 (1.65e+00)
INFO 2025-08-12 01:22:36,454 train_utils.py: 271: Train Epoch: [9][ 80/136] | Batch Time: 1.09 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (19.78/24.00) | Time Elapsed: 00d 00h 26m | Losses/train_all_loss: 4.27e-01 (1.57e+00)
INFO 2025-08-12 01:22:47,460 train_utils.py: 271: Train Epoch: [9][ 90/136] | Batch Time: 1.26 (1.22) | Data Time: 0.00 (0.09) | Mem (GB): 21.00 (19.74/24.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 1.38e+00 (1.53e+00)
INFO 2025-08-12 01:22:58,760 train_utils.py: 271: Train Epoch: [9][100/136] | Batch Time: 0.94 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (19.75/24.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 2.82e-01 (1.54e+00)
INFO 2025-08-12 01:23:10,443 train_utils.py: 271: Train Epoch: [9][110/136] | Batch Time: 1.21 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 21.00 (19.81/24.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 2.07e+00 (1.56e+00)
INFO 2025-08-12 01:23:21,429 train_utils.py: 271: Train Epoch: [9][120/136] | Batch Time: 1.09 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (19.77/24.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 4.07e-01 (1.51e+00)
INFO 2025-08-12 01:23:33,141 train_utils.py: 271: Train Epoch: [9][130/136] | Batch Time: 1.35 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 23.00 (19.85/24.00) | Time Elapsed: 00d 00h 27m | Losses/train_all_loss: 3.02e+00 (1.55e+00)
INFO 2025-08-12 01:23:40,520 trainer.py: 950: Estimated time remaining: 00d 01h 21m
INFO 2025-08-12 01:23:40,644 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:23:40,644 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5889583451344687, 'Losses/train_all_loss_mask': 0.0048773244939986716, 'Losses/train_all_loss_dice': 6.309958898845841, 'Losses/train_all_loss_iou': 0.6290266137734494, 'Losses/train_all_loss_class': 0.030454863097438247, 'Losses/train_all_core_loss': 1.5889583451344687, 'Trainer/where': 0.24981617647058824, 'Trainer/epoch': 9, 'Trainer/steps_train': 1360}
INFO 2025-08-12 01:23:52,670 train_utils.py: 271: Train Epoch: [10][  0/136] | Batch Time: 9.20 (9.20) | Data Time: 5.53 (5.53) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 3.38e+00 (3.38e+00)
INFO 2025-08-12 01:24:03,794 train_utils.py: 271: Train Epoch: [10][ 10/136] | Batch Time: 1.18 (1.85) | Data Time: 0.00 (0.50) | Mem (GB): 21.00 (20.27/23.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 1.97e+00 (1.61e+00)
INFO 2025-08-12 01:24:14,329 train_utils.py: 271: Train Epoch: [10][ 20/136] | Batch Time: 0.90 (1.47) | Data Time: 0.00 (0.26) | Mem (GB): 17.00 (19.62/23.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 2.60e-01 (1.47e+00)
INFO 2025-08-12 01:24:25,600 train_utils.py: 271: Train Epoch: [10][ 30/136] | Batch Time: 1.23 (1.36) | Data Time: 0.00 (0.18) | Mem (GB): 21.00 (19.71/23.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 1.83e+00 (1.42e+00)
INFO 2025-08-12 01:24:37,855 train_utils.py: 271: Train Epoch: [10][ 40/136] | Batch Time: 1.23 (1.33) | Data Time: 0.00 (0.14) | Mem (GB): 22.00 (20.07/24.00) | Time Elapsed: 00d 00h 28m | Losses/train_all_loss: 2.95e+00 (1.63e+00)
INFO 2025-08-12 01:24:49,368 train_utils.py: 271: Train Epoch: [10][ 50/136] | Batch Time: 0.94 (1.29) | Data Time: 0.00 (0.11) | Mem (GB): 17.00 (20.04/24.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 3.10e-01 (1.60e+00)
INFO 2025-08-12 01:25:00,334 train_utils.py: 271: Train Epoch: [10][ 60/136] | Batch Time: 0.92 (1.26) | Data Time: 0.00 (0.09) | Mem (GB): 15.00 (19.90/24.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 3.11e-01 (1.52e+00)
INFO 2025-08-12 01:25:11,839 train_utils.py: 271: Train Epoch: [10][ 70/136] | Batch Time: 1.37 (1.24) | Data Time: 0.00 (0.08) | Mem (GB): 23.00 (19.90/24.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 2.85e+00 (1.50e+00)
INFO 2025-08-12 01:25:23,375 train_utils.py: 271: Train Epoch: [10][ 80/136] | Batch Time: 1.21 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.93/24.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 2.88e+00 (1.50e+00)
INFO 2025-08-12 01:25:34,709 train_utils.py: 271: Train Epoch: [10][ 90/136] | Batch Time: 1.08 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.88/24.00) | Time Elapsed: 00d 00h 29m | Losses/train_all_loss: 5.40e-01 (1.46e+00)
INFO 2025-08-12 01:25:45,911 train_utils.py: 271: Train Epoch: [10][100/136] | Batch Time: 1.25 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 21.00 (19.85/24.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 2.15e+00 (1.44e+00)
INFO 2025-08-12 01:25:56,202 train_utils.py: 271: Train Epoch: [10][110/136] | Batch Time: 0.92 (1.20) | Data Time: 0.00 (0.05) | Mem (GB): 17.00 (19.72/24.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 3.06e-01 (1.38e+00)
INFO 2025-08-12 01:26:07,302 train_utils.py: 271: Train Epoch: [10][120/136] | Batch Time: 1.22 (1.19) | Data Time: 0.00 (0.05) | Mem (GB): 21.00 (19.69/24.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 1.91e+00 (1.36e+00)
INFO 2025-08-12 01:26:19,126 train_utils.py: 271: Train Epoch: [10][130/136] | Batch Time: 1.07 (1.19) | Data Time: 0.00 (0.04) | Mem (GB): 19.00 (19.76/24.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 4.90e-01 (1.39e+00)
INFO 2025-08-12 01:26:26,342 trainer.py: 950: Estimated time remaining: 00d 01h 18m
INFO 2025-08-12 01:26:26,527 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:26:26,528 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.392625085571233, 'Losses/train_all_loss_mask': 0.0036789367064985153, 'Losses/train_all_loss_dice': 5.48794744645848, 'Losses/train_all_loss_iou': 0.6219506278524504, 'Losses/train_all_loss_class': 0.04188266581192077, 'Losses/train_all_core_loss': 1.392625085571233, 'Trainer/where': 0.2748161764705882, 'Trainer/epoch': 10, 'Trainer/steps_train': 1496}
INFO 2025-08-12 01:26:36,798 train_utils.py: 271: Train Epoch: [11][  0/136] | Batch Time: 8.81 (8.81) | Data Time: 6.99 (6.99) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 00h 30m | Losses/train_all_loss: 3.18e-01 (3.18e-01)
INFO 2025-08-12 01:26:48,058 train_utils.py: 271: Train Epoch: [11][ 10/136] | Batch Time: 1.22 (1.82) | Data Time: 0.00 (0.64) | Mem (GB): 22.00 (20.00/24.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 2.29e+00 (1.48e+00)
INFO 2025-08-12 01:26:58,996 train_utils.py: 271: Train Epoch: [11][ 20/136] | Batch Time: 1.34 (1.48) | Data Time: 0.00 (0.33) | Mem (GB): 24.00 (19.86/24.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 2.89e+00 (1.39e+00)
INFO 2025-08-12 01:27:10,416 train_utils.py: 271: Train Epoch: [11][ 30/136] | Batch Time: 1.24 (1.37) | Data Time: 0.00 (0.23) | Mem (GB): 21.00 (20.00/24.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 1.95e+00 (1.48e+00)
INFO 2025-08-12 01:27:21,326 train_utils.py: 271: Train Epoch: [11][ 40/136] | Batch Time: 1.22 (1.30) | Data Time: 0.00 (0.17) | Mem (GB): 22.00 (19.90/24.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 3.29e+00 (1.49e+00)
INFO 2025-08-12 01:27:32,263 train_utils.py: 271: Train Epoch: [11][ 50/136] | Batch Time: 1.08 (1.26) | Data Time: 0.00 (0.14) | Mem (GB): 16.00 (19.76/24.00) | Time Elapsed: 00d 00h 31m | Losses/train_all_loss: 6.01e-01 (1.53e+00)
INFO 2025-08-12 01:27:43,307 train_utils.py: 271: Train Epoch: [11][ 60/136] | Batch Time: 0.90 (1.23) | Data Time: 0.00 (0.12) | Mem (GB): 17.00 (19.70/24.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 3.26e-01 (1.46e+00)
INFO 2025-08-12 01:27:53,931 train_utils.py: 271: Train Epoch: [11][ 70/136] | Batch Time: 1.24 (1.21) | Data Time: 0.00 (0.10) | Mem (GB): 21.00 (19.55/24.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 1.85e+00 (1.40e+00)
INFO 2025-08-12 01:28:06,186 train_utils.py: 271: Train Epoch: [11][ 80/136] | Batch Time: 1.43 (1.21) | Data Time: 0.00 (0.09) | Mem (GB): 23.00 (19.70/24.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 2.80e+00 (1.46e+00)
INFO 2025-08-12 01:28:17,828 train_utils.py: 271: Train Epoch: [11][ 90/136] | Batch Time: 1.23 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 21.00 (19.69/24.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 1.53e+00 (1.43e+00)
INFO 2025-08-12 01:28:29,085 train_utils.py: 271: Train Epoch: [11][100/136] | Batch Time: 0.93 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.69/24.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 2.82e-01 (1.43e+00)
INFO 2025-08-12 01:28:39,858 train_utils.py: 271: Train Epoch: [11][110/136] | Batch Time: 1.08 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.65/24.00) | Time Elapsed: 00d 00h 32m | Losses/train_all_loss: 4.93e-01 (1.42e+00)
INFO 2025-08-12 01:28:50,926 train_utils.py: 271: Train Epoch: [11][120/136] | Batch Time: 1.08 (1.18) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.63/24.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 7.47e-01 (1.40e+00)
INFO 2025-08-12 01:29:02,376 train_utils.py: 271: Train Epoch: [11][130/136] | Batch Time: 1.20 (1.18) | Data Time: 0.00 (0.05) | Mem (GB): 21.00 (19.66/24.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 1.50e+00 (1.40e+00)
INFO 2025-08-12 01:29:09,807 trainer.py: 950: Estimated time remaining: 00d 01h 14m
INFO 2025-08-12 01:29:10,137 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:29:10,138 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4160180547658134, 'Losses/train_all_loss_mask': 0.004533572321794482, 'Losses/train_all_loss_dice': 5.69365531995016, 'Losses/train_all_loss_iou': 0.5901931749163744, 'Losses/train_all_loss_class': 0.000573318279965834, 'Losses/train_all_core_loss': 1.4160180547658134, 'Trainer/where': 0.2998161764705882, 'Trainer/epoch': 11, 'Trainer/steps_train': 1632}
INFO 2025-08-12 01:29:21,076 train_utils.py: 271: Train Epoch: [12][  0/136] | Batch Time: 9.45 (9.45) | Data Time: 5.18 (5.18) | Mem (GB): 23.00 (23.00/23.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 2.33e+00 (2.33e+00)
INFO 2025-08-12 01:29:31,321 train_utils.py: 271: Train Epoch: [12][ 10/136] | Batch Time: 0.89 (1.79) | Data Time: 0.00 (0.47) | Mem (GB): 17.00 (19.45/23.00) | Time Elapsed: 00d 00h 33m | Losses/train_all_loss: 2.65e-01 (1.33e+00)
INFO 2025-08-12 01:29:43,247 train_utils.py: 271: Train Epoch: [12][ 20/136] | Batch Time: 1.22 (1.51) | Data Time: 0.00 (0.25) | Mem (GB): 19.00 (20.19/24.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 2.02e+00 (1.77e+00)
INFO 2025-08-12 01:29:54,411 train_utils.py: 271: Train Epoch: [12][ 30/136] | Batch Time: 1.09 (1.38) | Data Time: 0.00 (0.17) | Mem (GB): 19.00 (20.03/24.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 4.54e-01 (1.56e+00)
INFO 2025-08-12 01:30:05,672 train_utils.py: 271: Train Epoch: [12][ 40/136] | Batch Time: 1.26 (1.32) | Data Time: 0.00 (0.13) | Mem (GB): 22.00 (19.90/24.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 2.38e+00 (1.45e+00)
INFO 2025-08-12 01:30:16,982 train_utils.py: 271: Train Epoch: [12][ 50/136] | Batch Time: 1.25 (1.28) | Data Time: 0.00 (0.10) | Mem (GB): 22.00 (19.92/24.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 2.57e+00 (1.51e+00)
INFO 2025-08-12 01:30:28,634 train_utils.py: 271: Train Epoch: [12][ 60/136] | Batch Time: 1.08 (1.26) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (19.93/24.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 3.88e-01 (1.55e+00)
INFO 2025-08-12 01:30:40,042 train_utils.py: 271: Train Epoch: [12][ 70/136] | Batch Time: 0.92 (1.25) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.92/24.00) | Time Elapsed: 00d 00h 34m | Losses/train_all_loss: 2.60e-01 (1.57e+00)
INFO 2025-08-12 01:30:50,934 train_utils.py: 271: Train Epoch: [12][ 80/136] | Batch Time: 1.08 (1.23) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.75/24.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 5.56e-01 (1.49e+00)
INFO 2025-08-12 01:31:02,018 train_utils.py: 271: Train Epoch: [12][ 90/136] | Batch Time: 1.26 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (19.74/24.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 2.35e+00 (1.51e+00)
INFO 2025-08-12 01:31:13,094 train_utils.py: 271: Train Epoch: [12][100/136] | Batch Time: 0.93 (1.20) | Data Time: 0.00 (0.05) | Mem (GB): 17.00 (19.71/24.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 3.21e-01 (1.49e+00)
INFO 2025-08-12 01:31:24,654 train_utils.py: 271: Train Epoch: [12][110/136] | Batch Time: 1.07 (1.20) | Data Time: 0.00 (0.05) | Mem (GB): 19.00 (19.73/24.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 4.16e-01 (1.49e+00)
INFO 2025-08-12 01:31:36,989 train_utils.py: 271: Train Epoch: [12][120/136] | Batch Time: 1.23 (1.20) | Data Time: 0.00 (0.04) | Mem (GB): 22.00 (19.84/24.00) | Time Elapsed: 00d 00h 35m | Losses/train_all_loss: 3.79e+00 (1.53e+00)
INFO 2025-08-12 01:31:47,806 train_utils.py: 271: Train Epoch: [12][130/136] | Batch Time: 0.91 (1.19) | Data Time: 0.00 (0.04) | Mem (GB): 17.00 (19.77/24.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.53e-01 (1.50e+00)
INFO 2025-08-12 01:31:55,429 trainer.py: 950: Estimated time remaining: 00d 01h 12m
INFO 2025-08-12 01:31:55,438 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:31:55,438 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5225612201234873, 'Losses/train_all_loss_mask': 0.004633792823851228, 'Losses/train_all_loss_dice': 5.82093003041604, 'Losses/train_all_loss_iou': 0.6414397201391265, 'Losses/train_all_loss_class': 0.067904519412325, 'Losses/train_all_core_loss': 1.5225612201234873, 'Trainer/where': 0.32481617647058825, 'Trainer/epoch': 12, 'Trainer/steps_train': 1768}
INFO 2025-08-12 01:32:05,879 train_utils.py: 271: Train Epoch: [13][  0/136] | Batch Time: 8.95 (8.95) | Data Time: 6.12 (6.12) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 2.65e-01 (2.65e-01)
INFO 2025-08-12 01:32:17,131 train_utils.py: 271: Train Epoch: [13][ 10/136] | Batch Time: 1.20 (1.84) | Data Time: 0.00 (0.56) | Mem (GB): 22.00 (20.00/22.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.49e+00 (1.52e+00)
INFO 2025-08-12 01:32:27,858 train_utils.py: 271: Train Epoch: [13][ 20/136] | Batch Time: 1.38 (1.47) | Data Time: 0.00 (0.29) | Mem (GB): 23.00 (19.52/23.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 2.64e+00 (1.18e+00)
INFO 2025-08-12 01:32:40,180 train_utils.py: 271: Train Epoch: [13][ 30/136] | Batch Time: 1.25 (1.40) | Data Time: 0.00 (0.20) | Mem (GB): 22.00 (20.06/24.00) | Time Elapsed: 00d 00h 36m | Losses/train_all_loss: 3.28e+00 (1.50e+00)
INFO 2025-08-12 01:32:51,490 train_utils.py: 271: Train Epoch: [13][ 40/136] | Batch Time: 0.91 (1.33) | Data Time: 0.00 (0.15) | Mem (GB): 17.00 (20.00/24.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 1.87e+00 (1.50e+00)
INFO 2025-08-12 01:33:02,459 train_utils.py: 271: Train Epoch: [13][ 50/136] | Batch Time: 1.09 (1.28) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (19.82/24.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 4.58e-01 (1.54e+00)
INFO 2025-08-12 01:33:13,903 train_utils.py: 271: Train Epoch: [13][ 60/136] | Batch Time: 1.22 (1.26) | Data Time: 0.00 (0.10) | Mem (GB): 21.00 (19.89/24.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 1.92e+00 (1.56e+00)
INFO 2025-08-12 01:33:25,296 train_utils.py: 271: Train Epoch: [13][ 70/136] | Batch Time: 1.23 (1.24) | Data Time: 0.00 (0.09) | Mem (GB): 22.00 (19.89/24.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 3.24e+00 (1.57e+00)
INFO 2025-08-12 01:33:36,413 train_utils.py: 271: Train Epoch: [13][ 80/136] | Batch Time: 1.08 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (19.84/24.00) | Time Elapsed: 00d 00h 37m | Losses/train_all_loss: 4.36e-01 (1.54e+00)
INFO 2025-08-12 01:33:47,867 train_utils.py: 271: Train Epoch: [13][ 90/136] | Batch Time: 1.28 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.86/24.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 3.23e+00 (1.53e+00)
INFO 2025-08-12 01:33:59,483 train_utils.py: 271: Train Epoch: [13][100/136] | Batch Time: 1.21 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.85/24.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 4.51e-01 (1.51e+00)
INFO 2025-08-12 01:34:11,333 train_utils.py: 271: Train Epoch: [13][110/136] | Batch Time: 1.41 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 23.00 (19.91/24.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 2.52e+00 (1.54e+00)
INFO 2025-08-12 01:34:23,588 train_utils.py: 271: Train Epoch: [13][120/136] | Batch Time: 1.21 (1.21) | Data Time: 0.00 (0.05) | Mem (GB): 22.00 (20.03/24.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 3.15e+00 (1.62e+00)
INFO 2025-08-12 01:34:35,255 train_utils.py: 271: Train Epoch: [13][130/136] | Batch Time: 1.21 (1.21) | Data Time: 0.00 (0.05) | Mem (GB): 22.00 (20.05/24.00) | Time Elapsed: 00d 00h 38m | Losses/train_all_loss: 2.36e+00 (1.62e+00)
INFO 2025-08-12 01:34:42,537 trainer.py: 950: Estimated time remaining: 00d 01h 10m
INFO 2025-08-12 01:34:42,539 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:34:42,540 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.6183741156230955, 'Losses/train_all_loss_mask': 0.004875969883387905, 'Losses/train_all_loss_dice': 6.306664155686603, 'Losses/train_all_loss_iou': 0.6373054333688581, 'Losses/train_all_loss_class': 0.05892124625126418, 'Losses/train_all_core_loss': 1.6183741156230955, 'Trainer/where': 0.3498161764705882, 'Trainer/epoch': 13, 'Trainer/steps_train': 1904}
INFO 2025-08-12 01:34:52,952 train_utils.py: 271: Train Epoch: [14][  0/136] | Batch Time: 8.81 (8.81) | Data Time: 7.29 (7.29) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 4.53e-01 (4.53e-01)
INFO 2025-08-12 01:35:04,300 train_utils.py: 271: Train Epoch: [14][ 10/136] | Batch Time: 0.90 (1.83) | Data Time: 0.00 (0.66) | Mem (GB): 17.00 (20.09/23.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 2.65e-01 (1.50e+00)
INFO 2025-08-12 01:35:14,980 train_utils.py: 271: Train Epoch: [14][ 20/136] | Batch Time: 0.91 (1.47) | Data Time: 0.00 (0.35) | Mem (GB): 17.00 (19.71/23.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 2.89e-01 (1.38e+00)
INFO 2025-08-12 01:35:26,004 train_utils.py: 271: Train Epoch: [14][ 30/136] | Batch Time: 1.24 (1.35) | Data Time: 0.00 (0.24) | Mem (GB): 22.00 (19.65/23.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 3.52e+00 (1.38e+00)
INFO 2025-08-12 01:35:37,314 train_utils.py: 271: Train Epoch: [14][ 40/136] | Batch Time: 1.26 (1.30) | Data Time: 0.00 (0.18) | Mem (GB): 22.00 (19.63/23.00) | Time Elapsed: 00d 00h 39m | Losses/train_all_loss: 3.51e+00 (1.41e+00)
INFO 2025-08-12 01:35:48,683 train_utils.py: 271: Train Epoch: [14][ 50/136] | Batch Time: 1.22 (1.27) | Data Time: 0.00 (0.14) | Mem (GB): 22.00 (19.75/24.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 2.97e+00 (1.49e+00)
INFO 2025-08-12 01:36:00,519 train_utils.py: 271: Train Epoch: [14][ 60/136] | Batch Time: 1.10 (1.25) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (19.92/24.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 5.91e-01 (1.53e+00)
INFO 2025-08-12 01:36:11,888 train_utils.py: 271: Train Epoch: [14][ 70/136] | Batch Time: 1.26 (1.24) | Data Time: 0.00 (0.10) | Mem (GB): 22.00 (19.97/24.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 2.93e+00 (1.56e+00)
INFO 2025-08-12 01:36:23,444 train_utils.py: 271: Train Epoch: [14][ 80/136] | Batch Time: 1.21 (1.23) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (19.99/24.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 4.05e-01 (1.56e+00)
INFO 2025-08-12 01:36:34,977 train_utils.py: 271: Train Epoch: [14][ 90/136] | Batch Time: 0.92 (1.22) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (20.02/24.00) | Time Elapsed: 00d 00h 40m | Losses/train_all_loss: 2.79e-01 (1.60e+00)
INFO 2025-08-12 01:36:47,189 train_utils.py: 271: Train Epoch: [14][100/136] | Batch Time: 1.31 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.12/24.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 2.47e+00 (1.70e+00)
INFO 2025-08-12 01:36:58,736 train_utils.py: 271: Train Epoch: [14][110/136] | Batch Time: 0.91 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (20.12/24.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 4.17e-01 (1.71e+00)
INFO 2025-08-12 01:37:10,676 train_utils.py: 271: Train Epoch: [14][120/136] | Batch Time: 1.37 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 24.00 (20.17/24.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 2.83e+00 (1.72e+00)
INFO 2025-08-12 01:37:22,139 train_utils.py: 271: Train Epoch: [14][130/136] | Batch Time: 1.39 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 24.00 (20.15/24.00) | Time Elapsed: 00d 00h 41m | Losses/train_all_loss: 2.85e+00 (1.69e+00)
INFO 2025-08-12 01:37:29,798 trainer.py: 950: Estimated time remaining: 00d 01h 08m
INFO 2025-08-12 01:37:29,843 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:37:29,843 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.7147221385556108, 'Losses/train_all_loss_mask': 0.004954736275548759, 'Losses/train_all_loss_dice': 6.6644188735414955, 'Losses/train_all_loss_iou': 0.6952493676970548, 'Losses/train_all_loss_class': 0.06937271599422346, 'Losses/train_all_core_loss': 1.7147221385556108, 'Trainer/where': 0.37481617647058824, 'Trainer/epoch': 14, 'Trainer/steps_train': 2040}
INFO 2025-08-12 01:37:42,333 train_utils.py: 271: Train Epoch: [15][  0/136] | Batch Time: 9.43 (9.43) | Data Time: 5.14 (5.14) | Mem (GB): 21.00 (21.00/21.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 1.91e+00 (1.91e+00)
INFO 2025-08-12 01:37:53,564 train_utils.py: 271: Train Epoch: [15][ 10/136] | Batch Time: 1.34 (1.88) | Data Time: 0.00 (0.47) | Mem (GB): 23.00 (20.27/23.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 2.64e+00 (1.67e+00)
INFO 2025-08-12 01:38:04,697 train_utils.py: 271: Train Epoch: [15][ 20/136] | Batch Time: 1.19 (1.51) | Data Time: 0.00 (0.25) | Mem (GB): 21.00 (20.24/24.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 1.83e+00 (1.60e+00)
INFO 2025-08-12 01:38:16,155 train_utils.py: 271: Train Epoch: [15][ 30/136] | Batch Time: 0.91 (1.40) | Data Time: 0.00 (0.17) | Mem (GB): 17.00 (20.26/24.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 2.73e-01 (1.60e+00)
INFO 2025-08-12 01:38:27,885 train_utils.py: 271: Train Epoch: [15][ 40/136] | Batch Time: 0.91 (1.34) | Data Time: 0.00 (0.13) | Mem (GB): 17.00 (20.37/24.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 2.96e-01 (1.66e+00)
INFO 2025-08-12 01:38:39,306 train_utils.py: 271: Train Epoch: [15][ 50/136] | Batch Time: 1.22 (1.30) | Data Time: 0.00 (0.10) | Mem (GB): 21.00 (20.29/24.00) | Time Elapsed: 00d 00h 42m | Losses/train_all_loss: 1.77e+00 (1.59e+00)
INFO 2025-08-12 01:38:51,082 train_utils.py: 271: Train Epoch: [15][ 60/136] | Batch Time: 1.10 (1.28) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (20.31/24.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 4.27e-01 (1.58e+00)
INFO 2025-08-12 01:39:02,964 train_utils.py: 271: Train Epoch: [15][ 70/136] | Batch Time: 1.09 (1.27) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (20.31/24.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 4.84e-01 (1.64e+00)
INFO 2025-08-12 01:39:14,726 train_utils.py: 271: Train Epoch: [15][ 80/136] | Batch Time: 1.25 (1.26) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.31/24.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 2.95e+00 (1.65e+00)
INFO 2025-08-12 01:39:26,149 train_utils.py: 271: Train Epoch: [15][ 90/136] | Batch Time: 1.25 (1.24) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.29/24.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 3.22e+00 (1.65e+00)
INFO 2025-08-12 01:39:38,094 train_utils.py: 271: Train Epoch: [15][100/136] | Batch Time: 1.27 (1.24) | Data Time: 0.00 (0.05) | Mem (GB): 22.00 (20.35/24.00) | Time Elapsed: 00d 00h 43m | Losses/train_all_loss: 2.29e+00 (1.69e+00)
INFO 2025-08-12 01:39:50,374 train_utils.py: 271: Train Epoch: [15][110/136] | Batch Time: 0.93 (1.24) | Data Time: 0.00 (0.05) | Mem (GB): 17.00 (20.41/24.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 2.92e-01 (1.72e+00)
INFO 2025-08-12 01:40:01,823 train_utils.py: 271: Train Epoch: [15][120/136] | Batch Time: 1.35 (1.23) | Data Time: 0.00 (0.04) | Mem (GB): 19.00 (20.31/24.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 1.33e+00 (1.67e+00)
INFO 2025-08-12 01:40:12,798 train_utils.py: 271: Train Epoch: [15][130/136] | Batch Time: 0.92 (1.22) | Data Time: 0.00 (0.04) | Mem (GB): 17.00 (20.24/24.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 2.85e-01 (1.63e+00)
INFO 2025-08-12 01:40:19,853 trainer.py: 950: Estimated time remaining: 00d 01h 06m
INFO 2025-08-12 01:40:19,892 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:40:19,893 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5981338208212572, 'Losses/train_all_loss_mask': 0.004582595714215559, 'Losses/train_all_loss_dice': 6.444240376353264, 'Losses/train_all_loss_iou': 0.7390464170671561, 'Losses/train_all_loss_class': 0.0010855909677992353, 'Losses/train_all_core_loss': 1.5981338208212572, 'Trainer/where': 0.3998161764705882, 'Trainer/epoch': 15, 'Trainer/steps_train': 2176}
INFO 2025-08-12 01:40:31,228 train_utils.py: 271: Train Epoch: [16][  0/136] | Batch Time: 9.83 (9.83) | Data Time: 8.01 (8.01) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 00h 44m | Losses/train_all_loss: 3.82e+00 (3.82e+00)
INFO 2025-08-12 01:40:42,554 train_utils.py: 271: Train Epoch: [16][ 10/136] | Batch Time: 0.88 (1.92) | Data Time: 0.00 (0.73) | Mem (GB): 17.00 (20.45/24.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.94e-01 (1.90e+00)
INFO 2025-08-12 01:40:52,755 train_utils.py: 271: Train Epoch: [16][ 20/136] | Batch Time: 0.90 (1.49) | Data Time: 0.00 (0.38) | Mem (GB): 17.00 (19.52/24.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.76e-01 (1.33e+00)
INFO 2025-08-12 01:41:04,209 train_utils.py: 271: Train Epoch: [16][ 30/136] | Batch Time: 1.20 (1.38) | Data Time: 0.00 (0.26) | Mem (GB): 21.00 (19.87/24.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 1.93e+00 (1.46e+00)
INFO 2025-08-12 01:41:14,598 train_utils.py: 271: Train Epoch: [16][ 40/136] | Batch Time: 0.90 (1.30) | Data Time: 0.00 (0.20) | Mem (GB): 17.00 (19.59/24.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 3.41e-01 (1.31e+00)
INFO 2025-08-12 01:41:25,166 train_utils.py: 271: Train Epoch: [16][ 50/136] | Batch Time: 0.92 (1.25) | Data Time: 0.00 (0.16) | Mem (GB): 17.00 (19.41/24.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 3.94e-01 (1.18e+00)
INFO 2025-08-12 01:41:36,441 train_utils.py: 271: Train Epoch: [16][ 60/136] | Batch Time: 1.24 (1.23) | Data Time: 0.00 (0.13) | Mem (GB): 21.00 (19.49/24.00) | Time Elapsed: 00d 00h 45m | Losses/train_all_loss: 2.42e+00 (1.27e+00)
INFO 2025-08-12 01:41:48,066 train_utils.py: 271: Train Epoch: [16][ 70/136] | Batch Time: 1.00 (1.22) | Data Time: 0.00 (0.11) | Mem (GB): 17.00 (19.55/24.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.03e-01 (1.29e+00)
INFO 2025-08-12 01:41:59,126 train_utils.py: 271: Train Epoch: [16][ 80/136] | Batch Time: 1.22 (1.21) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (19.48/24.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 4.30e-01 (1.22e+00)
INFO 2025-08-12 01:42:10,923 train_utils.py: 271: Train Epoch: [16][ 90/136] | Batch Time: 1.21 (1.20) | Data Time: 0.00 (0.09) | Mem (GB): 22.00 (19.60/24.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 2.79e+00 (1.26e+00)
INFO 2025-08-12 01:42:22,579 train_utils.py: 271: Train Epoch: [16][100/136] | Batch Time: 1.37 (1.20) | Data Time: 0.00 (0.08) | Mem (GB): 24.00 (19.61/24.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 2.64e+00 (1.29e+00)
INFO 2025-08-12 01:42:33,686 train_utils.py: 271: Train Epoch: [16][110/136] | Batch Time: 0.92 (1.19) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.61/24.00) | Time Elapsed: 00d 00h 46m | Losses/train_all_loss: 3.05e-01 (1.31e+00)
INFO 2025-08-12 01:42:44,806 train_utils.py: 271: Train Epoch: [16][120/136] | Batch Time: 0.91 (1.19) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.61/24.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 2.82e-01 (1.29e+00)
INFO 2025-08-12 01:42:55,417 train_utils.py: 271: Train Epoch: [16][130/136] | Batch Time: 1.08 (1.18) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (19.56/24.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 4.63e-01 (1.24e+00)
INFO 2025-08-12 01:43:02,995 trainer.py: 950: Estimated time remaining: 00d 01h 01m
INFO 2025-08-12 01:43:03,088 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:43:03,088 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2743163173470426, 'Losses/train_all_loss_mask': 0.003935521427860402, 'Losses/train_all_loss_dice': 5.060632220085929, 'Losses/train_all_loss_iou': 0.5113092374111361, 'Losses/train_all_loss_class': 0.02218476491532663, 'Losses/train_all_core_loss': 1.2743163173470426, 'Trainer/where': 0.4248161764705882, 'Trainer/epoch': 16, 'Trainer/steps_train': 2312}
INFO 2025-08-12 01:43:13,893 train_utils.py: 271: Train Epoch: [17][  0/136] | Batch Time: 9.28 (9.28) | Data Time: 8.14 (8.14) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 4.41e-01 (4.41e-01)
INFO 2025-08-12 01:43:25,208 train_utils.py: 271: Train Epoch: [17][ 10/136] | Batch Time: 1.22 (1.87) | Data Time: 0.00 (0.74) | Mem (GB): 22.00 (20.36/24.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 2.46e+00 (1.70e+00)
INFO 2025-08-12 01:43:35,850 train_utils.py: 271: Train Epoch: [17][ 20/136] | Batch Time: 1.05 (1.49) | Data Time: 0.00 (0.39) | Mem (GB): 19.00 (19.71/24.00) | Time Elapsed: 00d 00h 47m | Losses/train_all_loss: 5.37e-01 (1.33e+00)
INFO 2025-08-12 01:43:47,414 train_utils.py: 271: Train Epoch: [17][ 30/136] | Batch Time: 1.23 (1.38) | Data Time: 0.00 (0.26) | Mem (GB): 22.00 (20.03/24.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 2.86e+00 (1.55e+00)
INFO 2025-08-12 01:43:58,710 train_utils.py: 271: Train Epoch: [17][ 40/136] | Batch Time: 1.20 (1.32) | Data Time: 0.00 (0.20) | Mem (GB): 21.00 (20.02/24.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 1.50e+00 (1.50e+00)
INFO 2025-08-12 01:44:10,413 train_utils.py: 271: Train Epoch: [17][ 50/136] | Batch Time: 1.42 (1.29) | Data Time: 0.00 (0.16) | Mem (GB): 23.00 (20.02/24.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 2.25e+00 (1.47e+00)
INFO 2025-08-12 01:44:21,177 train_utils.py: 271: Train Epoch: [17][ 60/136] | Batch Time: 1.25 (1.26) | Data Time: 0.00 (0.13) | Mem (GB): 21.00 (19.90/24.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 1.75e+00 (1.46e+00)
INFO 2025-08-12 01:44:32,844 train_utils.py: 271: Train Epoch: [17][ 70/136] | Batch Time: 1.36 (1.24) | Data Time: 0.00 (0.12) | Mem (GB): 24.00 (20.01/24.00) | Time Elapsed: 00d 00h 48m | Losses/train_all_loss: 2.56e+00 (1.54e+00)
INFO 2025-08-12 01:44:44,067 train_utils.py: 271: Train Epoch: [17][ 80/136] | Batch Time: 1.08 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (19.95/24.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 5.17e-01 (1.51e+00)
INFO 2025-08-12 01:44:54,473 train_utils.py: 271: Train Epoch: [17][ 90/136] | Batch Time: 0.92 (1.21) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (19.78/24.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 2.94e-01 (1.44e+00)
INFO 2025-08-12 01:45:05,695 train_utils.py: 271: Train Epoch: [17][100/136] | Batch Time: 0.93 (1.20) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (19.73/24.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 2.94e-01 (1.43e+00)
INFO 2025-08-12 01:45:16,579 train_utils.py: 271: Train Epoch: [17][110/136] | Batch Time: 1.12 (1.19) | Data Time: 0.00 (0.07) | Mem (GB): 16.00 (19.65/24.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 2.89e-01 (1.39e+00)
INFO 2025-08-12 01:45:27,961 train_utils.py: 271: Train Epoch: [17][120/136] | Batch Time: 1.20 (1.18) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.68/24.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 3.09e+00 (1.43e+00)
INFO 2025-08-12 01:45:39,689 train_utils.py: 271: Train Epoch: [17][130/136] | Batch Time: 1.24 (1.18) | Data Time: 0.00 (0.06) | Mem (GB): 21.00 (19.77/24.00) | Time Elapsed: 00d 00h 49m | Losses/train_all_loss: 1.91e+00 (1.48e+00)
INFO 2025-08-12 01:45:46,589 trainer.py: 950: Estimated time remaining: 00d 00h 58m
INFO 2025-08-12 01:45:46,719 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:45:46,719 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4562490117900513, 'Losses/train_all_loss_mask': 0.004619951310065945, 'Losses/train_all_loss_dice': 5.91813911409939, 'Losses/train_all_loss_iou': 0.553254108163802, 'Losses/train_all_loss_class': 0.0002720471945556991, 'Losses/train_all_core_loss': 1.4562490117900513, 'Trainer/where': 0.44981617647058825, 'Trainer/epoch': 17, 'Trainer/steps_train': 2448}
INFO 2025-08-12 01:45:57,393 train_utils.py: 271: Train Epoch: [18][  0/136] | Batch Time: 9.17 (9.17) | Data Time: 8.12 (8.12) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 3.59e-01 (3.59e-01)
INFO 2025-08-12 01:46:09,349 train_utils.py: 271: Train Epoch: [18][ 10/136] | Batch Time: 1.37 (1.92) | Data Time: 0.00 (0.74) | Mem (GB): 23.00 (20.64/24.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 3.77e+00 (1.80e+00)
INFO 2025-08-12 01:46:21,196 train_utils.py: 271: Train Epoch: [18][ 20/136] | Batch Time: 1.34 (1.57) | Data Time: 0.00 (0.39) | Mem (GB): 23.00 (21.00/24.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 3.00e+00 (2.02e+00)
INFO 2025-08-12 01:46:31,588 train_utils.py: 271: Train Epoch: [18][ 30/136] | Batch Time: 0.89 (1.40) | Data Time: 0.00 (0.26) | Mem (GB): 15.00 (20.32/24.00) | Time Elapsed: 00d 00h 50m | Losses/train_all_loss: 2.69e-01 (1.73e+00)
INFO 2025-08-12 01:46:42,301 train_utils.py: 271: Train Epoch: [18][ 40/136] | Batch Time: 1.36 (1.32) | Data Time: 0.00 (0.20) | Mem (GB): 19.00 (19.93/24.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 1.19e+00 (1.48e+00)
INFO 2025-08-12 01:46:53,911 train_utils.py: 271: Train Epoch: [18][ 50/136] | Batch Time: 1.22 (1.29) | Data Time: 0.00 (0.16) | Mem (GB): 22.00 (20.10/24.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 3.95e+00 (1.59e+00)
INFO 2025-08-12 01:47:06,032 train_utils.py: 271: Train Epoch: [18][ 60/136] | Batch Time: 1.21 (1.28) | Data Time: 0.00 (0.13) | Mem (GB): 22.00 (20.26/24.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 3.40e+00 (1.71e+00)
INFO 2025-08-12 01:47:18,121 train_utils.py: 271: Train Epoch: [18][ 70/136] | Batch Time: 1.37 (1.27) | Data Time: 0.00 (0.12) | Mem (GB): 23.00 (20.38/24.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 2.46e+00 (1.74e+00)
INFO 2025-08-12 01:47:29,654 train_utils.py: 271: Train Epoch: [18][ 80/136] | Batch Time: 0.94 (1.25) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (20.38/24.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 2.71e-01 (1.76e+00)
INFO 2025-08-12 01:47:40,818 train_utils.py: 271: Train Epoch: [18][ 90/136] | Batch Time: 0.91 (1.24) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (20.32/24.00) | Time Elapsed: 00d 00h 51m | Losses/train_all_loss: 9.62e-01 (1.74e+00)
INFO 2025-08-12 01:47:52,371 train_utils.py: 271: Train Epoch: [18][100/136] | Batch Time: 1.10 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (20.32/24.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 4.25e-01 (1.73e+00)
INFO 2025-08-12 01:48:03,507 train_utils.py: 271: Train Epoch: [18][110/136] | Batch Time: 1.27 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.26/24.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 2.33e+00 (1.69e+00)
INFO 2025-08-12 01:48:14,162 train_utils.py: 271: Train Epoch: [18][120/136] | Batch Time: 1.07 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (20.17/24.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 4.13e-01 (1.64e+00)
INFO 2025-08-12 01:48:25,394 train_utils.py: 271: Train Epoch: [18][130/136] | Batch Time: 1.08 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (20.16/24.00) | Time Elapsed: 00d 00h 52m | Losses/train_all_loss: 4.63e-01 (1.63e+00)
INFO 2025-08-12 01:48:32,508 trainer.py: 950: Estimated time remaining: 00d 00h 56m
INFO 2025-08-12 01:48:32,510 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:48:32,511 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.6156008936026518, 'Losses/train_all_loss_mask': 0.005224636719289907, 'Losses/train_all_loss_dice': 6.479293109739528, 'Losses/train_all_loss_iou': 0.6208945877630921, 'Losses/train_all_loss_class': 0.012701047271363982, 'Losses/train_all_core_loss': 1.6156008936026518, 'Trainer/where': 0.4748161764705882, 'Trainer/epoch': 18, 'Trainer/steps_train': 2584}
INFO 2025-08-12 01:48:43,649 train_utils.py: 271: Train Epoch: [19][  0/136] | Batch Time: 9.65 (9.65) | Data Time: 7.42 (7.42) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 2.92e+00 (2.92e+00)
INFO 2025-08-12 01:48:55,917 train_utils.py: 271: Train Epoch: [19][ 10/136] | Batch Time: 1.21 (1.99) | Data Time: 0.00 (0.68) | Mem (GB): 22.00 (21.73/24.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 2.91e+00 (2.55e+00)
INFO 2025-08-12 01:49:07,149 train_utils.py: 271: Train Epoch: [19][ 20/136] | Batch Time: 0.89 (1.58) | Data Time: 0.00 (0.35) | Mem (GB): 17.00 (21.00/24.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 2.68e-01 (2.09e+00)
INFO 2025-08-12 01:49:18,613 train_utils.py: 271: Train Epoch: [19][ 30/136] | Batch Time: 1.15 (1.44) | Data Time: 0.00 (0.24) | Mem (GB): 19.00 (20.35/24.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 4.59e-01 (1.82e+00)
INFO 2025-08-12 01:49:29,888 train_utils.py: 271: Train Epoch: [19][ 40/136] | Batch Time: 1.22 (1.36) | Data Time: 0.00 (0.18) | Mem (GB): 21.00 (20.20/24.00) | Time Elapsed: 00d 00h 53m | Losses/train_all_loss: 1.53e+00 (1.63e+00)
INFO 2025-08-12 01:49:41,070 train_utils.py: 271: Train Epoch: [19][ 50/136] | Batch Time: 0.91 (1.32) | Data Time: 0.00 (0.15) | Mem (GB): 17.00 (20.12/24.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 4.14e-01 (1.62e+00)
INFO 2025-08-12 01:49:53,209 train_utils.py: 271: Train Epoch: [19][ 60/136] | Batch Time: 0.91 (1.30) | Data Time: 0.00 (0.12) | Mem (GB): 17.00 (20.15/24.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 3.18e-01 (1.66e+00)
INFO 2025-08-12 01:50:04,438 train_utils.py: 271: Train Epoch: [19][ 70/136] | Batch Time: 1.09 (1.27) | Data Time: 0.00 (0.11) | Mem (GB): 19.00 (20.08/24.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 4.48e-01 (1.60e+00)
INFO 2025-08-12 01:50:15,682 train_utils.py: 271: Train Epoch: [19][ 80/136] | Batch Time: 1.10 (1.26) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (20.02/24.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 4.47e-01 (1.54e+00)
INFO 2025-08-12 01:50:27,576 train_utils.py: 271: Train Epoch: [19][ 90/136] | Batch Time: 1.23 (1.25) | Data Time: 0.00 (0.08) | Mem (GB): 21.00 (20.10/24.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 1.59e+00 (1.59e+00)
INFO 2025-08-12 01:50:38,701 train_utils.py: 271: Train Epoch: [19][100/136] | Batch Time: 1.30 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.03/24.00) | Time Elapsed: 00d 00h 54m | Losses/train_all_loss: 3.06e+00 (1.55e+00)
INFO 2025-08-12 01:50:50,331 train_utils.py: 271: Train Epoch: [19][110/136] | Batch Time: 1.38 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 23.00 (20.05/24.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 2.28e+00 (1.54e+00)
INFO 2025-08-12 01:51:01,220 train_utils.py: 271: Train Epoch: [19][120/136] | Batch Time: 0.91 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (19.96/24.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 2.63e-01 (1.50e+00)
INFO 2025-08-12 01:51:13,385 train_utils.py: 271: Train Epoch: [19][130/136] | Batch Time: 1.28 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.05/24.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 3.02e+00 (1.56e+00)
INFO 2025-08-12 01:51:20,197 trainer.py: 950: Estimated time remaining: 00d 00h 54m
INFO 2025-08-12 01:51:20,219 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:51:20,219 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.536644251031034, 'Losses/train_all_loss_mask': 0.004491531377264918, 'Losses/train_all_loss_dice': 6.243105158209801, 'Losses/train_all_loss_iou': 0.6525066887740704, 'Losses/train_all_loss_class': 0.00031827092073195433, 'Losses/train_all_core_loss': 1.536644251031034, 'Trainer/where': 0.49981617647058824, 'Trainer/epoch': 19, 'Trainer/steps_train': 2720}
INFO 2025-08-12 01:51:32,446 train_utils.py: 271: Train Epoch: [20][  0/136] | Batch Time: 9.16 (9.16) | Data Time: 7.43 (7.43) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 00h 55m | Losses/train_all_loss: 3.48e-01 (3.48e-01)
INFO 2025-08-12 01:51:44,412 train_utils.py: 271: Train Epoch: [20][ 10/136] | Batch Time: 1.19 (1.92) | Data Time: 0.00 (0.68) | Mem (GB): 21.00 (20.91/24.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 1.91e+00 (1.85e+00)
INFO 2025-08-12 01:51:54,850 train_utils.py: 271: Train Epoch: [20][ 20/136] | Batch Time: 0.90 (1.50) | Data Time: 0.00 (0.35) | Mem (GB): 17.00 (20.00/24.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 3.21e-01 (1.47e+00)
INFO 2025-08-12 01:52:06,300 train_utils.py: 271: Train Epoch: [20][ 30/136] | Batch Time: 1.24 (1.39) | Data Time: 0.00 (0.24) | Mem (GB): 21.00 (20.03/24.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 1.71e+00 (1.40e+00)
INFO 2025-08-12 01:52:17,883 train_utils.py: 271: Train Epoch: [20][ 40/136] | Batch Time: 1.20 (1.33) | Data Time: 0.00 (0.18) | Mem (GB): 21.00 (20.15/24.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 1.67e+00 (1.52e+00)
INFO 2025-08-12 01:52:29,556 train_utils.py: 271: Train Epoch: [20][ 50/136] | Batch Time: 1.06 (1.30) | Data Time: 0.00 (0.15) | Mem (GB): 19.00 (20.10/24.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 4.13e-01 (1.46e+00)
INFO 2025-08-12 01:52:40,898 train_utils.py: 271: Train Epoch: [20][ 60/136] | Batch Time: 1.25 (1.27) | Data Time: 0.00 (0.12) | Mem (GB): 22.00 (20.00/24.00) | Time Elapsed: 00d 00h 56m | Losses/train_all_loss: 3.04e+00 (1.44e+00)
INFO 2025-08-12 01:52:52,557 train_utils.py: 271: Train Epoch: [20][ 70/136] | Batch Time: 1.24 (1.26) | Data Time: 0.00 (0.11) | Mem (GB): 21.00 (20.00/24.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 1.87e+00 (1.48e+00)
INFO 2025-08-12 01:53:04,887 train_utils.py: 271: Train Epoch: [20][ 80/136] | Batch Time: 1.36 (1.25) | Data Time: 0.00 (0.09) | Mem (GB): 23.00 (20.16/24.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 3.47e+00 (1.58e+00)
INFO 2025-08-12 01:53:16,265 train_utils.py: 271: Train Epoch: [20][ 90/136] | Batch Time: 1.25 (1.24) | Data Time: 0.00 (0.08) | Mem (GB): 21.00 (20.07/24.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 1.72e+00 (1.54e+00)
INFO 2025-08-12 01:53:27,448 train_utils.py: 271: Train Epoch: [20][100/136] | Batch Time: 1.38 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 23.00 (20.01/24.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 2.62e+00 (1.48e+00)
INFO 2025-08-12 01:53:39,549 train_utils.py: 271: Train Epoch: [20][110/136] | Batch Time: 1.24 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.06/24.00) | Time Elapsed: 00d 00h 57m | Losses/train_all_loss: 3.12e+00 (1.54e+00)
INFO 2025-08-12 01:53:51,094 train_utils.py: 271: Train Epoch: [20][120/136] | Batch Time: 0.90 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (20.07/24.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 3.46e-01 (1.56e+00)
INFO 2025-08-12 01:54:02,402 train_utils.py: 271: Train Epoch: [20][130/136] | Batch Time: 0.91 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (20.07/24.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 2.76e-01 (1.56e+00)
INFO 2025-08-12 01:54:09,773 trainer.py: 950: Estimated time remaining: 00d 00h 52m
INFO 2025-08-12 01:54:09,775 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:54:09,775 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5470310098984663, 'Losses/train_all_loss_mask': 0.004370437906639863, 'Losses/train_all_loss_dice': 6.08890152854078, 'Losses/train_all_loss_iou': 0.639939287896542, 'Losses/train_all_loss_class': 0.048297489947076916, 'Losses/train_all_core_loss': 1.5470310098984663, 'Trainer/where': 0.5248161764705882, 'Trainer/epoch': 20, 'Trainer/steps_train': 2856}
INFO 2025-08-12 01:54:20,683 train_utils.py: 271: Train Epoch: [21][  0/136] | Batch Time: 9.46 (9.46) | Data Time: 8.38 (8.38) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 6.40e-01 (6.40e-01)
INFO 2025-08-12 01:54:31,527 train_utils.py: 271: Train Epoch: [21][ 10/136] | Batch Time: 1.31 (1.85) | Data Time: 0.00 (0.76) | Mem (GB): 24.00 (19.73/24.00) | Time Elapsed: 00d 00h 58m | Losses/train_all_loss: 2.98e+00 (1.10e+00)
INFO 2025-08-12 01:54:43,539 train_utils.py: 271: Train Epoch: [21][ 20/136] | Batch Time: 1.32 (1.54) | Data Time: 0.00 (0.40) | Mem (GB): 24.00 (20.67/24.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.42e+00 (1.56e+00)
INFO 2025-08-12 01:54:54,858 train_utils.py: 271: Train Epoch: [21][ 30/136] | Batch Time: 1.23 (1.41) | Data Time: 0.00 (0.27) | Mem (GB): 22.00 (20.35/24.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.78e+00 (1.52e+00)
INFO 2025-08-12 01:55:06,348 train_utils.py: 271: Train Epoch: [21][ 40/136] | Batch Time: 0.93 (1.34) | Data Time: 0.00 (0.21) | Mem (GB): 17.00 (20.15/24.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 4.09e-01 (1.43e+00)
INFO 2025-08-12 01:55:17,458 train_utils.py: 271: Train Epoch: [21][ 50/136] | Batch Time: 0.92 (1.30) | Data Time: 0.00 (0.17) | Mem (GB): 17.00 (20.00/24.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.72e-01 (1.35e+00)
INFO 2025-08-12 01:55:28,963 train_utils.py: 271: Train Epoch: [21][ 60/136] | Batch Time: 1.29 (1.27) | Data Time: 0.00 (0.14) | Mem (GB): 22.00 (20.02/24.00) | Time Elapsed: 00d 00h 59m | Losses/train_all_loss: 2.34e+00 (1.41e+00)
INFO 2025-08-12 01:55:41,282 train_utils.py: 271: Train Epoch: [21][ 70/136] | Batch Time: 1.08 (1.27) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (20.18/24.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 4.06e-01 (1.49e+00)
INFO 2025-08-12 01:55:52,744 train_utils.py: 271: Train Epoch: [21][ 80/136] | Batch Time: 1.22 (1.25) | Data Time: 0.00 (0.10) | Mem (GB): 21.00 (20.15/24.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 1.75e+00 (1.48e+00)
INFO 2025-08-12 01:56:04,043 train_utils.py: 271: Train Epoch: [21][ 90/136] | Batch Time: 1.22 (1.24) | Data Time: 0.00 (0.09) | Mem (GB): 21.00 (20.13/24.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 1.83e+00 (1.49e+00)
INFO 2025-08-12 01:56:14,760 train_utils.py: 271: Train Epoch: [21][100/136] | Batch Time: 0.93 (1.22) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (20.03/24.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 2.75e-01 (1.46e+00)
INFO 2025-08-12 01:56:26,808 train_utils.py: 271: Train Epoch: [21][110/136] | Batch Time: 1.27 (1.22) | Data Time: 0.00 (0.08) | Mem (GB): 22.00 (20.11/24.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 3.34e+00 (1.52e+00)
INFO 2025-08-12 01:56:37,864 train_utils.py: 271: Train Epoch: [21][120/136] | Batch Time: 1.24 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 21.00 (20.02/24.00) | Time Elapsed: 00d 01h 00m | Losses/train_all_loss: 1.43e+00 (1.48e+00)
INFO 2025-08-12 01:56:48,544 train_utils.py: 271: Train Epoch: [21][130/136] | Batch Time: 1.21 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (19.95/24.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 2.81e+00 (1.47e+00)
INFO 2025-08-12 01:56:55,676 trainer.py: 950: Estimated time remaining: 00d 00h 48m
INFO 2025-08-12 01:56:55,804 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:56:55,804 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4712662046008251, 'Losses/train_all_loss_mask': 0.004135157718230724, 'Losses/train_all_loss_dice': 5.922839385621688, 'Losses/train_all_loss_iou': 0.6140275974484051, 'Losses/train_all_loss_class': 0.019162262969949495, 'Losses/train_all_core_loss': 1.4712662046008251, 'Trainer/where': 0.5498161764705882, 'Trainer/epoch': 21, 'Trainer/steps_train': 2992}
INFO 2025-08-12 01:57:06,755 train_utils.py: 271: Train Epoch: [22][  0/136] | Batch Time: 9.54 (9.54) | Data Time: 7.63 (7.63) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 2.90e+00 (2.90e+00)
INFO 2025-08-12 01:57:17,796 train_utils.py: 271: Train Epoch: [22][ 10/136] | Batch Time: 1.33 (1.87) | Data Time: 0.00 (0.69) | Mem (GB): 24.00 (19.91/24.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 3.12e+00 (1.39e+00)
INFO 2025-08-12 01:57:29,428 train_utils.py: 271: Train Epoch: [22][ 20/136] | Batch Time: 0.90 (1.53) | Data Time: 0.00 (0.36) | Mem (GB): 17.00 (20.24/24.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 2.70e-01 (1.59e+00)
INFO 2025-08-12 01:57:40,718 train_utils.py: 271: Train Epoch: [22][ 30/136] | Batch Time: 0.91 (1.40) | Data Time: 0.00 (0.25) | Mem (GB): 17.00 (20.16/24.00) | Time Elapsed: 00d 01h 01m | Losses/train_all_loss: 2.91e-01 (1.60e+00)
INFO 2025-08-12 01:57:51,515 train_utils.py: 271: Train Epoch: [22][ 40/136] | Batch Time: 0.91 (1.32) | Data Time: 0.00 (0.19) | Mem (GB): 17.00 (19.95/24.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 2.76e-01 (1.47e+00)
INFO 2025-08-12 01:58:03,592 train_utils.py: 271: Train Epoch: [22][ 50/136] | Batch Time: 1.25 (1.30) | Data Time: 0.00 (0.15) | Mem (GB): 21.00 (20.10/24.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 1.80e+00 (1.56e+00)
INFO 2025-08-12 01:58:14,802 train_utils.py: 271: Train Epoch: [22][ 60/136] | Batch Time: 1.24 (1.27) | Data Time: 0.00 (0.13) | Mem (GB): 22.00 (20.03/24.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 3.34e+00 (1.54e+00)
INFO 2025-08-12 01:58:27,476 train_utils.py: 271: Train Epoch: [22][ 70/136] | Batch Time: 1.37 (1.27) | Data Time: 0.00 (0.11) | Mem (GB): 24.00 (20.25/24.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 3.74e+00 (1.70e+00)
INFO 2025-08-12 01:58:39,032 train_utils.py: 271: Train Epoch: [22][ 80/136] | Batch Time: 1.29 (1.26) | Data Time: 0.00 (0.09) | Mem (GB): 22.00 (20.22/24.00) | Time Elapsed: 00d 01h 02m | Losses/train_all_loss: 2.38e+00 (1.70e+00)
INFO 2025-08-12 01:58:50,574 train_utils.py: 271: Train Epoch: [22][ 90/136] | Batch Time: 1.08 (1.25) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (20.22/24.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 4.16e-01 (1.68e+00)
INFO 2025-08-12 01:59:01,577 train_utils.py: 271: Train Epoch: [22][100/136] | Batch Time: 1.09 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (20.15/24.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 4.90e-01 (1.64e+00)
INFO 2025-08-12 01:59:12,559 train_utils.py: 271: Train Epoch: [22][110/136] | Batch Time: 0.93 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (20.07/24.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 2.62e-01 (1.60e+00)
INFO 2025-08-12 01:59:24,145 train_utils.py: 271: Train Epoch: [22][120/136] | Batch Time: 1.24 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 21.00 (20.07/24.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 2.30e+00 (1.59e+00)
INFO 2025-08-12 01:59:36,756 train_utils.py: 271: Train Epoch: [22][130/136] | Batch Time: 1.37 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 24.00 (20.18/24.00) | Time Elapsed: 00d 01h 03m | Losses/train_all_loss: 2.77e+00 (1.62e+00)
INFO 2025-08-12 01:59:43,216 trainer.py: 950: Estimated time remaining: 00d 00h 46m
INFO 2025-08-12 01:59:43,324 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 01:59:43,324 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5754632515942348, 'Losses/train_all_loss_mask': 0.0051077131880340266, 'Losses/train_all_loss_dice': 6.302613067276337, 'Losses/train_all_loss_iou': 0.6386613279361936, 'Losses/train_all_loss_class': 0.008438402666270155, 'Losses/train_all_core_loss': 1.5754632515942348, 'Trainer/where': 0.5748161764705882, 'Trainer/epoch': 22, 'Trainer/steps_train': 3128}
INFO 2025-08-12 01:59:54,880 train_utils.py: 271: Train Epoch: [23][  0/136] | Batch Time: 10.13 (10.13) | Data Time: 8.35 (8.35) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 3.08e+00 (3.08e+00)
INFO 2025-08-12 02:00:04,968 train_utils.py: 271: Train Epoch: [23][ 10/136] | Batch Time: 1.05 (1.84) | Data Time: 0.00 (0.76) | Mem (GB): 19.00 (18.91/22.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 5.10e-01 (8.50e-01)
INFO 2025-08-12 02:00:16,281 train_utils.py: 271: Train Epoch: [23][ 20/136] | Batch Time: 1.33 (1.50) | Data Time: 0.00 (0.40) | Mem (GB): 24.00 (19.67/24.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 2.40e+00 (1.26e+00)
INFO 2025-08-12 02:00:27,939 train_utils.py: 271: Train Epoch: [23][ 30/136] | Batch Time: 1.34 (1.39) | Data Time: 0.00 (0.27) | Mem (GB): 20.00 (19.87/24.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 1.55e+00 (1.43e+00)
INFO 2025-08-12 02:00:39,466 train_utils.py: 271: Train Epoch: [23][ 40/136] | Batch Time: 1.08 (1.33) | Data Time: 0.00 (0.20) | Mem (GB): 19.00 (19.90/24.00) | Time Elapsed: 00d 01h 04m | Losses/train_all_loss: 4.69e-01 (1.42e+00)
INFO 2025-08-12 02:00:51,331 train_utils.py: 271: Train Epoch: [23][ 50/136] | Batch Time: 0.92 (1.31) | Data Time: 0.00 (0.16) | Mem (GB): 17.00 (19.96/24.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 2.81e-01 (1.48e+00)
INFO 2025-08-12 02:01:02,779 train_utils.py: 271: Train Epoch: [23][ 60/136] | Batch Time: 0.91 (1.28) | Data Time: 0.00 (0.14) | Mem (GB): 17.00 (19.95/24.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 2.95e-01 (1.49e+00)
INFO 2025-08-12 02:01:14,940 train_utils.py: 271: Train Epoch: [23][ 70/136] | Batch Time: 1.25 (1.27) | Data Time: 0.00 (0.12) | Mem (GB): 22.00 (20.14/24.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 2.91e+00 (1.60e+00)
INFO 2025-08-12 02:01:25,515 train_utils.py: 271: Train Epoch: [23][ 80/136] | Batch Time: 1.23 (1.24) | Data Time: 0.00 (0.10) | Mem (GB): 21.00 (19.94/24.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 1.53e+00 (1.49e+00)
INFO 2025-08-12 02:01:37,562 train_utils.py: 271: Train Epoch: [23][ 90/136] | Batch Time: 1.26 (1.24) | Data Time: 0.00 (0.09) | Mem (GB): 22.00 (20.03/24.00) | Time Elapsed: 00d 01h 05m | Losses/train_all_loss: 5.40e+00 (1.58e+00)
INFO 2025-08-12 02:01:49,113 train_utils.py: 271: Train Epoch: [23][100/136] | Batch Time: 1.39 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 23.00 (20.07/24.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 2.28e+00 (1.61e+00)
INFO 2025-08-12 02:02:01,257 train_utils.py: 271: Train Epoch: [23][110/136] | Batch Time: 1.23 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 21.00 (20.16/24.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 1.89e+00 (1.65e+00)
INFO 2025-08-12 02:02:12,794 train_utils.py: 271: Train Epoch: [23][120/136] | Batch Time: 0.90 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (20.21/24.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 3.07e-01 (1.70e+00)
INFO 2025-08-12 02:02:23,530 train_utils.py: 271: Train Epoch: [23][130/136] | Batch Time: 1.10 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 19.00 (20.10/24.00) | Time Elapsed: 00d 01h 06m | Losses/train_all_loss: 3.93e-01 (1.63e+00)
INFO 2025-08-12 02:02:30,854 trainer.py: 950: Estimated time remaining: 00d 00h 43m
INFO 2025-08-12 02:02:30,856 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:02:30,856 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.6333698367371279, 'Losses/train_all_loss_mask': 0.004940082677635741, 'Losses/train_all_loss_dice': 6.356167262091356, 'Losses/train_all_loss_iou': 0.6283665051793351, 'Losses/train_all_loss_class': 0.06356017425684654, 'Losses/train_all_core_loss': 1.6333698367371279, 'Trainer/where': 0.5998161764705883, 'Trainer/epoch': 23, 'Trainer/steps_train': 3264}
INFO 2025-08-12 02:02:41,621 train_utils.py: 271: Train Epoch: [24][  0/136] | Batch Time: 9.32 (9.32) | Data Time: 8.23 (8.23) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 2.85e-01 (2.85e-01)
INFO 2025-08-12 02:02:52,609 train_utils.py: 271: Train Epoch: [24][ 10/136] | Batch Time: 0.89 (1.85) | Data Time: 0.00 (0.75) | Mem (GB): 17.00 (19.45/22.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 2.85e-01 (1.32e+00)
INFO 2025-08-12 02:03:03,918 train_utils.py: 271: Train Epoch: [24][ 20/136] | Batch Time: 1.21 (1.51) | Data Time: 0.00 (0.39) | Mem (GB): 22.00 (19.71/24.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 3.03e+00 (1.42e+00)
INFO 2025-08-12 02:03:15,256 train_utils.py: 271: Train Epoch: [24][ 30/136] | Batch Time: 1.26 (1.39) | Data Time: 0.00 (0.27) | Mem (GB): 22.00 (19.87/24.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 3.00e+00 (1.49e+00)
INFO 2025-08-12 02:03:26,761 train_utils.py: 271: Train Epoch: [24][ 40/136] | Batch Time: 1.27 (1.33) | Data Time: 0.00 (0.20) | Mem (GB): 21.00 (19.83/24.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 1.55e+00 (1.53e+00)
INFO 2025-08-12 02:03:37,832 train_utils.py: 271: Train Epoch: [24][ 50/136] | Batch Time: 1.28 (1.28) | Data Time: 0.00 (0.16) | Mem (GB): 22.00 (19.82/24.00) | Time Elapsed: 00d 01h 07m | Losses/train_all_loss: 2.97e+00 (1.57e+00)
INFO 2025-08-12 02:03:49,459 train_utils.py: 271: Train Epoch: [24][ 60/136] | Batch Time: 0.92 (1.26) | Data Time: 0.00 (0.14) | Mem (GB): 17.00 (19.92/24.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 3.65e-01 (1.59e+00)
INFO 2025-08-12 02:04:01,486 train_utils.py: 271: Train Epoch: [24][ 70/136] | Batch Time: 1.09 (1.26) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (19.97/24.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 4.05e-01 (1.57e+00)
INFO 2025-08-12 02:04:12,510 train_utils.py: 271: Train Epoch: [24][ 80/136] | Batch Time: 0.93 (1.24) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (19.85/24.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 3.04e-01 (1.50e+00)
INFO 2025-08-12 02:04:24,890 train_utils.py: 271: Train Epoch: [24][ 90/136] | Batch Time: 1.24 (1.24) | Data Time: 0.00 (0.09) | Mem (GB): 22.00 (20.07/24.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 3.49e+00 (1.57e+00)
INFO 2025-08-12 02:04:35,866 train_utils.py: 271: Train Epoch: [24][100/136] | Batch Time: 0.94 (1.22) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (19.97/24.00) | Time Elapsed: 00d 01h 08m | Losses/train_all_loss: 1.25e+00 (1.53e+00)
INFO 2025-08-12 02:04:46,666 train_utils.py: 271: Train Epoch: [24][110/136] | Batch Time: 0.96 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.86/24.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 3.51e-01 (1.48e+00)
INFO 2025-08-12 02:04:59,047 train_utils.py: 271: Train Epoch: [24][120/136] | Batch Time: 1.21 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.99/24.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 2.04e+00 (1.53e+00)
INFO 2025-08-12 02:05:10,447 train_utils.py: 271: Train Epoch: [24][130/136] | Batch Time: 1.22 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (19.98/24.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 3.17e+00 (1.53e+00)
INFO 2025-08-12 02:05:17,969 trainer.py: 950: Estimated time remaining: 00d 00h 41m
INFO 2025-08-12 02:05:17,971 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:05:17,971 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5286985705880558, 'Losses/train_all_loss_mask': 0.004325608989722027, 'Losses/train_all_loss_dice': 6.0502290155957725, 'Losses/train_all_loss_iou': 0.640966146143482, 'Losses/train_all_loss_class': 0.039063205710327925, 'Losses/train_all_core_loss': 1.5286985705880558, 'Trainer/where': 0.6248161764705882, 'Trainer/epoch': 24, 'Trainer/steps_train': 3400}
INFO 2025-08-12 02:05:29,817 train_utils.py: 271: Train Epoch: [25][  0/136] | Batch Time: 9.28 (9.28) | Data Time: 5.37 (5.37) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 3.15e+00 (3.15e+00)
INFO 2025-08-12 02:05:40,711 train_utils.py: 271: Train Epoch: [25][ 10/136] | Batch Time: 0.89 (1.83) | Data Time: 0.00 (0.49) | Mem (GB): 17.00 (19.45/23.00) | Time Elapsed: 00d 01h 09m | Losses/train_all_loss: 3.02e-01 (1.72e+00)
INFO 2025-08-12 02:05:51,395 train_utils.py: 271: Train Epoch: [25][ 20/136] | Batch Time: 0.88 (1.47) | Data Time: 0.00 (0.26) | Mem (GB): 17.00 (19.43/23.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 3.88e-01 (1.61e+00)
INFO 2025-08-12 02:06:02,335 train_utils.py: 271: Train Epoch: [25][ 30/136] | Batch Time: 0.91 (1.35) | Data Time: 0.00 (0.17) | Mem (GB): 15.00 (19.39/23.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 3.87e-01 (1.56e+00)
INFO 2025-08-12 02:06:14,233 train_utils.py: 271: Train Epoch: [25][ 40/136] | Batch Time: 1.30 (1.31) | Data Time: 0.00 (0.13) | Mem (GB): 22.00 (19.56/23.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 3.05e+00 (1.57e+00)
INFO 2025-08-12 02:06:26,047 train_utils.py: 271: Train Epoch: [25][ 50/136] | Batch Time: 1.28 (1.28) | Data Time: 0.00 (0.11) | Mem (GB): 22.00 (19.69/23.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 2.80e+00 (1.57e+00)
INFO 2025-08-12 02:06:37,383 train_utils.py: 271: Train Epoch: [25][ 60/136] | Batch Time: 0.91 (1.26) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (19.70/23.00) | Time Elapsed: 00d 01h 10m | Losses/train_all_loss: 3.07e-01 (1.55e+00)
INFO 2025-08-12 02:06:49,183 train_utils.py: 271: Train Epoch: [25][ 70/136] | Batch Time: 0.92 (1.25) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (19.86/24.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 2.70e-01 (1.58e+00)
INFO 2025-08-12 02:07:00,558 train_utils.py: 271: Train Epoch: [25][ 80/136] | Batch Time: 1.09 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (19.86/24.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 4.53e-01 (1.56e+00)
INFO 2025-08-12 02:07:12,399 train_utils.py: 271: Train Epoch: [25][ 90/136] | Batch Time: 1.43 (1.23) | Data Time: 0.00 (0.06) | Mem (GB): 23.00 (19.93/24.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 2.67e+00 (1.55e+00)
INFO 2025-08-12 02:07:24,574 train_utils.py: 271: Train Epoch: [25][100/136] | Batch Time: 1.43 (1.23) | Data Time: 0.00 (0.05) | Mem (GB): 23.00 (19.98/24.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 2.49e+00 (1.55e+00)
INFO 2025-08-12 02:07:35,994 train_utils.py: 271: Train Epoch: [25][110/136] | Batch Time: 1.07 (1.22) | Data Time: 0.00 (0.05) | Mem (GB): 19.00 (19.93/24.00) | Time Elapsed: 00d 01h 11m | Losses/train_all_loss: 4.31e-01 (1.53e+00)
INFO 2025-08-12 02:07:47,116 train_utils.py: 271: Train Epoch: [25][120/136] | Batch Time: 1.21 (1.21) | Data Time: 0.00 (0.05) | Mem (GB): 21.00 (19.89/24.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 1.86e+00 (1.49e+00)
INFO 2025-08-12 02:07:58,119 train_utils.py: 271: Train Epoch: [25][130/136] | Batch Time: 0.91 (1.20) | Data Time: 0.00 (0.04) | Mem (GB): 17.00 (19.86/24.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 3.19e-01 (1.48e+00)
INFO 2025-08-12 02:08:05,392 trainer.py: 950: Estimated time remaining: 00d 00h 38m
INFO 2025-08-12 02:08:05,410 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:08:05,411 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4708058858180748, 'Losses/train_all_loss_mask': 0.004390806685264311, 'Losses/train_all_loss_dice': 5.920711884603781, 'Losses/train_all_loss_iou': 0.5896222748355392, 'Losses/train_all_loss_class': 0.015060798757637675, 'Losses/train_all_core_loss': 1.4708058858180748, 'Trainer/where': 0.6498161764705882, 'Trainer/epoch': 25, 'Trainer/steps_train': 3536}
INFO 2025-08-12 02:08:16,113 train_utils.py: 271: Train Epoch: [26][  0/136] | Batch Time: 9.27 (9.27) | Data Time: 8.04 (8.04) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 4.30e-01 (4.30e-01)
INFO 2025-08-12 02:08:26,401 train_utils.py: 271: Train Epoch: [26][ 10/136] | Batch Time: 1.18 (1.78) | Data Time: 0.00 (0.73) | Mem (GB): 21.00 (19.00/22.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 1.62e+00 (1.10e+00)
INFO 2025-08-12 02:08:38,304 train_utils.py: 271: Train Epoch: [26][ 20/136] | Batch Time: 1.23 (1.50) | Data Time: 0.00 (0.38) | Mem (GB): 22.00 (20.00/23.00) | Time Elapsed: 00d 01h 12m | Losses/train_all_loss: 2.53e+00 (1.55e+00)
INFO 2025-08-12 02:08:48,459 train_utils.py: 271: Train Epoch: [26][ 30/136] | Batch Time: 0.90 (1.34) | Data Time: 0.00 (0.26) | Mem (GB): 17.00 (19.45/23.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 2.67e-01 (1.25e+00)
INFO 2025-08-12 02:08:59,987 train_utils.py: 271: Train Epoch: [26][ 40/136] | Batch Time: 1.08 (1.30) | Data Time: 0.00 (0.20) | Mem (GB): 19.00 (19.78/24.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 4.33e-01 (1.45e+00)
INFO 2025-08-12 02:09:11,253 train_utils.py: 271: Train Epoch: [26][ 50/136] | Batch Time: 1.38 (1.26) | Data Time: 0.00 (0.16) | Mem (GB): 23.00 (19.76/24.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 2.86e+00 (1.41e+00)
INFO 2025-08-12 02:09:22,068 train_utils.py: 271: Train Epoch: [26][ 60/136] | Batch Time: 0.91 (1.23) | Data Time: 0.00 (0.13) | Mem (GB): 17.00 (19.69/24.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 2.65e-01 (1.35e+00)
INFO 2025-08-12 02:09:32,543 train_utils.py: 271: Train Epoch: [26][ 70/136] | Batch Time: 1.24 (1.21) | Data Time: 0.00 (0.11) | Mem (GB): 22.00 (19.56/24.00) | Time Elapsed: 00d 01h 13m | Losses/train_all_loss: 3.94e+00 (1.31e+00)
INFO 2025-08-12 02:09:44,876 train_utils.py: 271: Train Epoch: [26][ 80/136] | Batch Time: 1.35 (1.21) | Data Time: 0.00 (0.10) | Mem (GB): 23.00 (19.74/24.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 2.81e+00 (1.37e+00)
INFO 2025-08-12 02:09:56,723 train_utils.py: 271: Train Epoch: [26][ 90/136] | Batch Time: 1.08 (1.21) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (19.81/24.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 5.00e-01 (1.40e+00)
INFO 2025-08-12 02:10:07,250 train_utils.py: 271: Train Epoch: [26][100/136] | Batch Time: 1.12 (1.19) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (19.70/24.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 4.65e-01 (1.33e+00)
INFO 2025-08-12 02:10:18,879 train_utils.py: 271: Train Epoch: [26][110/136] | Batch Time: 0.90 (1.19) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.75/24.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 3.02e-01 (1.36e+00)
INFO 2025-08-12 02:10:30,231 train_utils.py: 271: Train Epoch: [26][120/136] | Batch Time: 0.91 (1.19) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.78/24.00) | Time Elapsed: 00d 01h 14m | Losses/train_all_loss: 2.93e-01 (1.39e+00)
INFO 2025-08-12 02:10:41,424 train_utils.py: 271: Train Epoch: [26][130/136] | Batch Time: 1.24 (1.18) | Data Time: 0.00 (0.06) | Mem (GB): 18.00 (19.73/24.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 1.64e+00 (1.37e+00)
INFO 2025-08-12 02:10:48,601 trainer.py: 950: Estimated time remaining: 00d 00h 34m
INFO 2025-08-12 02:10:48,603 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:10:48,603 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.371530526482007, 'Losses/train_all_loss_mask': 0.0038653136858449537, 'Losses/train_all_loss_dice': 5.495672519592678, 'Losses/train_all_loss_iou': 0.5889469621538678, 'Losses/train_all_loss_class': 0.01932063830468349, 'Losses/train_all_core_loss': 1.371530526482007, 'Trainer/where': 0.6748161764705882, 'Trainer/epoch': 26, 'Trainer/steps_train': 3672}
INFO 2025-08-12 02:10:59,010 train_utils.py: 271: Train Epoch: [27][  0/136] | Batch Time: 8.96 (8.96) | Data Time: 7.71 (7.71) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 3.06e-01 (3.06e-01)
INFO 2025-08-12 02:11:09,284 train_utils.py: 271: Train Epoch: [27][ 10/136] | Batch Time: 1.05 (1.75) | Data Time: 0.00 (0.70) | Mem (GB): 19.00 (18.64/22.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 3.89e-01 (8.33e-01)
INFO 2025-08-12 02:11:20,558 train_utils.py: 271: Train Epoch: [27][ 20/136] | Batch Time: 0.89 (1.45) | Data Time: 0.00 (0.37) | Mem (GB): 17.00 (18.95/23.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 2.97e-01 (1.08e+00)
INFO 2025-08-12 02:11:31,634 train_utils.py: 271: Train Epoch: [27][ 30/136] | Batch Time: 1.06 (1.34) | Data Time: 0.00 (0.25) | Mem (GB): 19.00 (19.10/23.00) | Time Elapsed: 00d 01h 15m | Losses/train_all_loss: 4.68e-01 (1.11e+00)
INFO 2025-08-12 02:11:42,501 train_utils.py: 271: Train Epoch: [27][ 40/136] | Batch Time: 1.24 (1.28) | Data Time: 0.00 (0.19) | Mem (GB): 22.00 (19.20/23.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 2.85e+00 (1.18e+00)
INFO 2025-08-12 02:11:53,965 train_utils.py: 271: Train Epoch: [27][ 50/136] | Batch Time: 1.23 (1.25) | Data Time: 0.00 (0.15) | Mem (GB): 21.00 (19.41/23.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 1.84e+00 (1.23e+00)
INFO 2025-08-12 02:12:04,627 train_utils.py: 271: Train Epoch: [27][ 60/136] | Batch Time: 1.09 (1.22) | Data Time: 0.00 (0.13) | Mem (GB): 19.00 (19.30/23.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 4.18e-01 (1.18e+00)
INFO 2025-08-12 02:12:15,846 train_utils.py: 271: Train Epoch: [27][ 70/136] | Batch Time: 1.08 (1.21) | Data Time: 0.00 (0.11) | Mem (GB): 19.00 (19.30/23.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 4.30e-01 (1.18e+00)
INFO 2025-08-12 02:12:27,284 train_utils.py: 271: Train Epoch: [27][ 80/136] | Batch Time: 1.08 (1.20) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (19.38/23.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 4.08e-01 (1.20e+00)
INFO 2025-08-12 02:12:38,346 train_utils.py: 271: Train Epoch: [27][ 90/136] | Batch Time: 1.10 (1.19) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (19.37/23.00) | Time Elapsed: 00d 01h 16m | Losses/train_all_loss: 4.48e-01 (1.20e+00)
INFO 2025-08-12 02:12:48,786 train_utils.py: 271: Train Epoch: [27][100/136] | Batch Time: 1.27 (1.18) | Data Time: 0.00 (0.08) | Mem (GB): 21.00 (19.30/23.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 1.66e+00 (1.16e+00)
INFO 2025-08-12 02:13:00,802 train_utils.py: 271: Train Epoch: [27][110/136] | Batch Time: 0.91 (1.18) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.44/23.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 3.30e-01 (1.22e+00)
INFO 2025-08-12 02:13:12,364 train_utils.py: 271: Train Epoch: [27][120/136] | Batch Time: 1.32 (1.18) | Data Time: 0.00 (0.06) | Mem (GB): 21.00 (19.49/24.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 1.46e+00 (1.24e+00)
INFO 2025-08-12 02:13:23,685 train_utils.py: 271: Train Epoch: [27][130/136] | Batch Time: 1.31 (1.17) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (19.51/24.00) | Time Elapsed: 00d 01h 17m | Losses/train_all_loss: 3.16e+00 (1.24e+00)
INFO 2025-08-12 02:13:30,978 trainer.py: 950: Estimated time remaining: 00d 00h 31m
INFO 2025-08-12 02:13:30,980 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:13:30,980 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.2422766672337757, 'Losses/train_all_loss_mask': 0.003476004262612311, 'Losses/train_all_loss_dice': 5.057749935809304, 'Losses/train_all_loss_iou': 0.5439759335097145, 'Losses/train_all_loss_class': 0.00027131312084804904, 'Losses/train_all_core_loss': 1.2422766672337757, 'Trainer/where': 0.6998161764705882, 'Trainer/epoch': 27, 'Trainer/steps_train': 3808}
INFO 2025-08-12 02:13:42,287 train_utils.py: 271: Train Epoch: [28][  0/136] | Batch Time: 9.88 (9.88) | Data Time: 8.19 (8.19) | Mem (GB): 23.00 (23.00/23.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 2.63e+00 (2.63e+00)
INFO 2025-08-12 02:13:53,695 train_utils.py: 271: Train Epoch: [28][ 10/136] | Batch Time: 1.20 (1.94) | Data Time: 0.00 (0.74) | Mem (GB): 19.00 (20.36/23.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 1.98e+00 (2.13e+00)
INFO 2025-08-12 02:14:04,659 train_utils.py: 271: Train Epoch: [28][ 20/136] | Batch Time: 1.20 (1.54) | Data Time: 0.00 (0.39) | Mem (GB): 22.00 (20.24/24.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 2.49e+00 (1.86e+00)
INFO 2025-08-12 02:14:15,134 train_utils.py: 271: Train Epoch: [28][ 30/136] | Batch Time: 0.92 (1.38) | Data Time: 0.00 (0.26) | Mem (GB): 17.00 (19.71/24.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 2.78e-01 (1.55e+00)
INFO 2025-08-12 02:14:26,083 train_utils.py: 271: Train Epoch: [28][ 40/136] | Batch Time: 1.07 (1.31) | Data Time: 0.00 (0.20) | Mem (GB): 19.00 (19.71/24.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 4.38e-01 (1.47e+00)
INFO 2025-08-12 02:14:37,039 train_utils.py: 271: Train Epoch: [28][ 50/136] | Batch Time: 1.07 (1.27) | Data Time: 0.00 (0.16) | Mem (GB): 19.00 (19.61/24.00) | Time Elapsed: 00d 01h 18m | Losses/train_all_loss: 4.34e-01 (1.38e+00)
INFO 2025-08-12 02:14:47,578 train_utils.py: 271: Train Epoch: [28][ 60/136] | Batch Time: 0.91 (1.23) | Data Time: 0.00 (0.13) | Mem (GB): 17.00 (19.49/24.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 3.22e-01 (1.33e+00)
INFO 2025-08-12 02:14:59,427 train_utils.py: 271: Train Epoch: [28][ 70/136] | Batch Time: 1.08 (1.23) | Data Time: 0.00 (0.12) | Mem (GB): 19.00 (19.65/24.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 4.29e-01 (1.35e+00)
INFO 2025-08-12 02:15:12,135 train_utils.py: 271: Train Epoch: [28][ 80/136] | Batch Time: 1.37 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 23.00 (19.84/24.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 3.61e+00 (1.42e+00)
INFO 2025-08-12 02:15:24,572 train_utils.py: 271: Train Epoch: [28][ 90/136] | Batch Time: 1.23 (1.23) | Data Time: 0.00 (0.09) | Mem (GB): 21.00 (20.00/24.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 1.78e+00 (1.51e+00)
INFO 2025-08-12 02:15:36,545 train_utils.py: 271: Train Epoch: [28][100/136] | Batch Time: 0.93 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (20.06/24.00) | Time Elapsed: 00d 01h 19m | Losses/train_all_loss: 2.61e-01 (1.53e+00)
INFO 2025-08-12 02:15:48,844 train_utils.py: 271: Train Epoch: [28][110/136] | Batch Time: 1.40 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 23.00 (20.14/24.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 2.63e+00 (1.57e+00)
INFO 2025-08-12 02:16:00,254 train_utils.py: 271: Train Epoch: [28][120/136] | Batch Time: 0.91 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (20.13/24.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 2.84e-01 (1.59e+00)
INFO 2025-08-12 02:16:11,003 train_utils.py: 271: Train Epoch: [28][130/136] | Batch Time: 0.92 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (20.04/24.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 3.04e-01 (1.55e+00)
INFO 2025-08-12 02:16:18,005 trainer.py: 950: Estimated time remaining: 00d 00h 30m
INFO 2025-08-12 02:16:18,007 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:16:18,008 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5314497770193745, 'Losses/train_all_loss_mask': 0.004499281664078915, 'Losses/train_all_loss_dice': 6.069813297951923, 'Losses/train_all_loss_iou': 0.6087503473219626, 'Losses/train_all_loss_class': 0.038262181688278635, 'Losses/train_all_core_loss': 1.5314497770193745, 'Trainer/where': 0.7248161764705883, 'Trainer/epoch': 28, 'Trainer/steps_train': 3944}
INFO 2025-08-12 02:16:29,536 train_utils.py: 271: Train Epoch: [29][  0/136] | Batch Time: 10.12 (10.12) | Data Time: 8.31 (8.31) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 3.06e+00 (3.06e+00)
INFO 2025-08-12 02:16:40,241 train_utils.py: 271: Train Epoch: [29][ 10/136] | Batch Time: 1.04 (1.89) | Data Time: 0.00 (0.76) | Mem (GB): 19.00 (19.09/22.00) | Time Elapsed: 00d 01h 20m | Losses/train_all_loss: 4.33e-01 (1.10e+00)
INFO 2025-08-12 02:16:51,308 train_utils.py: 271: Train Epoch: [29][ 20/136] | Batch Time: 1.21 (1.52) | Data Time: 0.00 (0.40) | Mem (GB): 18.00 (19.33/23.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 7.39e-01 (1.22e+00)
INFO 2025-08-12 02:17:02,759 train_utils.py: 271: Train Epoch: [29][ 30/136] | Batch Time: 0.91 (1.40) | Data Time: 0.00 (0.27) | Mem (GB): 17.00 (19.74/24.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 2.79e-01 (1.42e+00)
INFO 2025-08-12 02:17:14,323 train_utils.py: 271: Train Epoch: [29][ 40/136] | Batch Time: 1.24 (1.34) | Data Time: 0.00 (0.20) | Mem (GB): 22.00 (19.88/24.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 2.45e+00 (1.54e+00)
INFO 2025-08-12 02:17:25,135 train_utils.py: 271: Train Epoch: [29][ 50/136] | Batch Time: 1.08 (1.29) | Data Time: 0.00 (0.16) | Mem (GB): 19.00 (19.73/24.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 4.29e-01 (1.39e+00)
INFO 2025-08-12 02:17:36,095 train_utils.py: 271: Train Epoch: [29][ 60/136] | Batch Time: 0.92 (1.26) | Data Time: 0.00 (0.14) | Mem (GB): 15.00 (19.56/24.00) | Time Elapsed: 00d 01h 21m | Losses/train_all_loss: 3.14e-01 (1.34e+00)
INFO 2025-08-12 02:17:47,784 train_utils.py: 271: Train Epoch: [29][ 70/136] | Batch Time: 1.34 (1.24) | Data Time: 0.00 (0.12) | Mem (GB): 21.00 (19.65/24.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 1.57e+00 (1.39e+00)
INFO 2025-08-12 02:17:59,094 train_utils.py: 271: Train Epoch: [29][ 80/136] | Batch Time: 0.91 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (19.69/24.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 3.42e-01 (1.39e+00)
INFO 2025-08-12 02:18:10,183 train_utils.py: 271: Train Epoch: [29][ 90/136] | Batch Time: 0.95 (1.22) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (19.68/24.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 4.08e-01 (1.39e+00)
INFO 2025-08-12 02:18:21,421 train_utils.py: 271: Train Epoch: [29][100/136] | Batch Time: 1.09 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (19.65/24.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 5.38e-01 (1.36e+00)
INFO 2025-08-12 02:18:33,426 train_utils.py: 271: Train Epoch: [29][110/136] | Batch Time: 1.25 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 22.00 (19.79/24.00) | Time Elapsed: 00d 01h 22m | Losses/train_all_loss: 2.61e+00 (1.47e+00)
INFO 2025-08-12 02:18:44,733 train_utils.py: 271: Train Epoch: [29][120/136] | Batch Time: 0.93 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.81/24.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 2.82e-01 (1.46e+00)
INFO 2025-08-12 02:18:56,492 train_utils.py: 271: Train Epoch: [29][130/136] | Batch Time: 1.23 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 21.00 (19.84/24.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 1.89e+00 (1.48e+00)
INFO 2025-08-12 02:19:03,837 trainer.py: 950: Estimated time remaining: 00d 00h 27m
INFO 2025-08-12 02:19:03,974 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:19:03,974 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.490460463306483, 'Losses/train_all_loss_mask': 0.00457376249056217, 'Losses/train_all_loss_dice': 5.900407113573131, 'Losses/train_all_loss_iou': 0.6430435335427961, 'Losses/train_all_loss_class': 0.02168861841144701, 'Losses/train_all_core_loss': 1.490460463306483, 'Trainer/where': 0.7498161764705882, 'Trainer/epoch': 29, 'Trainer/steps_train': 4080}
INFO 2025-08-12 02:19:15,848 train_utils.py: 271: Train Epoch: [30][  0/136] | Batch Time: 9.33 (9.33) | Data Time: 7.70 (7.70) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 4.52e-01 (4.52e-01)
INFO 2025-08-12 02:19:27,148 train_utils.py: 271: Train Epoch: [30][ 10/136] | Batch Time: 1.04 (1.88) | Data Time: 0.00 (0.70) | Mem (GB): 19.00 (20.09/23.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 5.07e-01 (1.48e+00)
INFO 2025-08-12 02:19:38,509 train_utils.py: 271: Train Epoch: [30][ 20/136] | Batch Time: 1.22 (1.52) | Data Time: 0.00 (0.37) | Mem (GB): 21.00 (20.24/23.00) | Time Elapsed: 00d 01h 23m | Losses/train_all_loss: 1.23e+00 (1.54e+00)
INFO 2025-08-12 02:19:48,784 train_utils.py: 271: Train Epoch: [30][ 30/136] | Batch Time: 1.07 (1.36) | Data Time: 0.00 (0.25) | Mem (GB): 19.00 (19.77/23.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 4.27e-01 (1.33e+00)
INFO 2025-08-12 02:20:01,115 train_utils.py: 271: Train Epoch: [30][ 40/136] | Batch Time: 1.24 (1.33) | Data Time: 0.00 (0.19) | Mem (GB): 22.00 (20.24/24.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 2.95e+00 (1.58e+00)
INFO 2025-08-12 02:20:12,778 train_utils.py: 271: Train Epoch: [30][ 50/136] | Batch Time: 1.20 (1.30) | Data Time: 0.00 (0.15) | Mem (GB): 21.00 (20.25/24.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 1.58e+00 (1.63e+00)
INFO 2025-08-12 02:20:24,748 train_utils.py: 271: Train Epoch: [30][ 60/136] | Batch Time: 1.23 (1.28) | Data Time: 0.00 (0.13) | Mem (GB): 22.00 (20.38/24.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 3.00e+00 (1.70e+00)
INFO 2025-08-12 02:20:36,305 train_utils.py: 271: Train Epoch: [30][ 70/136] | Batch Time: 0.92 (1.26) | Data Time: 0.00 (0.11) | Mem (GB): 17.00 (20.34/24.00) | Time Elapsed: 00d 01h 24m | Losses/train_all_loss: 4.69e-01 (1.70e+00)
INFO 2025-08-12 02:20:48,235 train_utils.py: 271: Train Epoch: [30][ 80/136] | Batch Time: 1.08 (1.26) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (20.43/24.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 4.11e-01 (1.73e+00)
INFO 2025-08-12 02:20:58,693 train_utils.py: 271: Train Epoch: [30][ 90/136] | Batch Time: 1.23 (1.23) | Data Time: 0.00 (0.09) | Mem (GB): 21.00 (20.24/24.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 1.48e+00 (1.62e+00)
INFO 2025-08-12 02:21:10,391 train_utils.py: 271: Train Epoch: [30][100/136] | Batch Time: 1.25 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 22.00 (20.30/24.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 2.80e+00 (1.65e+00)
INFO 2025-08-12 02:21:22,238 train_utils.py: 271: Train Epoch: [30][110/136] | Batch Time: 1.26 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.32/24.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 3.31e+00 (1.66e+00)
INFO 2025-08-12 02:21:33,050 train_utils.py: 271: Train Epoch: [30][120/136] | Batch Time: 0.91 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (20.22/24.00) | Time Elapsed: 00d 01h 25m | Losses/train_all_loss: 2.56e-01 (1.62e+00)
INFO 2025-08-12 02:21:44,695 train_utils.py: 271: Train Epoch: [30][130/136] | Batch Time: 1.22 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.24/24.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 3.56e+00 (1.63e+00)
INFO 2025-08-12 02:21:52,255 trainer.py: 950: Estimated time remaining: 00d 00h 24m
INFO 2025-08-12 02:21:52,257 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:21:52,257 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.6509381938068306, 'Losses/train_all_loss_mask': 0.005147789417926962, 'Losses/train_all_loss_dice': 6.661982135737643, 'Losses/train_all_loss_iou': 0.6888695744085399, 'Losses/train_all_loss_class': 0.0005952195334160137, 'Losses/train_all_core_loss': 1.6509381938068306, 'Trainer/where': 0.7748161764705882, 'Trainer/epoch': 30, 'Trainer/steps_train': 4216}
INFO 2025-08-12 02:22:03,927 train_utils.py: 271: Train Epoch: [31][  0/136] | Batch Time: 10.24 (10.24) | Data Time: 7.30 (7.30) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 3.54e+00 (3.54e+00)
INFO 2025-08-12 02:22:15,132 train_utils.py: 271: Train Epoch: [31][ 10/136] | Batch Time: 1.33 (1.95) | Data Time: 0.00 (0.66) | Mem (GB): 23.00 (20.45/24.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 2.31e+00 (1.63e+00)
INFO 2025-08-12 02:22:26,187 train_utils.py: 271: Train Epoch: [31][ 20/136] | Batch Time: 0.89 (1.55) | Data Time: 0.00 (0.35) | Mem (GB): 17.00 (20.29/24.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 3.09e-01 (1.71e+00)
INFO 2025-08-12 02:22:36,939 train_utils.py: 271: Train Epoch: [31][ 30/136] | Batch Time: 1.07 (1.40) | Data Time: 0.00 (0.24) | Mem (GB): 19.00 (20.03/24.00) | Time Elapsed: 00d 01h 26m | Losses/train_all_loss: 4.39e-01 (1.56e+00)
INFO 2025-08-12 02:22:48,556 train_utils.py: 271: Train Epoch: [31][ 40/136] | Batch Time: 1.24 (1.34) | Data Time: 0.00 (0.18) | Mem (GB): 22.00 (20.24/24.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 2.63e+00 (1.70e+00)
INFO 2025-08-12 02:22:58,505 train_utils.py: 271: Train Epoch: [31][ 50/136] | Batch Time: 0.91 (1.27) | Data Time: 0.00 (0.14) | Mem (GB): 17.00 (19.80/24.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 3.01e-01 (1.44e+00)
INFO 2025-08-12 02:23:09,677 train_utils.py: 271: Train Epoch: [31][ 60/136] | Batch Time: 1.37 (1.25) | Data Time: 0.00 (0.12) | Mem (GB): 20.00 (19.74/24.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 1.70e+00 (1.40e+00)
INFO 2025-08-12 02:23:21,359 train_utils.py: 271: Train Epoch: [31][ 70/136] | Batch Time: 1.29 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 22.00 (19.85/24.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 2.29e+00 (1.45e+00)
INFO 2025-08-12 02:23:33,526 train_utils.py: 271: Train Epoch: [31][ 80/136] | Batch Time: 1.37 (1.23) | Data Time: 0.00 (0.09) | Mem (GB): 24.00 (19.95/24.00) | Time Elapsed: 00d 01h 27m | Losses/train_all_loss: 1.22e+01 (1.59e+00)
INFO 2025-08-12 02:23:45,382 train_utils.py: 271: Train Epoch: [31][ 90/136] | Batch Time: 0.92 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (20.01/24.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 2.95e-01 (1.58e+00)
INFO 2025-08-12 02:23:56,876 train_utils.py: 271: Train Epoch: [31][100/136] | Batch Time: 1.38 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 24.00 (20.03/24.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 3.49e+00 (1.58e+00)
INFO 2025-08-12 02:24:08,436 train_utils.py: 271: Train Epoch: [31][110/136] | Batch Time: 1.29 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.05/24.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 3.33e+00 (1.60e+00)
INFO 2025-08-12 02:24:19,225 train_utils.py: 271: Train Epoch: [31][120/136] | Batch Time: 0.90 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (19.98/24.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 2.73e-01 (1.56e+00)
INFO 2025-08-12 02:24:30,991 train_utils.py: 271: Train Epoch: [31][130/136] | Batch Time: 0.90 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (20.03/24.00) | Time Elapsed: 00d 01h 28m | Losses/train_all_loss: 2.90e-01 (1.61e+00)
INFO 2025-08-12 02:24:38,049 trainer.py: 950: Estimated time remaining: 00d 00h 21m
INFO 2025-08-12 02:24:38,051 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:24:38,051 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5868056176559013, 'Losses/train_all_loss_mask': 0.004976601130585871, 'Losses/train_all_loss_dice': 6.092707218054463, 'Losses/train_all_loss_iou': 0.5885651383603758, 'Losses/train_all_loss_class': 0.0763700962304639, 'Losses/train_all_core_loss': 1.5868056176559013, 'Trainer/where': 0.7998161764705882, 'Trainer/epoch': 31, 'Trainer/steps_train': 4352}
INFO 2025-08-12 02:24:49,577 train_utils.py: 271: Train Epoch: [32][  0/136] | Batch Time: 10.10 (10.10) | Data Time: 8.46 (8.46) | Mem (GB): 23.00 (23.00/23.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 2.56e+00 (2.56e+00)
INFO 2025-08-12 02:25:00,554 train_utils.py: 271: Train Epoch: [32][ 10/136] | Batch Time: 1.18 (1.92) | Data Time: 0.00 (0.77) | Mem (GB): 21.00 (20.27/23.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 1.78e+00 (1.65e+00)
INFO 2025-08-12 02:25:10,986 train_utils.py: 271: Train Epoch: [32][ 20/136] | Batch Time: 1.04 (1.50) | Data Time: 0.00 (0.40) | Mem (GB): 19.00 (19.71/23.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 5.63e-01 (1.42e+00)
INFO 2025-08-12 02:25:21,643 train_utils.py: 271: Train Epoch: [32][ 30/136] | Batch Time: 0.90 (1.36) | Data Time: 0.00 (0.27) | Mem (GB): 17.00 (19.48/24.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 3.83e-01 (1.30e+00)
INFO 2025-08-12 02:25:32,315 train_utils.py: 271: Train Epoch: [32][ 40/136] | Batch Time: 1.07 (1.29) | Data Time: 0.00 (0.21) | Mem (GB): 19.00 (19.39/24.00) | Time Elapsed: 00d 01h 29m | Losses/train_all_loss: 4.51e-01 (1.16e+00)
INFO 2025-08-12 02:25:43,475 train_utils.py: 271: Train Epoch: [32][ 50/136] | Batch Time: 1.25 (1.25) | Data Time: 0.00 (0.17) | Mem (GB): 22.00 (19.47/24.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 2.74e+00 (1.27e+00)
INFO 2025-08-12 02:25:55,190 train_utils.py: 271: Train Epoch: [32][ 60/136] | Batch Time: 1.21 (1.24) | Data Time: 0.00 (0.14) | Mem (GB): 21.00 (19.66/24.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 1.85e+00 (1.38e+00)
INFO 2025-08-12 02:26:06,759 train_utils.py: 271: Train Epoch: [32][ 70/136] | Batch Time: 0.92 (1.23) | Data Time: 0.00 (0.12) | Mem (GB): 17.00 (19.77/24.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 2.70e-01 (1.42e+00)
INFO 2025-08-12 02:26:18,328 train_utils.py: 271: Train Epoch: [32][ 80/136] | Batch Time: 1.38 (1.22) | Data Time: 0.00 (0.11) | Mem (GB): 23.00 (19.83/24.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 2.80e+00 (1.44e+00)
INFO 2025-08-12 02:26:29,680 train_utils.py: 271: Train Epoch: [32][ 90/136] | Batch Time: 1.07 (1.21) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (19.84/24.00) | Time Elapsed: 00d 01h 30m | Losses/train_all_loss: 5.30e-01 (1.42e+00)
INFO 2025-08-12 02:26:41,825 train_utils.py: 271: Train Epoch: [32][100/136] | Batch Time: 0.94 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (19.95/24.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 3.25e-01 (1.47e+00)
INFO 2025-08-12 02:26:52,572 train_utils.py: 271: Train Epoch: [32][110/136] | Batch Time: 0.92 (1.20) | Data Time: 0.00 (0.08) | Mem (GB): 17.00 (19.85/24.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 2.73e-01 (1.41e+00)
INFO 2025-08-12 02:27:03,000 train_utils.py: 271: Train Epoch: [32][120/136] | Batch Time: 1.35 (1.19) | Data Time: 0.00 (0.07) | Mem (GB): 23.00 (19.74/24.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 3.08e+00 (1.38e+00)
INFO 2025-08-12 02:27:14,260 train_utils.py: 271: Train Epoch: [32][130/136] | Batch Time: 0.91 (1.18) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.76/24.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 3.26e-01 (1.40e+00)
INFO 2025-08-12 02:27:21,364 trainer.py: 950: Estimated time remaining: 00d 00h 18m
INFO 2025-08-12 02:27:21,416 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:27:21,417 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.3896646953242666, 'Losses/train_all_loss_mask': 0.004010370260402851, 'Losses/train_all_loss_dice': 5.6415988575009735, 'Losses/train_all_loss_iou': 0.6033863526193753, 'Losses/train_all_loss_class': 0.0003046760166471743, 'Losses/train_all_core_loss': 1.3896646953242666, 'Trainer/where': 0.8248161764705882, 'Trainer/epoch': 32, 'Trainer/steps_train': 4488}
INFO 2025-08-12 02:27:30,946 train_utils.py: 271: Train Epoch: [33][  0/136] | Batch Time: 8.09 (8.09) | Data Time: 5.24 (5.24) | Mem (GB): 23.00 (23.00/23.00) | Time Elapsed: 00d 01h 31m | Losses/train_all_loss: 3.01e+00 (3.01e+00)
INFO 2025-08-12 02:27:41,462 train_utils.py: 271: Train Epoch: [33][ 10/136] | Batch Time: 1.03 (1.69) | Data Time: 0.00 (0.48) | Mem (GB): 16.00 (19.27/24.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 4.05e-01 (1.39e+00)
INFO 2025-08-12 02:27:52,660 train_utils.py: 271: Train Epoch: [33][ 20/136] | Batch Time: 1.23 (1.42) | Data Time: 0.00 (0.25) | Mem (GB): 22.00 (19.76/24.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 2.55e+00 (1.49e+00)
INFO 2025-08-12 02:28:04,310 train_utils.py: 271: Train Epoch: [33][ 30/136] | Batch Time: 1.06 (1.34) | Data Time: 0.00 (0.17) | Mem (GB): 19.00 (20.10/24.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 4.86e-01 (1.61e+00)
INFO 2025-08-12 02:28:16,149 train_utils.py: 271: Train Epoch: [33][ 40/136] | Batch Time: 1.36 (1.30) | Data Time: 0.00 (0.13) | Mem (GB): 19.00 (20.17/24.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 1.58e+00 (1.70e+00)
INFO 2025-08-12 02:28:27,350 train_utils.py: 271: Train Epoch: [33][ 50/136] | Batch Time: 1.09 (1.26) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (20.14/24.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 4.40e-01 (1.67e+00)
INFO 2025-08-12 02:28:38,860 train_utils.py: 271: Train Epoch: [33][ 60/136] | Batch Time: 1.08 (1.25) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (20.13/24.00) | Time Elapsed: 00d 01h 32m | Losses/train_all_loss: 4.20e-01 (1.68e+00)
INFO 2025-08-12 02:28:49,597 train_utils.py: 271: Train Epoch: [33][ 70/136] | Batch Time: 0.91 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (20.00/24.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 3.84e-01 (1.62e+00)
INFO 2025-08-12 02:29:00,477 train_utils.py: 271: Train Epoch: [33][ 80/136] | Batch Time: 0.91 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.91/24.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 3.41e-01 (1.56e+00)
INFO 2025-08-12 02:29:11,194 train_utils.py: 271: Train Epoch: [33][ 90/136] | Batch Time: 1.21 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 21.00 (19.85/24.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 1.91e+00 (1.53e+00)
INFO 2025-08-12 02:29:23,399 train_utils.py: 271: Train Epoch: [33][100/136] | Batch Time: 1.37 (1.19) | Data Time: 0.00 (0.05) | Mem (GB): 23.00 (19.92/24.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 3.52e+00 (1.57e+00)
INFO 2025-08-12 02:29:34,494 train_utils.py: 271: Train Epoch: [33][110/136] | Batch Time: 0.95 (1.19) | Data Time: 0.00 (0.05) | Mem (GB): 17.00 (19.88/24.00) | Time Elapsed: 00d 01h 33m | Losses/train_all_loss: 3.56e-01 (1.57e+00)
INFO 2025-08-12 02:29:45,627 train_utils.py: 271: Train Epoch: [33][120/136] | Batch Time: 1.14 (1.18) | Data Time: 0.00 (0.04) | Mem (GB): 19.00 (19.82/24.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 3.79e-01 (1.52e+00)
INFO 2025-08-12 02:29:56,506 train_utils.py: 271: Train Epoch: [33][130/136] | Batch Time: 1.08 (1.17) | Data Time: 0.00 (0.04) | Mem (GB): 19.00 (19.77/24.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 4.43e-01 (1.47e+00)
INFO 2025-08-12 02:30:03,571 trainer.py: 950: Estimated time remaining: 00d 00h 15m
INFO 2025-08-12 02:30:03,790 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:30:03,790 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4754929088932627, 'Losses/train_all_loss_mask': 0.00491037511432403, 'Losses/train_all_loss_dice': 5.900359758559396, 'Losses/train_all_loss_iou': 0.5520752822870717, 'Losses/train_all_loss_class': 0.013142762844281756, 'Losses/train_all_core_loss': 1.4754929088932627, 'Trainer/where': 0.8498161764705883, 'Trainer/epoch': 33, 'Trainer/steps_train': 4624}
INFO 2025-08-12 02:30:14,889 train_utils.py: 271: Train Epoch: [34][  0/136] | Batch Time: 9.67 (9.67) | Data Time: 8.71 (8.71) | Mem (GB): 17.00 (17.00/17.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 3.02e-01 (3.02e-01)
INFO 2025-08-12 02:30:26,158 train_utils.py: 271: Train Epoch: [34][ 10/136] | Batch Time: 0.88 (1.90) | Data Time: 0.00 (0.79) | Mem (GB): 15.00 (19.82/23.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 3.26e-01 (1.51e+00)
INFO 2025-08-12 02:30:37,581 train_utils.py: 271: Train Epoch: [34][ 20/136] | Batch Time: 1.19 (1.54) | Data Time: 0.00 (0.42) | Mem (GB): 21.00 (19.95/24.00) | Time Elapsed: 00d 01h 34m | Losses/train_all_loss: 1.95e+00 (1.56e+00)
INFO 2025-08-12 02:30:48,613 train_utils.py: 271: Train Epoch: [34][ 30/136] | Batch Time: 0.89 (1.40) | Data Time: 0.00 (0.28) | Mem (GB): 17.00 (19.81/24.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 3.11e-01 (1.54e+00)
INFO 2025-08-12 02:30:59,850 train_utils.py: 271: Train Epoch: [34][ 40/136] | Batch Time: 1.20 (1.33) | Data Time: 0.00 (0.21) | Mem (GB): 21.00 (19.85/24.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 1.54e+00 (1.52e+00)
INFO 2025-08-12 02:31:11,708 train_utils.py: 271: Train Epoch: [34][ 50/136] | Batch Time: 1.26 (1.30) | Data Time: 0.00 (0.17) | Mem (GB): 22.00 (20.04/24.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 2.54e+00 (1.57e+00)
INFO 2025-08-12 02:31:22,342 train_utils.py: 271: Train Epoch: [34][ 60/136] | Batch Time: 1.07 (1.26) | Data Time: 0.00 (0.14) | Mem (GB): 19.00 (19.87/24.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 4.61e-01 (1.49e+00)
INFO 2025-08-12 02:31:33,636 train_utils.py: 271: Train Epoch: [34][ 70/136] | Batch Time: 1.38 (1.25) | Data Time: 0.00 (0.12) | Mem (GB): 23.00 (19.83/24.00) | Time Elapsed: 00d 01h 35m | Losses/train_all_loss: 2.77e+00 (1.48e+00)
INFO 2025-08-12 02:31:45,798 train_utils.py: 271: Train Epoch: [34][ 80/136] | Batch Time: 1.07 (1.24) | Data Time: 0.00 (0.11) | Mem (GB): 19.00 (19.96/24.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 3.70e-01 (1.53e+00)
INFO 2025-08-12 02:31:56,790 train_utils.py: 271: Train Epoch: [34][ 90/136] | Batch Time: 1.07 (1.23) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (19.90/24.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 4.37e-01 (1.48e+00)
INFO 2025-08-12 02:32:08,615 train_utils.py: 271: Train Epoch: [34][100/136] | Batch Time: 0.93 (1.22) | Data Time: 0.00 (0.09) | Mem (GB): 17.00 (19.98/24.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 2.74e-01 (1.51e+00)
INFO 2025-08-12 02:32:19,850 train_utils.py: 271: Train Epoch: [34][110/136] | Batch Time: 1.23 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 22.00 (19.96/24.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 2.86e+00 (1.51e+00)
INFO 2025-08-12 02:32:31,054 train_utils.py: 271: Train Epoch: [34][120/136] | Batch Time: 0.90 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.97/24.00) | Time Elapsed: 00d 01h 36m | Losses/train_all_loss: 3.19e-01 (1.51e+00)
INFO 2025-08-12 02:32:42,487 train_utils.py: 271: Train Epoch: [34][130/136] | Batch Time: 1.08 (1.20) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (19.96/24.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 3.86e-01 (1.50e+00)
INFO 2025-08-12 02:32:50,152 trainer.py: 950: Estimated time remaining: 00d 00h 13m
INFO 2025-08-12 02:32:50,250 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:32:50,250 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.5112056447302593, 'Losses/train_all_loss_mask': 0.004599544449453841, 'Losses/train_all_loss_dice': 6.136592714225545, 'Losses/train_all_loss_iou': 0.5797891747754287, 'Losses/train_all_loss_class': 0.006945168362431322, 'Losses/train_all_core_loss': 1.5112056447302593, 'Trainer/where': 0.8748161764705882, 'Trainer/epoch': 34, 'Trainer/steps_train': 4760}
INFO 2025-08-12 02:33:02,467 train_utils.py: 271: Train Epoch: [35][  0/136] | Batch Time: 9.67 (9.67) | Data Time: 7.72 (7.72) | Mem (GB): 22.00 (22.00/22.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 2.49e+00 (2.49e+00)
INFO 2025-08-12 02:33:13,549 train_utils.py: 271: Train Epoch: [35][ 10/136] | Batch Time: 0.89 (1.89) | Data Time: 0.00 (0.70) | Mem (GB): 17.00 (20.36/24.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 2.76e-01 (1.84e+00)
INFO 2025-08-12 02:33:25,273 train_utils.py: 271: Train Epoch: [35][ 20/136] | Batch Time: 1.05 (1.55) | Data Time: 0.00 (0.37) | Mem (GB): 19.00 (20.33/24.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 4.13e-01 (1.76e+00)
INFO 2025-08-12 02:33:35,913 train_utils.py: 271: Train Epoch: [35][ 30/136] | Batch Time: 1.19 (1.39) | Data Time: 0.00 (0.25) | Mem (GB): 21.00 (19.97/24.00) | Time Elapsed: 00d 01h 37m | Losses/train_all_loss: 1.84e+00 (1.56e+00)
INFO 2025-08-12 02:33:47,487 train_utils.py: 271: Train Epoch: [35][ 40/136] | Batch Time: 0.91 (1.33) | Data Time: 0.00 (0.19) | Mem (GB): 17.00 (20.17/24.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 2.61e-01 (1.69e+00)
INFO 2025-08-12 02:33:58,785 train_utils.py: 271: Train Epoch: [35][ 50/136] | Batch Time: 1.21 (1.29) | Data Time: 0.00 (0.15) | Mem (GB): 22.00 (20.16/24.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 2.81e+00 (1.66e+00)
INFO 2025-08-12 02:34:09,738 train_utils.py: 271: Train Epoch: [35][ 60/136] | Batch Time: 1.23 (1.26) | Data Time: 0.00 (0.13) | Mem (GB): 22.00 (20.10/24.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 2.51e+00 (1.64e+00)
INFO 2025-08-12 02:34:21,627 train_utils.py: 271: Train Epoch: [35][ 70/136] | Batch Time: 1.38 (1.25) | Data Time: 0.00 (0.11) | Mem (GB): 24.00 (20.23/24.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 3.64e+00 (1.70e+00)
INFO 2025-08-12 02:34:32,909 train_utils.py: 271: Train Epoch: [35][ 80/136] | Batch Time: 0.91 (1.24) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (20.14/24.00) | Time Elapsed: 00d 01h 38m | Losses/train_all_loss: 2.56e-01 (1.67e+00)
INFO 2025-08-12 02:34:43,892 train_utils.py: 271: Train Epoch: [35][ 90/136] | Batch Time: 1.08 (1.22) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (20.08/24.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 4.34e-01 (1.64e+00)
INFO 2025-08-12 02:34:55,498 train_utils.py: 271: Train Epoch: [35][100/136] | Batch Time: 1.25 (1.21) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (20.08/24.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 1.54e+00 (1.62e+00)
INFO 2025-08-12 02:35:06,679 train_utils.py: 271: Train Epoch: [35][110/136] | Batch Time: 1.21 (1.21) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (20.07/24.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 3.46e+00 (1.64e+00)
INFO 2025-08-12 02:35:17,729 train_utils.py: 271: Train Epoch: [35][120/136] | Batch Time: 0.91 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (19.97/24.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 2.58e-01 (1.60e+00)
INFO 2025-08-12 02:35:28,722 train_utils.py: 271: Train Epoch: [35][130/136] | Batch Time: 0.91 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 15.00 (19.92/24.00) | Time Elapsed: 00d 01h 39m | Losses/train_all_loss: 4.11e-01 (1.58e+00)
INFO 2025-08-12 02:35:35,955 trainer.py: 950: Estimated time remaining: 00d 00h 10m
INFO 2025-08-12 02:35:35,957 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:35:35,957 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.596823527094196, 'Losses/train_all_loss_mask': 0.004458872441880564, 'Losses/train_all_loss_dice': 6.494667185580029, 'Losses/train_all_loss_iou': 0.7020203960812924, 'Losses/train_all_loss_class': 0.0014254503658438305, 'Losses/train_all_core_loss': 1.596823527094196, 'Trainer/where': 0.8998161764705882, 'Trainer/epoch': 35, 'Trainer/steps_train': 4896}
INFO 2025-08-12 02:35:47,004 train_utils.py: 271: Train Epoch: [36][  0/136] | Batch Time: 9.61 (9.61) | Data Time: 8.02 (8.02) | Mem (GB): 21.00 (21.00/21.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 2.12e+00 (2.12e+00)
INFO 2025-08-12 02:35:58,060 train_utils.py: 271: Train Epoch: [36][ 10/136] | Batch Time: 1.04 (1.88) | Data Time: 0.00 (0.73) | Mem (GB): 19.00 (20.18/23.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 4.92e-01 (1.49e+00)
INFO 2025-08-12 02:36:08,408 train_utils.py: 271: Train Epoch: [36][ 20/136] | Batch Time: 1.21 (1.48) | Data Time: 0.00 (0.38) | Mem (GB): 21.00 (19.33/23.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 1.80e+00 (1.17e+00)
INFO 2025-08-12 02:36:19,458 train_utils.py: 271: Train Epoch: [36][ 30/136] | Batch Time: 1.06 (1.36) | Data Time: 0.00 (0.26) | Mem (GB): 19.00 (19.42/24.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 3.86e-01 (1.20e+00)
INFO 2025-08-12 02:36:30,378 train_utils.py: 271: Train Epoch: [36][ 40/136] | Batch Time: 0.91 (1.29) | Data Time: 0.00 (0.20) | Mem (GB): 17.00 (19.44/24.00) | Time Elapsed: 00d 01h 40m | Losses/train_all_loss: 2.93e-01 (1.22e+00)
INFO 2025-08-12 02:36:41,135 train_utils.py: 271: Train Epoch: [36][ 50/136] | Batch Time: 1.37 (1.25) | Data Time: 0.00 (0.16) | Mem (GB): 23.00 (19.33/24.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 2.88e+00 (1.15e+00)
INFO 2025-08-12 02:36:53,114 train_utils.py: 271: Train Epoch: [36][ 60/136] | Batch Time: 1.29 (1.24) | Data Time: 0.00 (0.13) | Mem (GB): 22.00 (19.48/24.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 2.77e+00 (1.30e+00)
INFO 2025-08-12 02:37:04,914 train_utils.py: 271: Train Epoch: [36][ 70/136] | Batch Time: 1.20 (1.23) | Data Time: 0.00 (0.11) | Mem (GB): 21.00 (19.65/24.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 1.87e+00 (1.39e+00)
INFO 2025-08-12 02:37:16,263 train_utils.py: 271: Train Epoch: [36][ 80/136] | Batch Time: 1.24 (1.22) | Data Time: 0.00 (0.10) | Mem (GB): 22.00 (19.72/24.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 2.94e+00 (1.40e+00)
INFO 2025-08-12 02:37:27,243 train_utils.py: 271: Train Epoch: [36][ 90/136] | Batch Time: 1.08 (1.21) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (19.71/24.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 4.59e-01 (1.39e+00)
INFO 2025-08-12 02:37:38,085 train_utils.py: 271: Train Epoch: [36][100/136] | Batch Time: 1.11 (1.19) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (19.65/24.00) | Time Elapsed: 00d 01h 41m | Losses/train_all_loss: 3.87e-01 (1.36e+00)
INFO 2025-08-12 02:37:48,396 train_utils.py: 271: Train Epoch: [36][110/136] | Batch Time: 0.92 (1.18) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.56/24.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 2.73e-01 (1.32e+00)
INFO 2025-08-12 02:37:59,588 train_utils.py: 271: Train Epoch: [36][120/136] | Batch Time: 1.21 (1.18) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.60/24.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 2.18e+00 (1.35e+00)
INFO 2025-08-12 02:38:10,390 train_utils.py: 271: Train Epoch: [36][130/136] | Batch Time: 0.90 (1.17) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (19.58/24.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 3.79e-01 (1.32e+00)
INFO 2025-08-12 02:38:17,391 trainer.py: 950: Estimated time remaining: 00d 00h 07m
INFO 2025-08-12 02:38:17,442 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:38:17,443 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.3238024091457619, 'Losses/train_all_loss_mask': 0.003633059359879767, 'Losses/train_all_loss_dice': 5.217282249647028, 'Losses/train_all_loss_iou': 0.5525338621262241, 'Losses/train_all_loss_class': 0.04268209871954909, 'Losses/train_all_core_loss': 1.3238024091457619, 'Trainer/where': 0.9248161764705882, 'Trainer/epoch': 36, 'Trainer/steps_train': 5032}
INFO 2025-08-12 02:38:27,472 train_utils.py: 271: Train Epoch: [37][  0/136] | Batch Time: 8.61 (8.61) | Data Time: 7.32 (7.32) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 6.14e-01 (6.14e-01)
INFO 2025-08-12 02:38:39,291 train_utils.py: 271: Train Epoch: [37][ 10/136] | Batch Time: 1.34 (1.86) | Data Time: 0.00 (0.67) | Mem (GB): 23.00 (20.55/24.00) | Time Elapsed: 00d 01h 42m | Losses/train_all_loss: 2.23e+00 (1.60e+00)
INFO 2025-08-12 02:38:50,756 train_utils.py: 271: Train Epoch: [37][ 20/136] | Batch Time: 1.04 (1.52) | Data Time: 0.00 (0.35) | Mem (GB): 19.00 (20.43/24.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 5.40e-01 (1.62e+00)
INFO 2025-08-12 02:39:02,596 train_utils.py: 271: Train Epoch: [37][ 30/136] | Batch Time: 1.19 (1.41) | Data Time: 0.00 (0.24) | Mem (GB): 22.00 (20.65/24.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 2.65e+00 (1.86e+00)
INFO 2025-08-12 02:39:13,990 train_utils.py: 271: Train Epoch: [37][ 40/136] | Batch Time: 1.07 (1.34) | Data Time: 0.00 (0.18) | Mem (GB): 19.00 (20.49/24.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 3.87e-01 (1.75e+00)
INFO 2025-08-12 02:39:24,959 train_utils.py: 271: Train Epoch: [37][ 50/136] | Batch Time: 1.07 (1.30) | Data Time: 0.00 (0.14) | Mem (GB): 19.00 (20.27/24.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 4.05e-01 (1.67e+00)
INFO 2025-08-12 02:39:36,831 train_utils.py: 271: Train Epoch: [37][ 60/136] | Batch Time: 1.24 (1.28) | Data Time: 0.00 (0.12) | Mem (GB): 22.00 (20.38/24.00) | Time Elapsed: 00d 01h 43m | Losses/train_all_loss: 2.41e+00 (1.70e+00)
INFO 2025-08-12 02:39:48,744 train_utils.py: 271: Train Epoch: [37][ 70/136] | Batch Time: 1.23 (1.27) | Data Time: 0.00 (0.10) | Mem (GB): 21.00 (20.39/24.00) | Time Elapsed: 00d 01h 44m | Losses/train_all_loss: 1.87e+00 (1.71e+00)
INFO 2025-08-12 02:39:59,724 train_utils.py: 271: Train Epoch: [37][ 80/136] | Batch Time: 1.06 (1.25) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (20.26/24.00) | Time Elapsed: 00d 01h 44m | Losses/train_all_loss: 4.78e-01 (1.65e+00)
INFO 2025-08-12 02:40:11,099 train_utils.py: 271: Train Epoch: [37][ 90/136] | Batch Time: 1.41 (1.23) | Data Time: 0.00 (0.08) | Mem (GB): 23.00 (20.25/24.00) | Time Elapsed: 00d 01h 44m | Losses/train_all_loss: 2.32e+00 (1.64e+00)
INFO 2025-08-12 02:40:22,411 train_utils.py: 271: Train Epoch: [37][100/136] | Batch Time: 0.92 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (20.20/24.00) | Time Elapsed: 00d 01h 44m | Losses/train_all_loss: 2.92e-01 (1.61e+00)
INFO 2025-08-12 02:40:33,881 train_utils.py: 271: Train Epoch: [37][110/136] | Batch Time: 1.08 (1.22) | Data Time: 0.00 (0.07) | Mem (GB): 19.00 (20.19/24.00) | Time Elapsed: 00d 01h 44m | Losses/train_all_loss: 5.34e-01 (1.60e+00)
INFO 2025-08-12 02:40:44,842 train_utils.py: 271: Train Epoch: [37][120/136] | Batch Time: 0.90 (1.21) | Data Time: 0.00 (0.06) | Mem (GB): 17.00 (20.12/24.00) | Time Elapsed: 00d 01h 45m | Losses/train_all_loss: 2.92e-01 (1.57e+00)
INFO 2025-08-12 02:40:56,428 train_utils.py: 271: Train Epoch: [37][130/136] | Batch Time: 1.27 (1.20) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.14/24.00) | Time Elapsed: 00d 01h 45m | Losses/train_all_loss: 2.27e+00 (1.58e+00)
INFO 2025-08-12 02:41:03,624 trainer.py: 950: Estimated time remaining: 00d 00h 05m
INFO 2025-08-12 02:41:03,625 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:41:03,626 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.563176894231754, 'Losses/train_all_loss_mask': 0.004819978959537419, 'Losses/train_all_loss_dice': 6.28601520289393, 'Losses/train_all_loss_iou': 0.6834748117527103, 'Losses/train_all_loss_class': 0.0005795937878363672, 'Losses/train_all_core_loss': 1.563176894231754, 'Trainer/where': 0.9498161764705882, 'Trainer/epoch': 37, 'Trainer/steps_train': 5168}
INFO 2025-08-12 02:41:13,896 train_utils.py: 271: Train Epoch: [38][  0/136] | Batch Time: 8.85 (8.85) | Data Time: 7.74 (7.74) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 01h 45m | Losses/train_all_loss: 4.16e-01 (4.16e-01)
INFO 2025-08-12 02:41:25,492 train_utils.py: 271: Train Epoch: [38][ 10/136] | Batch Time: 1.32 (1.86) | Data Time: 0.00 (0.70) | Mem (GB): 24.00 (20.91/24.00) | Time Elapsed: 00d 01h 45m | Losses/train_all_loss: 2.69e+00 (1.60e+00)
INFO 2025-08-12 02:41:36,958 train_utils.py: 271: Train Epoch: [38][ 20/136] | Batch Time: 1.22 (1.52) | Data Time: 0.00 (0.37) | Mem (GB): 22.00 (20.62/24.00) | Time Elapsed: 00d 01h 45m | Losses/train_all_loss: 3.11e+00 (1.55e+00)
INFO 2025-08-12 02:41:47,246 train_utils.py: 271: Train Epoch: [38][ 30/136] | Batch Time: 0.91 (1.36) | Data Time: 0.00 (0.25) | Mem (GB): 17.00 (19.94/24.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 2.93e-01 (1.40e+00)
INFO 2025-08-12 02:41:58,920 train_utils.py: 271: Train Epoch: [38][ 40/136] | Batch Time: 1.26 (1.31) | Data Time: 0.00 (0.19) | Mem (GB): 21.00 (19.88/24.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 1.65e+00 (1.46e+00)
INFO 2025-08-12 02:42:10,422 train_utils.py: 271: Train Epoch: [38][ 50/136] | Batch Time: 0.91 (1.28) | Data Time: 0.00 (0.15) | Mem (GB): 17.00 (19.98/24.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 2.67e-01 (1.54e+00)
INFO 2025-08-12 02:42:21,222 train_utils.py: 271: Train Epoch: [38][ 60/136] | Batch Time: 1.08 (1.25) | Data Time: 0.00 (0.13) | Mem (GB): 19.00 (19.87/24.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 3.72e-01 (1.50e+00)
INFO 2025-08-12 02:42:32,200 train_utils.py: 271: Train Epoch: [38][ 70/136] | Batch Time: 1.29 (1.23) | Data Time: 0.00 (0.11) | Mem (GB): 22.00 (19.80/24.00) | Time Elapsed: 00d 01h 46m | Losses/train_all_loss: 3.00e+00 (1.45e+00)
INFO 2025-08-12 02:42:43,206 train_utils.py: 271: Train Epoch: [38][ 80/136] | Batch Time: 1.09 (1.21) | Data Time: 0.00 (0.10) | Mem (GB): 19.00 (19.74/24.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 3.95e-01 (1.42e+00)
INFO 2025-08-12 02:42:54,742 train_utils.py: 271: Train Epoch: [38][ 90/136] | Batch Time: 1.08 (1.21) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (19.77/24.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 3.68e-01 (1.40e+00)
INFO 2025-08-12 02:43:06,364 train_utils.py: 271: Train Epoch: [38][100/136] | Batch Time: 1.09 (1.20) | Data Time: 0.00 (0.08) | Mem (GB): 19.00 (19.81/24.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 4.01e-01 (1.41e+00)
INFO 2025-08-12 02:43:17,075 train_utils.py: 271: Train Epoch: [38][110/136] | Batch Time: 0.93 (1.19) | Data Time: 0.00 (0.07) | Mem (GB): 17.00 (19.75/24.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 2.50e-01 (1.38e+00)
INFO 2025-08-12 02:43:29,070 train_utils.py: 271: Train Epoch: [38][120/136] | Batch Time: 1.25 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (19.83/24.00) | Time Elapsed: 00d 01h 47m | Losses/train_all_loss: 2.41e+00 (1.42e+00)
INFO 2025-08-12 02:43:41,284 train_utils.py: 271: Train Epoch: [38][130/136] | Batch Time: 1.37 (1.19) | Data Time: 0.00 (0.06) | Mem (GB): 23.00 (19.92/24.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 2.52e+00 (1.46e+00)
INFO 2025-08-12 02:43:48,882 trainer.py: 950: Estimated time remaining: 00d 00h 02m
INFO 2025-08-12 02:43:48,979 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:43:48,979 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.4762749812182259, 'Losses/train_all_loss_mask': 0.0041309168174099315, 'Losses/train_all_loss_dice': 5.878138351089814, 'Losses/train_all_loss_iou': 0.6642731037470239, 'Losses/train_all_loss_class': 0.02321058913198674, 'Losses/train_all_core_loss': 1.4762749812182259, 'Trainer/where': 0.9748161764705883, 'Trainer/epoch': 38, 'Trainer/steps_train': 5304}
INFO 2025-08-12 02:43:59,150 train_utils.py: 271: Train Epoch: [39][  0/136] | Batch Time: 8.76 (8.76) | Data Time: 7.30 (7.30) | Mem (GB): 19.00 (19.00/19.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 3.91e-01 (3.91e-01)
INFO 2025-08-12 02:44:11,394 train_utils.py: 271: Train Epoch: [39][ 10/136] | Batch Time: 1.29 (1.91) | Data Time: 0.00 (0.66) | Mem (GB): 22.00 (20.73/23.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 2.74e+00 (1.87e+00)
INFO 2025-08-12 02:44:23,140 train_utils.py: 271: Train Epoch: [39][ 20/136] | Batch Time: 1.30 (1.56) | Data Time: 0.00 (0.35) | Mem (GB): 22.00 (20.43/23.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 2.99e+00 (1.83e+00)
INFO 2025-08-12 02:44:35,123 train_utils.py: 271: Train Epoch: [39][ 30/136] | Batch Time: 0.90 (1.44) | Data Time: 0.00 (0.24) | Mem (GB): 17.00 (20.45/23.00) | Time Elapsed: 00d 01h 48m | Losses/train_all_loss: 3.17e-01 (1.79e+00)
INFO 2025-08-12 02:44:45,513 train_utils.py: 271: Train Epoch: [39][ 40/136] | Batch Time: 1.45 (1.34) | Data Time: 0.00 (0.18) | Mem (GB): 23.00 (20.00/23.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 3.06e+00 (1.52e+00)
INFO 2025-08-12 02:44:56,755 train_utils.py: 271: Train Epoch: [39][ 50/136] | Batch Time: 1.44 (1.30) | Data Time: 0.00 (0.14) | Mem (GB): 23.00 (19.90/23.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 3.84e+00 (1.50e+00)
INFO 2025-08-12 02:45:08,719 train_utils.py: 271: Train Epoch: [39][ 60/136] | Batch Time: 1.31 (1.28) | Data Time: 0.00 (0.12) | Mem (GB): 22.00 (19.92/23.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 2.77e+00 (1.49e+00)
INFO 2025-08-12 02:45:20,474 train_utils.py: 271: Train Epoch: [39][ 70/136] | Batch Time: 0.95 (1.27) | Data Time: 0.00 (0.10) | Mem (GB): 17.00 (19.92/23.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 3.03e-01 (1.49e+00)
INFO 2025-08-12 02:45:32,779 train_utils.py: 271: Train Epoch: [39][ 80/136] | Batch Time: 1.07 (1.26) | Data Time: 0.00 (0.09) | Mem (GB): 19.00 (20.02/23.00) | Time Elapsed: 00d 01h 49m | Losses/train_all_loss: 4.75e-01 (1.57e+00)
INFO 2025-08-12 02:45:43,977 train_utils.py: 271: Train Epoch: [39][ 90/136] | Batch Time: 1.06 (1.25) | Data Time: 0.00 (0.08) | Mem (GB): 16.00 (19.95/23.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 3.13e-01 (1.52e+00)
INFO 2025-08-12 02:45:55,513 train_utils.py: 271: Train Epoch: [39][100/136] | Batch Time: 1.24 (1.24) | Data Time: 0.00 (0.07) | Mem (GB): 22.00 (19.99/23.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 2.40e+00 (1.55e+00)
INFO 2025-08-12 02:46:07,058 train_utils.py: 271: Train Epoch: [39][110/136] | Batch Time: 1.25 (1.23) | Data Time: 0.00 (0.07) | Mem (GB): 21.00 (19.97/23.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 1.51e+00 (1.54e+00)
INFO 2025-08-12 02:46:19,576 train_utils.py: 271: Train Epoch: [39][120/136] | Batch Time: 1.28 (1.23) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.07/23.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 3.33e+00 (1.57e+00)
INFO 2025-08-12 02:46:30,632 train_utils.py: 271: Train Epoch: [39][130/136] | Batch Time: 1.25 (1.22) | Data Time: 0.00 (0.06) | Mem (GB): 22.00 (20.03/23.00) | Time Elapsed: 00d 01h 50m | Losses/train_all_loss: 3.02e+00 (1.54e+00)
INFO 2025-08-12 02:46:38,065 trainer.py: 950: Estimated time remaining: 00d 00h 00m
INFO 2025-08-12 02:46:38,107 trainer.py: 892: Synchronizing meters
INFO 2025-08-12 02:46:38,108 trainer.py: 830: Losses and meters: {'Losses/train_all_loss': 1.574764593559153, 'Losses/train_all_loss_mask': 0.004196647033656037, 'Losses/train_all_loss_dice': 6.217925060321303, 'Losses/train_all_loss_iou': 0.6962493758319932, 'Losses/train_all_loss_class': 0.04504705808975603, 'Losses/train_all_core_loss': 1.574764593559153, 'Trainer/where': 0.9998161764705882, 'Trainer/epoch': 39, 'Trainer/steps_train': 5440}
